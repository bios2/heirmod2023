[
  {
    "objectID": "about.html#cibles-de-formation",
    "href": "about.html#cibles-de-formation",
    "title": "Advanced Statistics and hierarchical models",
    "section": "Cibles de formation",
    "text": "Cibles de formation\nApprendre la théorie statistique pour mieux construire, appliquer et interpréter différents modèles statistiques appliqués à des systèmes biologiques. Devenir familier avec la recherche primaire en en modélisation statistique de systèmes biologiques. Développer des connaissances et gagner de l’expérience à travailler de façon collaborative sur des problématiques lié au développement et à l’application de méthodes statisiques.\nThis course will feature hierarchical models and spatial statistics."
  },
  {
    "objectID": "about.html#course-instructors",
    "href": "about.html#course-instructors",
    "title": "Advanced Statistics and hierarchical models",
    "section": "Course Instructors",
    "text": "Course Instructors\n\n\n\n\n\n\nDr Guillaume Blanchet\n\n\n\n\n\n\n\nDr Andrew MacDonald\n\n\n\n\n\n\n\nDr Vincent Tolon"
  },
  {
    "objectID": "day_1.html#content",
    "href": "day_1.html#content",
    "title": "Day 1",
    "section": "Content",
    "text": "Content\nThe Secret Weapon\n\nAfternoon practical exercises"
  },
  {
    "objectID": "day_1.html#course-setup-information",
    "href": "day_1.html#course-setup-information",
    "title": "Day 1",
    "section": "Course setup information",
    "text": "Course setup information\n\nsite information\nplagiarism"
  },
  {
    "objectID": "day_1.html#simulation",
    "href": "day_1.html#simulation",
    "title": "Day 1",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "day_1.html#quantifying-uncertainty",
    "href": "day_1.html#quantifying-uncertainty",
    "title": "Day 1",
    "section": "Quantifying uncertainty",
    "text": "Quantifying uncertainty"
  },
  {
    "objectID": "day_1.html#resampling",
    "href": "day_1.html#resampling",
    "title": "Day 1",
    "section": "Resampling",
    "text": "Resampling\nIn frequentist models, we can use the variance covariance matrix of parameters to resample new parameters values. This lets us propagate uncertainty from the estimated parameters to the predicted relationship.\nLet’s demonstrate this with one specific mite:\n\nlrug_water <- mite_water |> \n  filter(sp == \"LRUG\")\n\nlrug_glm <- glm(pa ~ water, data = lrug_water, family = \"binomial\")\n\nNow, with our model object, we can create the resampling distribution of the model predicitons:\n\n# Set seed\nset.seed(42) # The answer !\n\n# a sequence along the range of water values in the data\npredVal <- seq(from = min(lrug_water$water),\n               to = max(lrug_water$water),\n               length.out = 30)\n\nn_resamp <- 500\n\n# Result object\nresampModel <- array(NA_real_,\n                   dim = c(length(predVal), n_resamp))\n\n# Resample model parameters and calculate model predictions\nparamMean <- summary(lrug_glm)$coefficients[,1]\nparamCov <- summary(lrug_glm)$cov.unscaled\n\n# Resample model parameters\nparamSmpl <- MASS::mvrnorm(n_resamp, paramMean, paramCov)\n\n# Calculate model predictions using the resampled model parameters\nfor(j in 1:n_resamp){\n  resampModel[,j] <- binomial(link = \"logit\")$linkinv(\n    paramSmpl[j,1] + paramSmpl[j,2] * predVal)\n}\n\n# make a plot of these predictions\nmatplot(predVal, resampModel, type = \"l\", col = \"grey\", lty = 1)\n\n\n\n\nIf we want to find some kind of confidence interval for this line, we can take the quantiles of this resampling:\n\nlow <- apply(resampModel, 1, quantile, probs = .015)\nhigh <- apply(resampModel, 1, quantile, probs = .985)\n\n# plot\nwith(lrug_water, plot(pa ~ water, pch = 21, bg = \"lightblue\"))\npolygon(c(predVal,rev(predVal)),\n        c(low,rev(high)), col=\"thistle\", border=NA)\nlines(predVal, \n      predict(lrug_glm, newdata = list(water = predVal), type = \"response\")\n      )\n\n\n\n\nWe can also do this in a tidyverse style, if you are more comfortable with that:\n\ntibble(predVal) |> \n  rowwise() |> \n  mutate(intercept = list(paramSmpl[,1]),\n         slope = list(paramSmpl[,2]),\n         prediction = list(intercept + slope*predVal),\n         prediction_probability = list(plogis(prediction)),\n         low  = quantile(prediction_probability, .015),\n         high = quantile(prediction_probability, .985)) |> \n  ggplot(aes(x = predVal, ymin = low, ymax = high)) + \n  geom_ribbon(fill = \"thistle\") + \n  theme_bw() + \n  ylim(c(0,1))"
  },
  {
    "objectID": "day_1.html#bayesian-approach",
    "href": "day_1.html#bayesian-approach",
    "title": "Day 1",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nhere is a simple bayesian model to generate the same inference:\n\\[\n\\begin{align}\ny &\\sim \\text{Bernoulli}(p)\\\\\n\\text{logit}(p) &= \\alpha + X\\beta\\\\\n\\alpha &\\sim \\text{Normal}(-2.5, .5)\\\\\n\\beta &\\sim \\text{Normal}(0, .5)\\\\\n\\end{align}\n\\]\nnormally we would go through a careful process of checking our priors here. At this time we won’t because the point here is to show how the bayesian posterior includes uncertainty, not to demonstrate a full Bayes workflow.\nFirst we compile the model, then we’ll look at the Stan code:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/blag1404/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\nlogistic_glm_stan <- cmdstan_model(stan_file = \"stan/logistic_bern_logit.stan\", \n                               pedantic = TRUE)\n\nlogistic_glm_stan\n\ndata {\n  int<lower=0> n;\n  vector[n] x;\n  array[n] int<lower=0,upper=1> y;\n}\nparameters {\n  real intercept;\n  real slope;\n}\nmodel {\n  y ~ bernoulli_logit(intercept + slope * x);\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n}\n\n\nHere we see the same three parts of a Stan model that we have reviewed already:\n\ndata\nparameters\nprobability statements\n\nAs you can see, we are using a handy Stan function called bernoulli_logit. This function expects our prediction for the average to be on the logit scale, then applies the logit link function for us.\n\n\nAs a quick review, the logit equation, or inverse-log-odds, is written as \\[\n\\frac{e^\\mu}{1 + e^\\mu}\n\\] Which is also written as\n\\[\n\\frac{1}{1 + e^{-\\mu}}\n\\]\nStan expects our data as a list.\n\nlogistic_glm_stan_samples <- logistic_glm_stan$sample(\n  data = list(n = nrow(lrug_water),\n              y = lrug_water$pa,\n              x = lrug_water$water),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.0 seconds.\n\ncoef(lrug_glm)\n\n(Intercept)       water \n-2.48153943  0.00874349 \n\nlibrary(tidybayes)\n\nspread_rvars(logistic_glm_stan_samples, intercept, slope[]) |> \n  bind_cols(predVal = predVal) |> \n  mutate(pred = posterior::rfun(plogis)(predVal * slope + intercept)) |> \n  ggplot(aes(x = predVal, ydist = pred)) + \n  stat_dist_lineribbon() + \n  guides(fill = \"none\") + \n  ylim(c(0,1))\n\n\n\n\n\nAlternative parameterization\nStan contains many functions intended to facilitate writing statistical models. Above, we used the function bernoulli_logit so that we could provide the expression for the average on the logit scale.  Stan also provides an even more efficient function that we can use; it is especially good when we have more than one predictor variable and a vector of slopes:This idea is the core concept of a GLM, or generalized linear model. Statistical distributions have parameters, but for most distributions these have constraints – only some values are “allowed”. For example, the only parameter of a Bernoulli distribution is \\(p\\), the probability of success. We respect this constraint by using a link function: we write an expression for the average of a distribution that can be any real number, and put it through a link function to get the value for \\(p\\).\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE NOTE below you will see the relative path to the stan file (stan/logistic.stan). Immediately below you will see the Stan file content. You can copy and paste this to your own computer!\n\n\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm <- cmdstan_model(stan_file = \"stan/logistic.stan\", \n                               pedantic = TRUE)\n\nlogistic_bern_glm\n\ndata {\n  int<lower=0> N;\n  matrix[N, 1] x;\n  array[N] int<lower=0,upper=1> y;\n}\nparameters {\n  real intercept;\n  vector[1] slope;\n}\nmodel {\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n  y ~ bernoulli_logit_glm(x, intercept, slope);\n}"
  },
  {
    "objectID": "day_2.html#convergence-diagnostics",
    "href": "day_2.html#convergence-diagnostics",
    "title": "Day 2",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics"
  },
  {
    "objectID": "day_2.html#model-validation-with-simulation",
    "href": "day_2.html#model-validation-with-simulation",
    "title": "Day 2",
    "section": "Model validation with simulation",
    "text": "Model validation with simulation"
  },
  {
    "objectID": "day_2.html#dags",
    "href": "day_2.html#dags",
    "title": "Day 2",
    "section": "DAGs",
    "text": "DAGs\n\nforks, pipes and colliders\nsimulation from a DAG\ndemonstration of errors from disregarding DAGs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course contents",
    "section": "",
    "text": "Here are documents containing all code and theory for the entire course:\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDay 1\n\n\n\n\n\nGeneral Introduction, introduction to simulation, model checking, and Stan workflows.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\nMCMC, Diagnostic tests, model validation, and DAGs.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "practice_example_template.html",
    "href": "practice_example_template.html",
    "title": "Template",
    "section": "",
    "text": "Some text\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "slides/00_Introduction/index.html#internet",
    "href": "slides/00_Introduction/index.html#internet",
    "title": "Introduction",
    "section": "Internet",
    "text": "Internet\nNetwork : \nPassword :"
  },
  {
    "objectID": "slides/00_Introduction/index.html#dos-and-donts-at-the-station-biologique-des-laurentides",
    "href": "slides/00_Introduction/index.html#dos-and-donts-at-the-station-biologique-des-laurentides",
    "title": "Introduction",
    "section": "Dos and don’ts at the Station biologique des Laurentides",
    "text": "Dos and don’ts at the Station biologique des Laurentides"
  },
  {
    "objectID": "slides/00_Introduction/index.html#course-material",
    "href": "slides/00_Introduction/index.html#course-material",
    "title": "Introduction",
    "section": "Course material",
    "text": "Course material\nWebsite :\nOn it, you will find the slides but also Rmarkdown files with practical examples.\nYou will also find course syllabus\nLet’s take a look…"
  },
  {
    "objectID": "slides/00_Introduction/index.html#general-daily-schedule",
    "href": "slides/00_Introduction/index.html#general-daily-schedule",
    "title": "Introduction",
    "section": "General daily schedule",
    "text": "General daily schedule\n9h00 - Start of the day\n9h00 to 12h00 - Lecture/Practice\n12h00 to 13h00 - Lunch\n13h00 to 17h00 - Lecture/Practice\n17h00 to 18h00 - Project presentations"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-are-hierarchical-models",
    "href": "slides/00_Introduction/index.html#what-are-hierarchical-models",
    "title": "Introduction",
    "section": "What are hierarchical models ?",
    "text": "What are hierarchical models ?\nFor this course, hierarchical models are regression models in which the parameters (the regression coefficients) are themselves given a probability model (Gelman and Hill 2007)."
  },
  {
    "objectID": "slides/00_Introduction/index.html#particularities-of-hierarchical-models",
    "href": "slides/00_Introduction/index.html#particularities-of-hierarchical-models",
    "title": "Introduction",
    "section": "Particularities of hierarchical models",
    "text": "Particularities of hierarchical models\nHierarchical models are\n\nA challenging bit of technology (probably more than you might think!).\nVery flexible models (in many more ways that are usually expected!)\nConstrained to the same particularities as (generalized) (non) linear models (sometimes to a more severe extent)"
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nHierarchical models, as we will see them in this course, are also known under different names\n\nRandom effect models\nMixed models\nMultilevel models\nVariance component models\nError component models"
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary-1",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary-1",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nWe decided to use the term hierachical model to prevent confusions that sometimes arises in the litterature about random and fixed effects, which are terms commonly used when referring to mixed effect models.\nActually, random and fixed effects have multiple definitions, which leads to confusion"
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary-2",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary-2",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nMultiple Definition of fixed and random effects\n\n(Kreft and De Leeuw 1998) Fixed effects are constant and random effect vary\n(Searl et al. 1992) Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population\n(Green and Tukey 1960) When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random\n(Roy LaMotte 2014) If an effect is assumed to be a realized value of a random variable, it is called a random effect\n(Robinson 1991) Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage."
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-history",
    "href": "slides/00_Introduction/index.html#a-bit-of-history",
    "title": "Introduction",
    "section": "A bit of history",
    "text": "A bit of history\nBecause of the different name used for hierarchial models, the history of this subfield of statistics is a little murky, but we know that a few important figures in statistics dabbed into this field, most notably\n\n\nFisher R.A. (1919). The Correlation between Relatives on the Supposition of Mendelian Inheritance. Transactions of the Royal Society of Edinburgh, 52 399–433."
  },
  {
    "objectID": "slides/00_Introduction/index.html#implementation",
    "href": "slides/00_Introduction/index.html#implementation",
    "title": "Introduction",
    "section": "Implementation",
    "text": "Implementation\n\nHierarchical models have been implemented in many software packages,\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\nin julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/00_Introduction/index.html#implementation-1",
    "href": "slides/00_Introduction/index.html#implementation-1",
    "title": "Introduction",
    "section": "Implementation",
    "text": "Implementation\nWhat we will use\nWe will not use any of these software packages… because under specific circumstances, what may seem like the same implementation of a model may lead to different answers… and both can actually be right !\nThis is because the underlying model implemented in the software package may actually be slightly different.\nInstead we will implement our own models from scratch using Stan."
  },
  {
    "objectID": "slides/00_Introduction/index.html#good-reference",
    "href": "slides/00_Introduction/index.html#good-reference",
    "title": "Introduction",
    "section": "Good reference",
    "text": "Good reference\n\nA good portion of this course material is based on this book."
  },
  {
    "objectID": "slides/00_Introduction/index.html#great-technical-references",
    "href": "slides/00_Introduction/index.html#great-technical-references",
    "title": "Introduction",
    "section": "Great technical references",
    "text": "Great technical references\n\nEverything is there but it can gets technical !"
  },
  {
    "objectID": "slides/00_Introduction/index.html#r-and-stan",
    "href": "slides/00_Introduction/index.html#r-and-stan",
    "title": "Introduction",
    "section": "R and Stan",
    "text": "R and Stan\nAll of the practical aspect of the course will be done with R and Stan.\nRStudio\nWe strongly (!) encourage you to use RStudio and to start a project for the course."
  },
  {
    "objectID": "slides/00_Introduction/index.html#r-package-to-install",
    "href": "slides/00_Introduction/index.html#r-package-to-install",
    "title": "Introduction",
    "section": "R package to install",
    "text": "R package to install\n\ninstall.packages(c(\"vegan\"))"
  },
  {
    "objectID": "slides/01_Data/index.html#illustrative-datasets",
    "href": "slides/01_Data/index.html#illustrative-datasets",
    "title": "Data used for this course",
    "section": "Illustrative datasets",
    "text": "Illustrative datasets\nTo illustrate the different models and methods we will discuss in this course, we will rely on a few data sets, which are directly available in different R package\n\nmite, mite.env and mite.xy available in the vegan R package\npenguins available in the palmerpenguins R package\n\nThese datasets are practical because they are manageable in size and will allow you to see how to work out the different example presented in this course.\nLet’s look at them in more details"
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data",
    "href": "slides/01_Data/index.html#oribatid-mite-data",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nAside from being a very interesting datasets this data has been sample on the southern shore of Lac Geai (a few minutes walk from here ! We will go see it this week)\nSampling was carried out in June 1989 on the partially floating vegetation mat surrounding the lake from the forest border to the free water by Daniel Borcard"
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data-1",
    "href": "slides/01_Data/index.html#oribatid-mite-data-1",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nOribatid mites are small (usually ranging in size from 0.2 to 1.4 mm) invertebrates that are part of the Arachnida class (so they have 8 legs).\n\nIn the mite data, 35 morphospecies were identified and counted across 70 samples."
  },
  {
    "objectID": "slides/01_Data/index.html#sites-coordinates",
    "href": "slides/01_Data/index.html#sites-coordinates",
    "title": "Data used for this course",
    "section": "Sites coordinates",
    "text": "Sites coordinates"
  },
  {
    "objectID": "slides/01_Data/index.html#vegetation-cover",
    "href": "slides/01_Data/index.html#vegetation-cover",
    "title": "Data used for this course",
    "section": "Vegetation cover",
    "text": "Vegetation cover\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "href": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "title": "Data used for this course",
    "section": "Microtopography and shrub cover",
    "text": "Microtopography and shrub cover\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/01_Data/index.html#substrate-density-and-water-content",
    "href": "slides/01_Data/index.html#substrate-density-and-water-content",
    "title": "Data used for this course",
    "section": "Substrate density and water content",
    "text": "Substrate density and water content\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#basic-regression-model-1",
    "href": "slides/02_Linear_model/index.html#basic-regression-model-1",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\nFor example if we are interested in knowing how a newly discovered plant species (Bidonia exemplaris) reacts to humidity, we can relate the biomass of B. exemplaris sampled at 100 sites with the soil humidity content and readily visual the data and the trend."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#basic-regression-model-2",
    "href": "slides/02_Linear_model/index.html#basic-regression-model-2",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#gaussian-error",
    "href": "slides/02_Linear_model/index.html#gaussian-error",
    "title": "Linear models",
    "section": "Gaussian error",
    "text": "Gaussian error"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nMany techniques have been proposed to estimate the parameters of a regression model.\nThe goal of this course is not to study these techniques but we will learn how to play with the estimated parameters because it will be very useful as we move along.\nFor example, if we want to build a model’s confidence interval from a linear regression coded as\n\n\n\n\nreg <- lm(b.exemplaris ~ humidity)\n\nHow would you do it ?\nLet’s look at the model’s results are worth studying"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-1",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-1",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nModel’s results\n\n\n(summaryReg <- summary(reg))\n\n\nCall:\nlm(formula = b.exemplaris ~ humidity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47988 -0.26475  0.00611  0.32590  1.36077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.57389    0.04720   54.53   <2e-16 ***\nhumidity     1.10086    0.07976   13.80   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4718 on 98 degrees of freedom\nMultiple R-squared:  0.6603,    Adjusted R-squared:  0.6569 \nF-statistic: 190.5 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\nLet’s say we want to construst the model’s confidence intervals by sampling multiple times (say 100 times!) the regression parameters, which we will assume follow Gaussian distribution. How would you do this ?"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#basic-regression-model---gaussian-error",
    "href": "slides/02_Linear_model/index.html#basic-regression-model---gaussian-error",
    "title": "Linear models",
    "section": "Basic regression model - Gaussian error",
    "text": "Basic regression model - Gaussian error\nTo go around this problem, generalized linear models (GLMs) have been proposed. In essence, GLMs use link functions to adapt models for them to be used on non-Gaussian data.\nFor example\nlogit link function :\nlog link function :\nThe logit link function is often used for modelling binary data while the log link function is commonly used for modelling count data."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#basic-regression-model-3",
    "href": "slides/02_Linear_model/index.html#basic-regression-model-3",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\nThere are however a few major pitfall in using the simple linear model for problems in the life sciences\n\nOne explanatory is almost never enough to answer the questions we want to aproach"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#second",
    "href": "slides/02_Linear_model/index.html#second",
    "title": "Linear models",
    "section": "second",
    "text": "second\n\nTest your model"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#section",
    "href": "slides/02_Linear_model/index.html#section",
    "title": "Linear models",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#section-1",
    "href": "slides/02_Linear_model/index.html#section-1",
    "title": "Linear models",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#section-2",
    "href": "slides/02_Linear_model/index.html#section-2",
    "title": "Linear models",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#choose-parameters",
    "href": "slides/02_Linear_model/index.html#choose-parameters",
    "title": "Linear models",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#make-up-an-x-variable",
    "href": "slides/02_Linear_model/index.html#make-up-an-x-variable",
    "title": "Linear models",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#calculate-the-average",
    "href": "slides/02_Linear_model/index.html#calculate-the-average",
    "title": "Linear models",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#simulate-some-observations",
    "href": "slides/02_Linear_model/index.html#simulate-some-observations",
    "title": "Linear models",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#finally-visualize",
    "href": "slides/02_Linear_model/index.html#finally-visualize",
    "title": "Linear models",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#here-it-is-all-on-one-slide",
    "href": "slides/02_Linear_model/index.html#here-it-is-all-on-one-slide",
    "title": "Linear models",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/02_Linear_model/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Linear models",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#another-equation",
    "href": "slides/02_Linear_model/index.html#another-equation",
    "title": "Linear models",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#the-equation",
    "href": "slides/02_Linear_model/index.html#the-equation",
    "title": "Linear models",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#the-model",
    "href": "slides/02_Linear_model/index.html#the-model",
    "title": "Linear models",
    "section": "The model",
    "text": "The model\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#declare-the-data",
    "href": "slides/02_Linear_model/index.html#declare-the-data",
    "title": "Linear models",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#state-parameters",
    "href": "slides/02_Linear_model/index.html#state-parameters",
    "title": "Linear models",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#write-the-likelihood-and-priors",
    "href": "slides/02_Linear_model/index.html#write-the-likelihood-and-priors",
    "title": "Linear models",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#un-beau-titre",
    "href": "slides/template/index.html#un-beau-titre",
    "title": "Template for presentations",
    "section": "Un beau titre",
    "text": "Un beau titre\nthis is the first slide"
  },
  {
    "objectID": "slides/template/index.html#second",
    "href": "slides/template/index.html#second",
    "title": "Template for presentations",
    "section": "second",
    "text": "second\n\nTest your model"
  },
  {
    "objectID": "slides/template/index.html#section",
    "href": "slides/template/index.html#section",
    "title": "Template for presentations",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/template/index.html#section-1",
    "href": "slides/template/index.html#section-1",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#section-2",
    "href": "slides/template/index.html#section-2",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#choose-parameters",
    "href": "slides/template/index.html#choose-parameters",
    "title": "Template for presentations",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5"
  },
  {
    "objectID": "slides/template/index.html#make-up-an-x-variable",
    "href": "slides/template/index.html#make-up-an-x-variable",
    "title": "Template for presentations",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/template/index.html#calculate-the-average",
    "href": "slides/template/index.html#calculate-the-average",
    "title": "Template for presentations",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x"
  },
  {
    "objectID": "slides/template/index.html#simulate-some-observations",
    "href": "slides/template/index.html#simulate-some-observations",
    "title": "Template for presentations",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/template/index.html#finally-visualize",
    "href": "slides/template/index.html#finally-visualize",
    "title": "Template for presentations",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#here-it-is-all-on-one-slide",
    "href": "slides/template/index.html#here-it-is-all-on-one-slide",
    "title": "Template for presentations",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Template for presentations",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#another-equation",
    "href": "slides/template/index.html#another-equation",
    "title": "Template for presentations",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-equation",
    "href": "slides/template/index.html#the-equation",
    "title": "Template for presentations",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-model",
    "href": "slides/template/index.html#the-model",
    "title": "Template for presentations",
    "section": "The model",
    "text": "The model\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#declare-the-data",
    "href": "slides/template/index.html#declare-the-data",
    "title": "Template for presentations",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#state-parameters",
    "href": "slides/template/index.html#state-parameters",
    "title": "Template for presentations",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#write-the-likelihood-and-priors",
    "href": "slides/template/index.html#write-the-likelihood-and-priors",
    "title": "Template for presentations",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "topics/secret_weapon.html",
    "href": "topics/secret_weapon.html",
    "title": "Summarizing many univariate models",
    "section": "",
    "text": "We’ve already looked at univariate models. When we fit the same model to multiple different groups, we don’t expect the same values for all the coefficients. Each thing we are studying will respond to the same variable in different ways.\nHierarchial models represent a way to model this variation, in ways that range from simple to complex.\nBefore we dive in with hierarchical structure, let’s build a bridge between these two approaches.\nThis is useful to help us understand what a hierarchical model does.\nHowever it is also useful from a strict model-building perspective – so useful that Andrew Gelman calls it a “Secret Weapon” tk link\nTo keep things simple and univariate, let’s consider only water:\nFirst, a quick word about centering and scaling a predictor variable:\nsome things to notice about this figure:\nAs you can see, some of these estimates are high, others low. We could also plot these as histograms to see this distribution.\nOnce again, the two parameters of this model represent:"
  },
  {
    "objectID": "topics/secret_weapon.html#say-it-in-stan",
    "href": "topics/secret_weapon.html#say-it-in-stan",
    "title": "Summarizing many univariate models",
    "section": "Say it in Stan",
    "text": "Say it in Stan\nThe above tidyverse approach is very appealing and intuitive, but we can also do the same procedure in Stan."
  },
  {
    "objectID": "topics/secret_weapon.html#modelling-variation-in-slopes",
    "href": "topics/secret_weapon.html#modelling-variation-in-slopes",
    "title": "Summarizing many univariate models",
    "section": "Modelling variation in slopes",
    "text": "Modelling variation in slopes\nClearly there is variation among species in the values of these parameters. Like all variation, we can develop a scientific model to describe it. The simplest model we’ll consider is a simple univariate distribution.\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm <- cmdstan_model(\n  stan_file = here::here(\"topics/secret_weapon_univariate.stan\"), \n  pedantic = TRUE)\n\nmite_bin <- mite\nmite_bin[mite_bin>0] <- 1\n\nlogistic_bern_glm$sample(data = list(\n  Nsites = nrow(mite_bin),\n  K = 2,\n  S = ncol(mite_bin),\n  x = cbind(1, with(mite.env, (WatrCont - mean(WatrCont))/100)),\n  y = as.matrix(mite_bin)\n))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 9.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 9.3 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 10.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 10.3 seconds.\nTotal execution time: 42.0 seconds.\n\n\n variable     mean   median   sd  mad       q5      q95 rhat ess_bulk ess_tail\n   lp__   -1171.18 -1170.78 8.32 8.07 -1185.34 -1157.97 1.00      643     1511\n   z[1,1]     1.67     1.65 0.36 0.36     1.11     2.29 1.00     1214     2032\n   z[2,1]     0.04     0.04 0.39 0.37    -0.62     0.67 1.00     2184     2490\n   z[1,2]    -0.41    -0.40 0.26 0.25    -0.86     0.00 1.00      839     1684\n   z[2,2]    -0.92    -0.90 0.43 0.41    -1.68    -0.26 1.00     2123     2361\n   z[1,3]     2.06     2.04 0.42 0.41     1.41     2.80 1.00     1256     2335\n   z[2,3]    -0.10    -0.09 0.42 0.40    -0.81     0.58 1.00     2376     2473\n   z[1,4]    -0.62    -0.61 0.28 0.28    -1.09    -0.17 1.00      706     1606\n   z[2,4]    -1.04    -1.02 0.45 0.44    -1.80    -0.36 1.00     1918     2508\n   z[1,5]    -1.07    -1.06 0.32 0.32    -1.61    -0.57 1.01      770     1442\n\n # showing 10 of 145 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nlet’s take a look at these values:\n\ndata(\"mite.xy\", package = \"vegan\")\nmite.xy\n\n      x   y\n1  0.20 0.1\n2  1.00 0.1\n3  1.20 0.3\n4  1.40 0.5\n5  2.40 0.7\n6  1.80 0.9\n7  0.05 1.1\n8  2.00 1.3\n9  2.00 1.5\n10 1.20 1.7\n11 2.40 1.9\n12 0.20 2.1\n13 0.40 2.1\n14 2.00 2.3\n15 2.20 2.3\n16 0.05 2.7\n17 0.20 2.7\n18 2.20 2.7\n19 2.40 2.7\n20 1.20 2.9\n21 0.05 3.1\n22 1.40 3.1\n23 2.40 3.1\n24 0.20 3.5\n25 1.20 3.7\n26 0.80 3.9\n27 1.60 3.9\n28 0.20 4.1\n29 0.80 4.1\n30 1.80 4.5\n31 0.20 4.7\n32 1.40 4.7\n33 0.60 5.3\n34 1.00 5.3\n35 2.40 5.3\n36 1.40 5.5\n37 1.80 5.5\n38 0.40 5.9\n39 1.00 5.9\n40 1.80 5.9\n41 2.00 5.9\n42 0.05 6.1\n43 0.20 6.1\n44 0.40 6.1\n45 1.20 6.1\n46 1.60 6.1\n47 1.60 6.3\n48 0.40 6.5\n49 1.80 6.7\n50 0.60 6.9\n51 2.00 7.1\n52 0.05 7.3\n53 0.40 7.3\n54 1.40 7.5\n55 2.20 7.5\n56 0.20 7.9\n57 1.60 7.9\n58 2.40 7.9\n59 0.05 8.1\n60 1.20 8.1\n61 1.40 8.1\n62 2.00 8.1\n63 1.60 8.5\n64 1.60 8.7\n65 1.00 8.9\n66 1.60 8.9\n67 2.40 9.1\n68 2.20 9.3\n69 1.80 9.5\n70 0.40 9.7\n\nnrow(mite)\n\n[1] 70"
  },
  {
    "objectID": "day_2.html#convergence-diagnostics",
    "href": "day_2.html#convergence-diagnostics",
    "title": "Day 2",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics"
  },
  {
    "objectID": "day_2.html#model-validation-with-simulation",
    "href": "day_2.html#model-validation-with-simulation",
    "title": "Day 2",
    "section": "Model validation with simulation",
    "text": "Model validation with simulation"
  },
  {
    "objectID": "day_2.html#dags",
    "href": "day_2.html#dags",
    "title": "Day 2",
    "section": "DAGs",
    "text": "DAGs\n\nforks, pipes and colliders\nsimulation from a DAG\ndemonstration of errors from disregarding DAGs"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-2",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-2",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nWe could sample the model parameters but how can we do this properly ?\nAny suggestions?"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-3",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-3",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIf we look at the estimated regression model coefficient, we can learn a few things\n\nsummaryReg$coefficients\n\n            Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 2.573889 0.04720304 54.52804 4.011925e-75\nhumidity    1.100865 0.07975800 13.80256 1.035796e-24\n\n\nNotably, there are uncertainty around the parameters.\nMaybe we can use this information to sample model parameters and reconstruct models across different iterations of parameters.\nLet’s give it a shot !"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-4",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-4",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIf we assume that the parameters of our particular model follow a Gaussian distribution, we can state that\n\\[\\beta_0 \\sim N(2.574, 0.047^2)\\] \\[\\beta_1 \\sim N(1.101, 0.080^2)\\]"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-5",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-5",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIn R, we can do this as follow\n\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta_0 <- rnorm(100, mean = regCoef[1,1], sd = regCoef[1,2])\nbeta_1 <- rnorm(100, mean = regCoef[2,1], sd = regCoef[2,2])"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-6",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-6",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-7",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-7",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nBut is this the right way to do it ?"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-8",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-8",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nActually, even if the model’s confidence interval look about right, they are wrong !"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-9",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-9",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nThe approach presented in the previous slide works only if we assume that the parameters are completely independent from one another.\nA situation that happens only in very specific circumstances.\nSo… We need to find a way to account for the non-independencies between the parameters.\nHow can we do this ? Any ideas ?"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-10",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-10",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\n\nAssuming that the regression parameters are normally distributed is not a bad assumption.\nHowever to consider a dependencies between the parameters we need to sample them from a multivariate normal distribution where the variance of each parameter and their dependency is defined by a covariance matrix estimated specifically for the data we model.\nThe good news is that this covariance matrix is given by summary.lm function\n\n\n(covReg <- summaryReg$cov.unscaled)\n\n              (Intercept)      humidity\n(Intercept)  0.0100098513 -0.0005305969\nhumidity    -0.0005305969  0.0285782940"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-11",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-11",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nFor our specific model, mathematically, we assume that\n\\[\\begin{bmatrix}\n  \\beta_0\\\\\n  \\beta_1\\\\\n\\end{bmatrix} \\sim MVN \\left( \\begin{bmatrix}\n  2.574\\\\\n  1.101\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0.0100 & -0.0005 \\\\\n  -0.0005 & 0.0286 \\\\\n\\end{bmatrix} \\right)\\]\nNote To present the multivariate normal distribution, we rely on matrix notation. This is our first introduction into matrix algebra. We will talk about this more into this course."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-12",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-12",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIn R, we can sample the parameters using a multivariate normal distribution using the following code\n\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta <- MASS::mvrnorm(100, regCoef[,1], Sigma = covReg)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-13",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-13",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-14",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-14",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters - Comparison"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-15",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-15",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSo far, we focused on the most commonly assessed regression parameters of the simple linear model, the slope and the intercept, but there is another one that is very important to consider, especially for this course.\nAny ideas which one it is ?"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-16",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-16",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nIf we go back to the mathematical description of the model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\nwe can see that in the simple linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\\[\\varepsilon \\sim N(0, \\sigma^2)\\] where \\(\\sigma^2\\) is an estimated variance.\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\nFor most the course, we will play with the variance parameter \\(\\sigma^2\\) in a bunch of different ways.\nBut before we do this, we need to understand a bit more about how this parameter influence the model."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-17",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-17",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\nIn essence, \\(\\sigma^2\\) tells us about what the model could not account for.\nFor example, let’s compare the biomass of Bidonia exemplaris with that of Ilovea chicktighii, another species (a carnivorous plant)"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#the-very-basics",
    "href": "slides/03_Matrix_algebra/index.html#the-very-basics",
    "title": "Matrix algebra",
    "section": "The very basics",
    "text": "The very basics\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#addition-and-substraction",
    "href": "slides/03_Matrix_algebra/index.html#addition-and-substraction",
    "title": "Matrix algebra",
    "section": "Addition and Substraction",
    "text": "Addition and Substraction\n\\[\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}\\] \\[C_{ij} = A_{ij} \\pm B_{ij}\\]\n\n\\[\\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} +\n            \\begin{bmatrix}\n                2 & 1\\\\\n                4 & -2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                3+2 & 5+1\\\\\n                1+4 & -2-2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                5 & 6\\\\\n                5 & -4\\\\\n            \\end{bmatrix}\\]\n\nIn R\n\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB <- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#multiplying-a-matrix-by-a-scalar",
    "href": "slides/03_Matrix_algebra/index.html#multiplying-a-matrix-by-a-scalar",
    "title": "Matrix algebra",
    "section": "Multiplying a matrix by a scalar",
    "text": "Multiplying a matrix by a scalar\n\\[\\mathbf{B} = c\\mathbf{A}\\] \\[B_{ij} = cA_{ij}\\]\n\n\\[\n            0.3 \\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} =  \n            \\begin{bmatrix}\n                0.9 & 1.5\\\\\n                0.3 & -0.6\\\\\n            \\end{bmatrix}\n\\]\n\nIn R\n\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc <- 0.3\nc*A \n\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#matrix-multiplications-not-divisions",
    "href": "slides/03_Matrix_algebra/index.html#matrix-multiplications-not-divisions",
    "title": "Matrix algebra",
    "section": "Matrix multiplications (not divisions!)",
    "text": "Matrix multiplications (not divisions!)\n\\[\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\]\n\\[C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}\\]\nRules\nAssociative: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nDistributive: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}\\)\nNot commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#inner-product",
    "href": "slides/03_Matrix_algebra/index.html#inner-product",
    "title": "Matrix algebra",
    "section": "Inner product",
    "text": "Inner product\n\\[(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j\\]\n\n\\[\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n\\]\n\nIn R\n\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx <- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n\n     [,1]\n[1,]   31\n[2,]   -8"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#dot-product",
    "href": "slides/03_Matrix_algebra/index.html#dot-product",
    "title": "Matrix algebra",
    "section": "Dot product",
    "text": "Dot product\n\\[\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n\\]\n\n\\[\n            \\begin{bmatrix}\n                3 & 1\\\\\n            \\end{bmatrix}\n            \\cdot\n            \\begin{bmatrix}\n                2\\\\ 5\\\\\n            \\end{bmatrix} =\n                3 \\times  2 + 1 \\times 5 = 11\n\\]\n\nIn R\n\nv <- c(3, 1)\nx <- c(2,5)\nsum(v * x)\n\n[1] 11"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation",
    "href": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation",
    "title": "Matrix algebra",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\\[\n\\begin{align*}\n        1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n        0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n        1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n    \\end{align*}\n\\] \\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation-1",
    "href": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation-1",
    "title": "Matrix algebra",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we mathematically solve for \\(\\boldsymbol{\\beta}\\)?\n\\[\n    \\begin{align*}\n        \\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n    \\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#eigenvectors-and-eigenvalues",
    "href": "slides/03_Matrix_algebra/index.html#eigenvectors-and-eigenvalues",
    "title": "Matrix algebra",
    "section": "Eigenvectors and eigenvalues",
    "text": "Eigenvectors and eigenvalues\n\nRight eigenvector is :\n\\[\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}\\] Left eigenvector is :\n\\[\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}\\]\nRules\n\n\\(\\mathbf{A}\\) has to be a square matrix\nIf \\(\\mathbf{w}\\) is an eigenvector of \\(\\mathbf{A}\\), so is \\(c\\mathbf{w}\\) for any value of \\(c \\neq0\\)\nThe right eigenvector of \\(\\mathbf{A}^T\\) is the left eigenvector of \\(\\mathbf{A}\\)\nEigenvectors are linearly independent"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation-2",
    "href": "slides/03_Matrix_algebra/index.html#solving-systems-of-linear-equation-2",
    "title": "Matrix algebra",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we solve for \\(\\boldsymbol{\\beta}\\) in R?\n\nX <- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny <- c(1, 0, 1)\n\n(beta <- solve(X, y))\n\n[1]  0.20000000  0.05714286 -0.02857143"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#usefulness-of-eigenvalues-for-this-course",
    "href": "slides/03_Matrix_algebra/index.html#usefulness-of-eigenvalues-for-this-course",
    "title": "Matrix algebra",
    "section": "Usefulness of eigenvalues for this course",
    "text": "Usefulness of eigenvalues for this course\nBeyond the technicality of extracting eigenvalues from a square matrix, as is explained in the previous slides"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#section",
    "href": "slides/03_Matrix_algebra/index.html#section",
    "title": "Matrix algebra",
    "section": "",
    "text": "Usefulness of eigenvalues for this course\nBeyond the technicality of extracting eigenvalues from a square matrix, as is explained in the previous slides"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#positive-definite-matrix",
    "href": "slides/03_Matrix_algebra/index.html#positive-definite-matrix",
    "title": "Matrix algebra",
    "section": "Positive definite matrix",
    "text": "Positive definite matrix\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\nError: Matrix X is not positive definite\n\nor similarly\n\nError: Matrix X is not positive semi-definite\n\nWhat does this mean ? Any idea ?"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#positive-semi-definite-matrix",
    "href": "slides/03_Matrix_algebra/index.html#positive-semi-definite-matrix",
    "title": "Matrix algebra",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nNerdy mathematical definition\nPositive definite matrix\n\\(\\mathbf{M}\\) is a positive definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} > 0\\)\nPositive semi-definite matrix\n\\(\\mathbf{M}\\) is a positive semi-definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0\\)"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#positive-semi-definite-matrix-1",
    "href": "slides/03_Matrix_algebra/index.html#positive-semi-definite-matrix-1",
    "title": "Matrix algebra",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nChecking if a matrix is positive (semi-)definite\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\nAll we have to do is look at the eigenvalue of a square matrix.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than 0, matrix \\(\\mathbf{M}\\) is positive definite.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than or equal ro 0, matrix \\(\\mathbf{M}\\) is positive semi-definite."
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#cholesky-decomposition",
    "href": "slides/03_Matrix_algebra/index.html#cholesky-decomposition",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\n\nThe Cholesky decomposition allows to decompose a matrix in a triangular, which, when multiplied by its transposed will allow us to recover the initial matrix.\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice\nIn math terms the Cholesky decomposition is defined as \\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\] Example\n\\[\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            1 & 5 & 5\\\\\n            1 & 5 & 14\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            1 & 0 & 0\\\\\n            1 & 2 & 0\\\\\n            1 & 2 & 3 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            0 & 2 & 2\\\\\n            0 & 0 & 3 \\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#determinant-of-a-matrix",
    "href": "slides/03_Matrix_algebra/index.html#determinant-of-a-matrix",
    "title": "Matrix algebra",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nto add stuff here\nto add stuff here\nto add stuff here"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-18",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-18",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nVariance of the model (\\(\\sigma^2\\))\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using summary.lm)\n\n# Regression model\nregBexemplaris <- lm(b.exemplaris ~ humidity)\nregIchicktighii <- lm(i.chicktighii ~ humidity)\n\n# Summary\nsummaryRegBexemplaris <- summary(regBexemplaris)\nsummaryRegIchicktighii <- summary(regIchicktighii)"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#estimating-regression-parameters-19",
    "href": "slides/02_Linear_model/index.html#estimating-regression-parameters-19",
    "title": "Linear models",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\n\nFor Bidonia exemplaris\n\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n\n            Estimate Std. Error\n(Intercept) 2.573889 0.04720304\nhumidity    1.100865 0.07975800\n\n# Estimated variance\nsummaryRegBexemplaris$sigma\n\n[1] 0.471798\n\n\n\nFor Ilovea chicktighii\n\n# Estimated coefficients\nsummaryRegIchicktighii$coefficients[,1:2]\n\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n\n# Estimated variance\nsummaryRegIchicktighii$sigma\n\n[1] 0.089239"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#limits-of-the-simple-linear-regression",
    "href": "slides/02_Linear_model/index.html#limits-of-the-simple-linear-regression",
    "title": "Linear models",
    "section": "Limits of the simple linear regression",
    "text": "Limits of the simple linear regression\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\nOne explanatory is almost never enough to approach biological questions nowadays.\nThe simple linear model assumes that the error follows a Gaussian distribution."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#limits-of-the-simple-linear-regression-1",
    "href": "slides/02_Linear_model/index.html#limits-of-the-simple-linear-regression-1",
    "title": "Linear models",
    "section": "Limits of the simple linear regression",
    "text": "Limits of the simple linear regression\nMultiple linear regression\n\nSimple linear regression can be extended to account for multiple explanatory variables to study more complexe problems. This type of regression model is known as a multiple linear regression.\nMathematically, a multiple linear regression can be defined as\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, in practice, matrix algebra is quite practical to use in this context and also for this course in genral.\nIn this respect, let’s take a bit of time to get acquinted with different basic (and maybe not so basic!) knowledge of matrix algebra."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#multiple-linear-regression-1",
    "href": "slides/02_Linear_model/index.html#multiple-linear-regression-1",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the \\(\\beta\\)s) can depend on other other data and parameters."
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#a-general-way-to-write-matrices",
    "href": "slides/03_Matrix_algebra/index.html#a-general-way-to-write-matrices",
    "title": "Matrix algebra",
    "section": "A general way to write matrices",
    "text": "A general way to write matrices\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\] \\[A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#square-matrix",
    "href": "slides/03_Matrix_algebra/index.html#square-matrix",
    "title": "Matrix algebra",
    "section": "Square matrix",
    "text": "Square matrix\nThe square matrix has as many rows at it has columns \\[\n\\mathbf{B} = \\begin{bmatrix}\n                B_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n                B_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#matrix-inversion",
    "href": "slides/03_Matrix_algebra/index.html#matrix-inversion",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix \\(\\mathbf{A}\\) is defined as \\(\\mathbf{A}^{-1}\\)\nAs such, \\[\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\] In R\n\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv <- solve(A))\n\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n\nA %*% Ainv\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#the-transpose-of-a-matrix",
    "href": "slides/03_Matrix_algebra/index.html#the-transpose-of-a-matrix",
    "title": "Matrix algebra",
    "section": "The transpose of a matrix",
    "text": "The transpose of a matrix\n\n\n\n\\[A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\\[A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\nIn R\n\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n\nt(A)\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#symmetric-matrix",
    "href": "slides/03_Matrix_algebra/index.html#symmetric-matrix",
    "title": "Matrix algebra",
    "section": "Symmetric matrix",
    "text": "Symmetric matrix\nThe values on the above and below the diagonal are match so that \\(A = A^t\\)\n\n\\[\n\\begin{bmatrix}\n                3 & 4 & -10\\\\\n                4 & 5 & 7\\\\\n                -10 & 7 & -6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#diagonal-matrix",
    "href": "slides/03_Matrix_algebra/index.html#diagonal-matrix",
    "title": "Matrix algebra",
    "section": "Diagonal matrix",
    "text": "Diagonal matrix\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n\\[D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}\\]\nAn example\n\n\\[\n\\begin{bmatrix}\n                -1 & 0 & 0\\\\\n                0 & 0 & 0\\\\\n                0 & 0 & 6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#identity-matrix",
    "href": "slides/03_Matrix_algebra/index.html#identity-matrix",
    "title": "Matrix algebra",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix is a square matrix where all values of its diagonal are 0 except the diagonal values which are all 1s.\n\n\n\\[\n\\mathbf{I}=\\begin{bmatrix}\n                1 & 0 & 0\\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\\\\\n        \\end{bmatrix}\n\\]\n\nThe identity matrix is important because\n\\[\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}\\] or\n\\[\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "href": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "title": "Matrix algebra",
    "section": "Eigenvectors and eigenvalues",
    "text": "Eigenvectors and eigenvalues\n\nRight eigenvector is :\n\\[\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}\\] Left eigenvector is :\n\\[\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}\\]\nRules\n\n\\(\\mathbf{A}\\) has to be a square matrix\nIf \\(\\mathbf{w}\\) is an eigenvector of \\(\\mathbf{A}\\), so is \\(c\\mathbf{w}\\) for any value of \\(c \\neq0\\)\nThe right eigenvector of \\(\\mathbf{A}^T\\) is the left eigenvector of \\(\\mathbf{A}\\)\nEigenvectors are linearly independent"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "title": "Matrix algebra",
    "section": "Positive definite matrix",
    "text": "Positive definite matrix\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\nError: Matrix X is not positive definite\n\nor similarly\n\nError: Matrix X is not positive semi-definite\n\nWhat does this mean ? Any idea ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "title": "Matrix algebra",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nNerdy mathematical definition\nPositive definite matrix\n\\(\\mathbf{M}\\) is a positive definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} > 0\\)\nPositive semi-definite matrix\n\\(\\mathbf{M}\\) is a positive semi-definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "title": "Matrix algebra",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nChecking if a matrix is positive (semi-)definite\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\nAll we have to do is look at the eigenvalue of a square matrix.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than 0, matrix \\(\\mathbf{M}\\) is positive definite.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than or equal ro 0, matrix \\(\\mathbf{M}\\) is positive semi-definite."
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#matrix-inversion-1",
    "href": "slides/03_Matrix_algebra/index.html#matrix-inversion-1",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\nInverting a diagonal matrix\n\\[D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#triangular-matrix",
    "href": "slides/03_Matrix_algebra/index.html#triangular-matrix",
    "title": "Matrix algebra",
    "section": "Triangular matrix",
    "text": "Triangular matrix\n\n\nLower triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}\\]\n\nUpper triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/03_Matrix_algebra/index.html#cholesky-decomposition-1",
    "href": "slides/03_Matrix_algebra/index.html#cholesky-decomposition-1",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\nWhy is it useful ?\nThere are actually two main reasons :\n\nWorking with triangular matrices is computationally more efficient\nIt can be used to rescale matrices and make MCMC algorithms converge more easily"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#multiple-linear-regression",
    "href": "slides/02_Linear_model/index.html#multiple-linear-regression",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nSimple linear regression can be extended to account for multiple explanatory variables to study more complexe problems. This type of regression model is known as a multiple linear regression.\nMathematically, a multiple linear regression can be defined as\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, in practice, matrix algebra is quite practical to use in this context and also for this course in genral.\nIn this respect, let’s take a bit of time to get acquinted with different basic (and maybe not so basic!) knowledge of matrix algebra."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#error-term-in-linear-models",
    "href": "slides/02_Linear_model/index.html#error-term-in-linear-models",
    "title": "Linear models",
    "section": "Error term in linear models",
    "text": "Error term in linear models\nAs previously mentioned, in (simple and multiple!) linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\\[\\varepsilon \\sim N(0, \\sigma^2)\\] where \\(\\sigma^2\\) is an estimated variance\nwhich means that the error linear regression follows a Gaussian distribution with an estimateed variance."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#generalized-linear-models",
    "href": "slides/02_Linear_model/index.html#generalized-linear-models",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nTo go around this problem, generalized linear models (GLMs) have been proposed. In essence, GLMs use link functions to adapt models for them to be used on non-Gaussian data.\nMathematically, the generic way to write generalized linear model is\n\\[\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n\\] or in matrix notation\n\\[\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n\\]\nwhere \\(g\\) is the link function and \\(g^{-1}\\) the inverse link function."
  },
  {
    "objectID": "slides/02_Linear_model/index.html#generalized-linear-models-1",
    "href": "slides/02_Linear_model/index.html#generalized-linear-models-1",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\n\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\nArguably the most common link function in ecology are\nlogit link function\nIt is commonly used for modelling binary (0-1) data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n\\] The inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n\\]"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#section-3",
    "href": "slides/02_Linear_model/index.html#section-3",
    "title": "Linear models",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/02_Linear_model/index.html#generalized-linear-models-2",
    "href": "slides/02_Linear_model/index.html#generalized-linear-models-2",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\n\nAnother commonly used link function is\nlog link function\nIt is commonly used for modelling count data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n\\] The inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#frequentist",
    "href": "slides/04_Bayesian/index.html#frequentist",
    "title": "Bayesian modelling",
    "section": "Frequentist",
    "text": "Frequentist\n\nIn introductory statistics course, it is common to rely on the frequentist paradigm when inferring results from data.\nFrequentists want to find the best model parameter(s) for the data at hand\n\\[\\text{Likelihood}\\hspace{1.5cm}P(\\text{Data}|\\text{Model})\\]\nThey are interested in maximizing the Likelihood\nThey need data\nEstimating model parameters\n\nMinimizing the sums of squares\nSimulated annealing\nNelder-Mead Simplex\n…"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#second",
    "href": "slides/04_Bayesian/index.html#second",
    "title": "Bayesian modelling",
    "section": "second",
    "text": "second\n\nTest your model"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#section",
    "href": "slides/04_Bayesian/index.html#section",
    "title": "Bayesian modelling",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#section-1",
    "href": "slides/04_Bayesian/index.html#section-1",
    "title": "Bayesian modelling",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#section-2",
    "href": "slides/04_Bayesian/index.html#section-2",
    "title": "Bayesian modelling",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#choose-parameters",
    "href": "slides/04_Bayesian/index.html#choose-parameters",
    "title": "Bayesian modelling",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#make-up-an-x-variable",
    "href": "slides/04_Bayesian/index.html#make-up-an-x-variable",
    "title": "Bayesian modelling",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#calculate-the-average",
    "href": "slides/04_Bayesian/index.html#calculate-the-average",
    "title": "Bayesian modelling",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#simulate-some-observations",
    "href": "slides/04_Bayesian/index.html#simulate-some-observations",
    "title": "Bayesian modelling",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#finally-visualize",
    "href": "slides/04_Bayesian/index.html#finally-visualize",
    "title": "Bayesian modelling",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#here-it-is-all-on-one-slide",
    "href": "slides/04_Bayesian/index.html#here-it-is-all-on-one-slide",
    "title": "Bayesian modelling",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/04_Bayesian/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Bayesian modelling",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#another-equation",
    "href": "slides/04_Bayesian/index.html#another-equation",
    "title": "Bayesian modelling",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#the-equation",
    "href": "slides/04_Bayesian/index.html#the-equation",
    "title": "Bayesian modelling",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#the-model",
    "href": "slides/04_Bayesian/index.html#the-model",
    "title": "Bayesian modelling",
    "section": "The model",
    "text": "The model\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#declare-the-data",
    "href": "slides/04_Bayesian/index.html#declare-the-data",
    "title": "Bayesian modelling",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#state-parameters",
    "href": "slides/04_Bayesian/index.html#state-parameters",
    "title": "Bayesian modelling",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#write-the-likelihood-and-priors",
    "href": "slides/04_Bayesian/index.html#write-the-likelihood-and-priors",
    "title": "Bayesian modelling",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#bayesian",
    "href": "slides/04_Bayesian/index.html#bayesian",
    "title": "Bayesian modelling",
    "section": "Bayesian",
    "text": "Bayesian\n\nBayesians want to find how good the model parameter(s) are given some data\n\\[\\text{Posterior}\\hspace{1.5cm}P(\\text{Model}|\\text{Data})\\]\nThey are interested in the posterior distribution\nThey need data and prior information\nThe general framework used in Bayesian modelling is\n\\[\\underbrace{P(\\text{Model}|\\text{Data})}_\\text{Posterior}\\propto \\underbrace{P(\\text{Data}|\\text{Model})}_\\text{Likelihood}\\underbrace{P(\\text{Model})}_\\text{Prior}\\]\nEstimating model parameters\n\nMarkov Chain Monte Carlo\nIntegrated nested Laplace approximation\n…"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#our-way-of-thinking-is-bayesian",
    "href": "slides/04_Bayesian/index.html#our-way-of-thinking-is-bayesian",
    "title": "Bayesian modelling",
    "section": "Our way of thinking is Bayesian",
    "text": "Our way of thinking is Bayesian"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#our-way-of-thinking-is-bayesian-1",
    "href": "slides/04_Bayesian/index.html#our-way-of-thinking-is-bayesian-1",
    "title": "Bayesian modelling",
    "section": "Our way of thinking is Bayesian",
    "text": "Our way of thinking is Bayesian"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#a-few-words-about-the-prior",
    "href": "slides/04_Bayesian/index.html#a-few-words-about-the-prior",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n\nDefinition of prior probability\nThe prior probability informes us about the probability of the model being true before the current data is considered.\nTypes of priors\n“Uninformative”\nThese priors are meant to bring very little information about the model\nInformative\nThese priors bring information about the model that is available\nConjugate\nThese priors have the same functional form (mathematically speaking) as the likelihood"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-1",
    "href": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-1",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n“Uninformative” priors\n\nExample If we have no idea of how elevation influence sugar maple\nGaussian distribution\n\n\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\\(\\mu = 0\\)\n\\(\\sigma = \\text{Large say 100}\\)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-2",
    "href": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-2",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nInformative priors\n\nExample If we know that\n\nThere are less sugar maples the higher we go\nThe influence of elevation on sugar maple cannot be more than two folds\n\nUniform distribution\n\n\n\\[f(x)=\\left\\{\n  \\begin{array}{cl}\n    \\frac{1}{b-a} & \\text{for } x\\in [a,b]\\\\\n    0 &\\text{otherwise}\\\\\n  \\end{array}\n\\right.\\]\n\n\\(a > -2\\)\n\\(b < 0\\)"
  },
  {
    "objectID": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-3",
    "href": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-3",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nThese types of priors are convenient to use because\n\nThey are computationally faster to use\nThey can be interepreted as additional data\n\nWhy are they useful?\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation."
  },
  {
    "objectID": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-4",
    "href": "slides/04_Bayesian/index.html#a-few-words-about-the-prior-4",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nWhat does it mean to be of the same functional form?\nIt means that both distribution have th same mathematical structure.\n\n\nBinomial distribution \\[\\theta^a(1-\\theta)^b\\]\n\nBeta distribution \\[\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\n\n\nhttps://en.wikipedia.org/wiki/Conjugate_prior"
  }
]