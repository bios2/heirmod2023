[
  {
    "objectID": "slides/01_Data/index.html#illustrative-datasets",
    "href": "slides/01_Data/index.html#illustrative-datasets",
    "title": "Data used for this course",
    "section": "Illustrative datasets",
    "text": "Illustrative datasets\nTo illustrate the different models and methods we will discuss in this course, we will rely on a few data sets, which are directly available in different R package\n\nmite, mite.env and mite.xy available in the vegan R package\npenguins available in the palmerpenguins R package\n\nThese datasets are practical because they are manageable in size and will allow you to see how to work out the different example presented in this course.\nLet’s look at them in more details"
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data",
    "href": "slides/01_Data/index.html#oribatid-mite-data",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nAside from being a very interesting datasets this data has been sample on the southern shore of Lac Geai (a few minutes walk from here ! We will go see it this week)\nSampling was carried out in June 1989 on the partially floating vegetation mat surrounding the lake from the forest border to the free water by Daniel Borcard"
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data-1",
    "href": "slides/01_Data/index.html#oribatid-mite-data-1",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nOribatid mites are small (usually ranging in size from 0.2 to 1.4 mm) invertebrates that are part of the Arachnida class (so they have 8 legs).\n\nIn the mite data, 35 morphospecies were identified and counted across 70 samples."
  },
  {
    "objectID": "slides/01_Data/index.html#sites-coordinates",
    "href": "slides/01_Data/index.html#sites-coordinates",
    "title": "Data used for this course",
    "section": "Sites coordinates",
    "text": "Sites coordinates"
  },
  {
    "objectID": "slides/01_Data/index.html#vegetation-cover",
    "href": "slides/01_Data/index.html#vegetation-cover",
    "title": "Data used for this course",
    "section": "Vegetation cover",
    "text": "Vegetation cover\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "href": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "title": "Data used for this course",
    "section": "Microtopography and shrub cover",
    "text": "Microtopography and shrub cover\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/01_Data/index.html#substrate-density-and-water-content",
    "href": "slides/01_Data/index.html#substrate-density-and-water-content",
    "title": "Data used for this course",
    "section": "Substrate density and water content",
    "text": "Substrate density and water content\n{fig-align=“center” width = 105%}"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#complexe-hierarchical-model",
    "href": "slides/07_Complex_hierarchical_model/index.html#complexe-hierarchical-model",
    "title": "Complex hierarchical models",
    "section": "“Complexe” hierarchical model",
    "text": "“Complexe” hierarchical model"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#the-general-formulation",
    "href": "slides/07_Complex_hierarchical_model/index.html#the-general-formulation",
    "title": "Complex hierarchical models",
    "section": "The general formulation",
    "text": "The general formulation\n\nAs discuss yesterday, a linear model can be writen as\n\\[\\mathbf{y}\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2\\mathbf{I})\\]\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory varaibles)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#a-very-general-formulation",
    "href": "slides/07_Complex_hierarchical_model/index.html#a-very-general-formulation",
    "title": "Complex hierarchical models",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})\\]\n\n\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is another matrix of explanatory variables with \\(n\\) rows (samples) and \\(q\\) columns (explanatory variables)\n\\(\\mathbf{b}\\) is a vector \\(q\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{Z}\\)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#a-very-general-formulation-1",
    "href": "slides/07_Complex_hierarchical_model/index.html#a-very-general-formulation-1",
    "title": "Complex hierarchical models",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})\\] What is also noticeable in this model is the conditional relationship between \\(\\mathbf{y}\\) and \\(\\mathbf{b}\\).\nSpecifically, in this formulation,\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] where \\(\\mathbf{\\Sigma}\\) is a covariance matrix.\nBased on this general formulation, we can now define all unconstrained hierarchical models."
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-1-fg",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-1-fg",
    "title": "Complex hierarchical models",
    "section": "y ~ (1 | f/g)",
    "text": "y ~ (1 | f/g)\n\nOther notation used : (1 | f) + (1 | f:g)\nThis model assumes there is a hierarchy that varies among the levels of factor f and among the levels of factor g but only within the levels of factor f.\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f[l_f]} + \\mathbf{b}_{g[l_g]\\in f[l_f]},\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,l_g = 1\\dots k_g\\] or\n\\[y_i = b_{f[l_f]} + b_{g[l_g]\\in f[l_f]} + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,,\\,\\,\\,l_g = 1\\dots k_g\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In words, this means that the model values \\(b\\) will change for a sample \\(i\\) only when the level \\(l_f\\) of the factor \\(f\\) changes or when the level \\(l_g\\) of the factor \\(g\\) within the level \\(l_f\\) of the factor \\(f\\) changes.\nThis is because in this model\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(0, \\begin{bmatrix}\n                                  \\sigma^2_f & 0\\\\\n                                  0& \\sigma^2_{g\\in f}\\\\\n                                \\end{bmatrix}\n                                \\right)\\]"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-1-fg-1",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-1-fg-1",
    "title": "Complex hierarchical models",
    "section": "y ~ (1 | f/g)",
    "text": "y ~ (1 | f/g)\nStan code for this model\n\nFor Andrew?"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-1-fg-2",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-1-fg-2",
    "title": "Complex hierarchical models",
    "section": "y ~ (1 | f/g)",
    "text": "y ~ (1 | f/g)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-1-f-1-g",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-1-f-1-g",
    "title": "Complex hierarchical models",
    "section": "y ~ (1 | f) + (1 | g)",
    "text": "y ~ (1 | f) + (1 | g)\n\nOther notation used : y ~ 1 + (1 | f) + (1 | g)\nThis model assumes there is a hierarchy that varies among the two factors.\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f[l_f]\\times g[l_g]},\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,l_g = 1\\dots k_g\\] or\n\\[y_i = b_{f_i[l_f]\\times g_i[l_g]} + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,,\\,\\,\\,l_g = 1\\dots k_g\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In words, this means that the model values \\(b\\) will change for a sample \\(i\\) only when the interaction the level \\(l_f\\) of the factor \\(f\\) and the level \\(l_g\\) of the factor \\(g\\) changes.\nThis is because in this model\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(0,\n                                \\begin{bmatrix}\n                                  \\sigma^2_f & \\sigma_f\\sigma_g\\\\\n                                  \\sigma_f\\sigma_g& \\sigma^2_g\\\\\n                                \\end{bmatrix}\n                                \\right)\\]"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-1-f-1-g-1",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-1-f-1-g-1",
    "title": "Complex hierarchical models",
    "section": "y ~ (1 | f) + (1 | g)",
    "text": "y ~ (1 | f) + (1 | g)\nThis model has a number interesting properties\n\nIt does not assumes that the two factors act independent. Actually, if you are interested in such a model, the code to use is not as straight forward to write with these packages.\nIf a particular levels is associated to the same samples for the two factors, usually this create technical problems and the model cannot be estimated properly (this is true regardless of how you estimate these parameter)\nThis can be generalized to as many factors as we want. We will see how this can be useful later."
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#y-x-x-g",
    "href": "slides/07_Complex_hierarchical_model/index.html#y-x-x-g",
    "title": "Complex hierarchical models",
    "section": "y ~ x + (x || g)",
    "text": "y ~ x + (x || g)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#second",
    "href": "slides/07_Complex_hierarchical_model/index.html#second",
    "title": "Complex hierarchical models",
    "section": "second",
    "text": "second\n\nTest your model"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#section",
    "href": "slides/07_Complex_hierarchical_model/index.html#section",
    "title": "Complex hierarchical models",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#section-1",
    "href": "slides/07_Complex_hierarchical_model/index.html#section-1",
    "title": "Complex hierarchical models",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#section-2",
    "href": "slides/07_Complex_hierarchical_model/index.html#section-2",
    "title": "Complex hierarchical models",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#choose-parameters",
    "href": "slides/07_Complex_hierarchical_model/index.html#choose-parameters",
    "title": "Complex hierarchical models",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#make-up-an-x-variable",
    "href": "slides/07_Complex_hierarchical_model/index.html#make-up-an-x-variable",
    "title": "Complex hierarchical models",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#calculate-the-average",
    "href": "slides/07_Complex_hierarchical_model/index.html#calculate-the-average",
    "title": "Complex hierarchical models",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#simulate-some-observations",
    "href": "slides/07_Complex_hierarchical_model/index.html#simulate-some-observations",
    "title": "Complex hierarchical models",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#finally-visualize",
    "href": "slides/07_Complex_hierarchical_model/index.html#finally-visualize",
    "title": "Complex hierarchical models",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#here-it-is-all-on-one-slide",
    "href": "slides/07_Complex_hierarchical_model/index.html#here-it-is-all-on-one-slide",
    "title": "Complex hierarchical models",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/07_Complex_hierarchical_model/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Complex hierarchical models",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#another-equation",
    "href": "slides/07_Complex_hierarchical_model/index.html#another-equation",
    "title": "Complex hierarchical models",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#the-equation",
    "href": "slides/07_Complex_hierarchical_model/index.html#the-equation",
    "title": "Complex hierarchical models",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#the-model",
    "href": "slides/07_Complex_hierarchical_model/index.html#the-model",
    "title": "Complex hierarchical models",
    "section": "The model",
    "text": "The model\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#declare-the-data",
    "href": "slides/07_Complex_hierarchical_model/index.html#declare-the-data",
    "title": "Complex hierarchical models",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#state-parameters",
    "href": "slides/07_Complex_hierarchical_model/index.html#state-parameters",
    "title": "Complex hierarchical models",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/07_Complex_hierarchical_model/index.html#write-the-likelihood-and-priors",
    "href": "slides/07_Complex_hierarchical_model/index.html#write-the-likelihood-and-priors",
    "title": "Complex hierarchical models",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#simple-hierarchical-model",
    "href": "slides/06_Simple_hierarchical_model/index.html#simple-hierarchical-model",
    "title": "‘Simple’ hierarchical models",
    "section": "“Simple” hierarchical model",
    "text": "“Simple” hierarchical model\nHere, we use the term “simple” in a rather loose way to discuss hierarchical models without any constrains, whether they are spatial, temporal, phylogenetic or others.\nFuthermore, for most of this lecture, we will focus on models with a Gaussian error term to develop the underlying theory.\nWhen we will have done this, it will be reasonably straight forward to move to non-Gaussian hierarchical model."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation",
    "title": "‘Simple’ hierarchical models",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nAs discuss yesterday, a linear model can be writen as\n\\[(\\mathbf{y}|\\mathbf{X}, \\boldsymbol{\\beta}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma\\mathbf{y}^2\\mathbf{I})\\]\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory varaibles)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation-1",
    "title": "‘Simple’ hierarchical models",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is another matrix of explanatory variables with \\(n\\) rows (samples) and \\(q\\) columns (explanatory variables)\n\\(\\mathbf{b}\\) is a vector \\(q\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{Z}\\)"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-very-general-formulation-2",
    "title": "‘Simple’ hierarchical models",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})\\] What is also noticeable in this model is the conditional relationship between \\(\\mathbf{y}\\) and \\(\\mathbf{b}\\).\nSpecifically, in this formulation,\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] where \\(\\mathbf{\\Sigma}\\) is a covariance matrix.\nBased on this general formulation, we can now define all unconstrained hierarchical models."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#the",
    "href": "slides/06_Simple_hierarchical_model/index.html#the",
    "title": "‘Simple’ hierarchical models",
    "section": "The “|”",
    "text": "The “|”\n\nMost of you have probably already used the packages lme4 or brms to build hierarchical models and so you have used the | to include a hierachy in your model.\nBut do you know what the underlying mathematical structure of the model you built look like ? Does it really answer the question you were asking ?\n\n\n\nLet’s look at different lme4 models to learn about some basic (and not so basic!) hierarchical models."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation",
    "title": "‘Simple’ hierarchical models",
    "section": "A bit of notation",
    "text": "A bit of notation\nBefore we get into writing math, we need to define a bit of notation in addition of the one we have used so far.\nSpecifically, when define a hierarchy in a model, it is common to do this using at least one factor. Mathematically, we will define the different level of a factor in a model by a subscript.\nWe will use square brackets to define the level of interest\nExample\n\\[\\mathbf{Z}_{f[l]}\\] This means that, within \\(\\mathbf{Z}\\), we focus on \\(l^{\\text{th}}\\) level of factor \\(f\\)."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept\n\nlme4 notation used y ~ (1 | f) or y ~ 1 + (1 | f)\nThis model assumes there is a hierarchy solely on the intercept.\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f[l]},\\sigma^2\\mathbf{I}) \\quad \\forall\\quad l = 1\\dots k\\] or\n\\[y_i = b_{{f_i[l]}} + \\varepsilon \\quad \\forall\\quad l = 1\\dots k\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In words, this means that the model values \\(b\\) will change for a sample \\(i\\) only when the level \\(l\\) of the factor \\(f\\) changes.\nThis is because in this model\n\\[\\mathbf{b} \\sim \\mathcal{N}(0, \\sigma^2_f)\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-2",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slopes",
    "text": "Hierarchy on the slopes\n\nlme4 notation : y ~ 1 + (x | f)\nThis model assumes there is a hierarchy on the parameters associated to variable x.\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\beta_0 + \\mathbf{Z}\\mathbf{b}_{f[l_f]},\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\] or\n\\[y_i = \\beta_0 + b_{f[l_f]}z_i + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In words, this means that the weight on variable \\(z\\) will change for a sample \\(i\\) only when the level \\(l\\) of the factor \\(f\\) changes.\nThis is because in this model\n\\[\\mathbf{b} \\sim \\mathcal{N}(0, \\sigma^2_f)\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slopes",
    "text": "Hierarchy on the slopes"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-2",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slopes",
    "text": "Hierarchy on the slopes"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on intercept and slope",
    "text": "Hierarchy on intercept and slope\nMathematically speaking, what are the differences between having a hierarchy on the intercept and a slope ? Any idea ?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on intercept and slope",
    "text": "Hierarchy on intercept and slope\nAnswer : Very little !\n\nActually, if we return the way \\(\\mathbf{b}\\) is defined we see that in both case it is defined as\n\\[\\mathbf{b} \\sim \\mathcal{N}(0, \\sigma^2_f)\\] with the sole difference that \\(\\mathbf{b}\\) linked to an explanatory variable when the hierarchy is on the slope, while when the hierarchy is on the intercept it is not linked to any explanatory variable.\nWell… Actually… When a hierarchy is applied on the intercept it is technically associated to a constant explanatory variable."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nA common question that often gets asked is :\n“How many level is enough ?”\nThis is a simple questions that sadly does not have a simple answer."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-1",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nIn these types of models we are interested in estimating the variance parameter \\(\\sigma^2_f\\) in\n\\[\\mathbf{b} \\sim \\mathcal{N}(0, \\sigma^2_f)\\] to get the best estimation of \\(\\mathbf{b}\\).\nSo, another way to ask this question is “What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?”\nHowever, in the context of how we defined hierarchical models, a sample amounts to being the level of a factor."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-2",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nIs 3 enough ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.234"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-3",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-3",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nMaybe 5 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.12"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-4",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-4",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 10 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.174"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-5",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-5",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 50 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.331"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-6",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-6",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 100 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.271"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-7",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-7",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 1000 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.251"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-8",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-8",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nThere is a consensus among researchers working intimately with hierarchical that when the interest is to properly estimate the variance parameter \\(\\sigma^2\\), 5 or 6 levels is the extreme minimum.\n\nIn the book Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects, James S. Hodges (2016) makes this very thoughtful statement :\n\n“Treating factors with small numbers of levels as random will in the best case lead to very small and/or imprecise estimates of random effects; in the worst case it will lead to various numerical difficulties such as lack of convergence, zero variance estimates, etc.”"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-9",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-9",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nSo, what to do if the number of level is not high enough for your comfort ?\nYou can still use the hierarchy in your model but focus on the mean of the levels instead of the variance.\nHow does this translate mathematically, with what we have seen so far ?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol{\\beta}_{f[l]},\\sigma_\\mathbf{y}^2\\mathbf{I}) \\quad \\forall\\quad l = 1\\dots k\\] or\n\\[y_i = \\beta_{f_i[l]} + \\varepsilon \\quad \\forall\\quad l = 1\\dots k\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In this model, we assume that \\(\\boldsymbol{\\beta}_{f[l]}\\) is distributed as\n\\[\\boldsymbol{\\beta}_{f[l]} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\] In words, this means that all the samples among the \\(l^\\text{th}\\) level of factor \\(f\\) are used to estimate \\(\\boldsymbol{\\beta}_{f[l]}\\).\nBy developping our model this way, we focus on estimating the mean of groups in the hierarchy instead of only the variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nLet’s take a deeper look at \\[\\boldsymbol{\\beta}_{f[l]} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\] When we study this way of sampling \\(\\boldsymbol{\\beta}_{f[l]}\\), although our interest is more on \\(\\mu_{f}\\), we also have to estimate the variance term \\(\\sigma^2_{f}\\).\nNote: This is essentially the same thing as a one-way analysis of variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-2",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-3",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-3",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nHowever, it can be assumed to be all the same variance regardless of the group considered\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n        \\end{bmatrix}\\]\n\nIn this case, \\(\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\) can be rewritten as \\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})\\] Note: This is essentially the same thing as a one-way analysis of variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-4",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-4",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-5",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-5",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope’s mean",
    "text": "Hierarchy on the slope’s mean\n\nDevelopping a hierarchy on the slope’s mean translate mathematically in a very similar way as it does for the intercept.\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol{\\beta}_0+\\mathbf{X}\\boldsymbol{\\beta}_{f[l]},\\sigma_\\mathbf{y}^2\\mathbf{I}) \\quad \\forall\\quad l = 1\\dots k\\] or\n\\[y_i = \\beta_0 + \\beta_{f_i[l]}x_i + \\varepsilon \\quad \\forall\\quad l = 1\\dots k\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n\\] In this model, we assume that \\(\\boldsymbol{\\beta}_{f[l]}\\) is distributed as\n\\[\\boldsymbol{\\beta}_{f[l]} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\] In words, this means that all the samples among the \\(l^\\text{th}\\) level of factor \\(f\\) are used to estimate \\(\\boldsymbol{\\beta}_{f[l]}\\).\nBy developping our model this way, we focus on estimating the average slope for each group in the hierarchy instead of only the variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope’s mean",
    "text": "Hierarchy on the slope’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-2",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope’s mean",
    "text": "Hierarchy on the slope’s mean\nStan code\n\nFor Andrew?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#tracking-the-estimated-parameters",
    "href": "slides/06_Simple_hierarchical_model/index.html#tracking-the-estimated-parameters",
    "title": "‘Simple’ hierarchical models",
    "section": "Tracking the estimated parameters",
    "text": "Tracking the estimated parameters\n\nAs can be seen, it is important in hierarchical model to track the different parameters that are estimated to make sure we can make proper inferences with our model.\nHowever, we need to be careful because the notation used can play trick on us. This is especially true when using matrix notation.\nFor example, in\n\\[\\mathbf{b}_{f}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] the number of levels are not explicitly defined and it is not clear if $ $ includes the same variance value on the diagonal or different ones and whether the off diagonal elements are 0 or another values.\nIn any case, make sure to keep track of the estimated parameters so that you can better understand the limits of the model you are building and using."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#choosing-the-right-model",
    "href": "slides/06_Simple_hierarchical_model/index.html#choosing-the-right-model",
    "title": "‘Simple’ hierarchical models",
    "section": "Choosing the right model",
    "text": "Choosing the right model\nAlthough these different models are mathematically quite similar, they approach very different biological questions.\nA comparison of the different figures caricaturizing how each model works should give a good insight about what each model can do.\nIt is thus important to make sure you design your biological question well so that deciding on which model to use is reasonably straight forward."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#move-this-in-another-place",
    "href": "slides/06_Simple_hierarchical_model/index.html#move-this-in-another-place",
    "title": "‘Simple’ hierarchical models",
    "section": "Move this in another place",
    "text": "Move this in another place\nTechnically, we can sample all \\(\\boldsymbol{\\beta}_{f[l]}\\) independently, however, using multivariate Gaussian distribution, we can sample the \\(\\boldsymbol{\\beta}_{f}\\) for all levels of the factor in one go as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\] where\n\n\\(\\boldsymbol{\\mu}_{f}\\) is a vector of \\(k\\) means, one for each level of the factor\n\\(\\mathbf{D}_f\\) is a \\(k\\times k\\) diagonal matrix with variance term on the diagonal"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#square-matrix",
    "href": "slides/nn_extra_slides/index.html#square-matrix",
    "title": "Extra stuff",
    "section": "Square matrix",
    "text": "Square matrix\nThe square matrix has as many rows at it has columns \\[\n\\mathbf{B} = \\begin{bmatrix}\n                B_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n                B_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "href": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "title": "Extra stuff",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nNot sure if it should be included or not"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "href": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "title": "Extra stuff",
    "section": "Eigenvectors and eigenvalues",
    "text": "Eigenvectors and eigenvalues\n\nRight eigenvector is :\n\\[\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}\\] Left eigenvector is :\n\\[\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}\\]\nRules\n\n\\(\\mathbf{A}\\) has to be a square matrix\nIf \\(\\mathbf{w}\\) is an eigenvector of \\(\\mathbf{A}\\), so is \\(c\\mathbf{w}\\) for any value of \\(c \\neq0\\)\nThe right eigenvector of \\(\\mathbf{A}^T\\) is the left eigenvector of \\(\\mathbf{A}\\)\nEigenvectors are linearly independent"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive definite matrix",
    "text": "Positive definite matrix\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\nError: Matrix X is not positive definite\n\nor similarly\n\nError: Matrix X is not positive semi-definite\n\nWhat does this mean ? Any idea ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nNerdy mathematical definition\nPositive definite matrix\n\\(\\mathbf{M}\\) is a positive definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} > 0\\)\nPositive semi-definite matrix\n\\(\\mathbf{M}\\) is a positive semi-definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nChecking if a matrix is positive (semi-)definite\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\nAll we have to do is look at the eigenvalue of a square matrix.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than 0, matrix \\(\\mathbf{M}\\) is positive definite.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than or equal ro 0, matrix \\(\\mathbf{M}\\) is positive semi-definite."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#dot-product",
    "href": "slides/nn_extra_slides/index.html#dot-product",
    "title": "Extra stuff",
    "section": "Dot product",
    "text": "Dot product\n\\[\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n\\]\n\n\\[\n            \\begin{bmatrix}\n                3 & 1\\\\\n            \\end{bmatrix}\n            \\cdot\n            \\begin{bmatrix}\n                2\\\\ 5\\\\\n            \\end{bmatrix} =\n                3 \\times  2 + 1 \\times 5 = 11\n\\]\n\nIn R\n\nv <- c(3, 1)\nx <- c(2,5)\nsum(v * x)\n\n[1] 11"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\\[\n\\begin{align*}\n        1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n        0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n        1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n    \\end{align*}\n\\] \\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we mathematically solve for \\(\\boldsymbol{\\beta}\\)?\n\\[\n    \\begin{align*}\n        \\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n    \\end{align*}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we solve for \\(\\boldsymbol{\\beta}\\) in R?\n\nX <- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny <- c(1, 0, 1)\n\n(beta <- solve(X, y))\n\n[1]  0.20000000  0.05714286 -0.02857143"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nThese types of priors are convenient to use because\n\nThey are computationally faster to use\nThey can be interepreted as additional data\n\nWhy are they useful?\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nWhat does it mean to be of the same functional form?\nIt means that both distribution have th same mathematical structure.\n\n\nBinomial distribution \\[\\theta^a(1-\\theta)^b\\]\n\nBeta distribution \\[\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\n\n\nhttps://en.wikipedia.org/wiki/Conjugate_prior"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "href": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "title": "Extra stuff",
    "section": "Move this in another place",
    "text": "Move this in another place\nTechnically, we can sample all \\(\\boldsymbol{\\beta}_{f[l]}\\) independently, however, using multivariate Gaussian distribution, we can sample the \\(\\boldsymbol{\\beta}_{f}\\) for all levels of the factor in one go as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\] where\n\n\\(\\boldsymbol{\\mu}_{f}\\) is a vector of \\(k\\) means, one for each level of the factor\n\\(\\mathbf{D}_f\\) is a \\(k\\times k\\) diagonal matrix with variance term on the diagonal"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nThe structure of matrix \\(\\mathbf{D}_f\\) can be considered in two different ways in\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\]\nWritten in the general form as we did in the equation above, we assume that all variance on the diagonal are potentially different. Or in other words, the variance in each group is assumed to be different\n\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f[1]} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f[2]} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f[l]} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f[k]}\\\\\n        \\end{bmatrix}\\]"
  },
  {
<<<<<<< HEAD
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nHowever, it can be assumed to be all the same variance regardless of the group considered\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n        \\end{bmatrix}\\]\n\nIn this case, \\(\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\) can be rewritten as \\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})\\] Note: This is essentially the same thing as a one-way analysis of variance."
=======
    "objectID": "slides/template/index.html#section",
    "href": "slides/template/index.html#section",
    "title": "Template for presentations",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/template/index.html#section-1",
    "href": "slides/template/index.html#section-1",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#section-2",
    "href": "slides/template/index.html#section-2",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#choose-parameters",
    "href": "slides/template/index.html#choose-parameters",
    "title": "Template for presentations",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5"
  },
  {
    "objectID": "slides/template/index.html#make-up-an-x-variable",
    "href": "slides/template/index.html#make-up-an-x-variable",
    "title": "Template for presentations",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/template/index.html#calculate-the-average",
    "href": "slides/template/index.html#calculate-the-average",
    "title": "Template for presentations",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x"
  },
  {
    "objectID": "slides/template/index.html#simulate-some-observations",
    "href": "slides/template/index.html#simulate-some-observations",
    "title": "Template for presentations",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/template/index.html#finally-visualize",
    "href": "slides/template/index.html#finally-visualize",
    "title": "Template for presentations",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#here-it-is-all-on-one-slide",
    "href": "slides/template/index.html#here-it-is-all-on-one-slide",
    "title": "Template for presentations",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Template for presentations",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#another-equation",
    "href": "slides/template/index.html#another-equation",
    "title": "Template for presentations",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-equation",
    "href": "slides/template/index.html#the-equation",
    "title": "Template for presentations",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-model",
    "href": "slides/template/index.html#the-model",
    "title": "Template for presentations",
    "section": "The model",
    "text": "The model\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#declare-the-data",
    "href": "slides/template/index.html#declare-the-data",
    "title": "Template for presentations",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#state-parameters",
    "href": "slides/template/index.html#state-parameters",
    "title": "Template for presentations",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#write-the-likelihood-and-priors",
    "href": "slides/template/index.html#write-the-likelihood-and-priors",
    "title": "Template for presentations",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html",
    "href": "topics/01_data_simulation/data_simulation.html",
    "title": "Data simulation",
    "section": "",
    "text": "Why would you want to? there are a few good reasons:\n\nUnderstand your priors. By simulating data from a model we get an idea of what priors actually mean scientifically. With all but the simplest models, this is essential\nDemonstrate your understanding of the model. If you can’t simulate data from a model, you probably don’t understand it!\nValidate that the model works correctly. If you can recover parameters when you know the truth, then we have more confidence that it will work correctly on real data."
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html#watching-for-birds",
    "href": "topics/01_data_simulation/data_simulation.html#watching-for-birds",
    "title": "Data simulation",
    "section": "Watching for birds",
    "text": "Watching for birds\nLet’s start by simulating a simple dataset with one parameter: the number of birds each of us is going to see on a hike today.\nWhat kind of numbers do we expect to get? what is a reasonable limit to how many we would see?\n\nSimulation in R\nlet’s simulate from a poisson distribution. As you probably know, the poisson in R is just rpois. Every statistical distribution that is in R (which is a lot! almost all! ) has four functions. For a distribution called dist, they are:\n\nrdist = the distribution functions\nqdist = the quantile functions\npdist = the probability density function\nddist the density function\n\nLet’s begin by simulating data in R\n\nn_people <- 23\nobservations <- rpois(n_people, lambda = 20)\n\nhist(observations)\n\n\n\n\nWe can do the same process in the programming language Stan\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/amacdonald/.cmdstan/cmdstan-2.31.0\n\n\n- CmdStan version: 2.31.0\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\npoisson_simulation <- cmdstan_model(stan_file = \"topics/01_data_simulation/poisson_simulation.stan\")\n\npoisson_simulation\n\ndata {\n  int<lower=0> n_people;\n  real avg_observed;\n}\ngenerated quantities {\n  array[n_people] int<lower=0> observations;\n  \n  for (i in 1:n_people){\n    observations[i] = poisson_rng(avg_observed);\n  }\n}\n\n\n\ncompare and contrast the R and Stan formulations\nintro to Stan syntax\n\n\npoisson_simulation$sample(data = list(n_people = 23,\n                                      avg_observed = 19.5),\n                          fixed_param = TRUE)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Sampling) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Sampling) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Sampling) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Sampling) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Sampling) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:   1 / 1000 [  0%]  (Sampling) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Sampling) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Sampling) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:   1 / 1000 [  0%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Sampling) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Sampling) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Sampling) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:   1 / 1000 [  0%]  (Sampling) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Sampling) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Sampling) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Sampling) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\n\n         variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n observations[1]  19.50  19.00 4.44 4.45 13.00 27.00 1.00     3717     3780\n observations[2]  19.60  19.00 4.49 4.45 13.00 27.00 1.00     3866     3788\n observations[3]  19.47  19.00 4.42 4.45 12.00 27.00 1.00     3845     3669\n observations[4]  19.57  19.00 4.37 4.45 13.00 27.00 1.00     3968     3943\n observations[5]  19.43  19.00 4.42 4.45 13.00 27.00 1.00     4008     3950\n observations[6]  19.50  19.00 4.40 4.45 13.00 27.00 1.00     3991     3820\n observations[7]  19.50  19.00 4.43 4.45 13.00 27.00 1.00     3971     3947\n observations[8]  19.59  19.00 4.33 4.45 13.00 27.00 1.00     4057     3956\n observations[9]  19.55  19.00 4.40 4.45 13.00 27.00 1.00     3980     4031\n observations[10] 19.51  19.00 4.32 4.45 13.00 27.00 1.00     3932     4028\n\n # showing 10 of 23 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)"
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html#write-down-model",
    "href": "topics/01_data_simulation/data_simulation.html#write-down-model",
    "title": "Data simulation",
    "section": "Write down model",
    "text": "Write down model"
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html#fit-model-to-a-simulation",
    "href": "topics/01_data_simulation/data_simulation.html#fit-model-to-a-simulation",
    "title": "Data simulation",
    "section": "Fit model to a simulation",
    "text": "Fit model to a simulation"
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html#learning-how-to-plot-a-posterior",
    "href": "topics/01_data_simulation/data_simulation.html#learning-how-to-plot-a-posterior",
    "title": "Data simulation",
    "section": "learning how to plot a posterior",
    "text": "learning how to plot a posterior\nNow we can move on to the second step outlined above: we can just fit the same data to our model, vice versa, and see if we can recover that parameter."
  },
  {
    "objectID": "topics/01_data_simulation/data_simulation.html#r-based-alternatives",
    "href": "topics/01_data_simulation/data_simulation.html#r-based-alternatives",
    "title": "Data simulation",
    "section": "R based alternatives",
    "text": "R based alternatives\nIn R, there are several ways to do it: first, we can use R do it in two ways: fitdistr, and glm.\nThen we do the same thing in Stan.\nThen we look to see if we have recovered our parameter.\nThe next steop in visualization, which we also do with this simple model."
  },
  {
    "objectID": "topics/01_simulation/index.html",
    "href": "topics/01_simulation/index.html",
    "title": "Data simulation",
    "section": "",
    "text": "Before starting work on real data, we are going to begin by learning how to make up some of our own. There are at least three reasons why this is a good idea:"
  },
  {
    "objectID": "topics/01_simulation/index.html#simple-exercise-in-simulation",
    "href": "topics/01_simulation/index.html#simple-exercise-in-simulation",
    "title": "Data simulation",
    "section": "Simple exercise in simulation",
    "text": "Simple exercise in simulation\nLet’s imagine we are taking a walk as a group today at the beautiful SBL. What is the number of birds each of us is going to see on our hike?\n\n\n\n\n\n\nNote\n\n\n\nSimulating data really helps a scientist to ask an important question: “Where do my numbers come from?”. What kind of numbers do we expect to get? Do they have an upper limit? a lower limit? What kind of observation would be a little suprising? VERY surprising?\n\n\n\nWhat is the process\nWe might imagine that each one of us is likely to see about the same number of birds. We know our data (number of birds) is going to be a positive integer: 0 or more birds\n\\[\n\\begin{align}\n\\text{Number of Birds} &\\sim \\text{Poisson}(\\lambda) \\\\\n\\lambda &= 20\n\\end{align}\n\\]\n$$ \\[\\begin{align}\n\\text{Number of Birds} &\\sim \\text{Poisson}(\\lambda_i) \\\\\n\\lambda_i &= \\mu \\\\\n\n\\end{align}\\] $$\n\nabd ~ 1 + (1 | person)\n\nabd ~ 1 + (1 | person)\n\n\n\n\nSimulation in R\nlet’s simulate from a poisson distribution.\n\nset.seed(1234)\nn_people <- 21\nrpois(n_people, lambda = 20)\n\n [1] 14 21 21 16 22 17 24 17 17 12 20 24 19 17 23 17 20 17 20 19 23\n\n\nas you probably know, to draw random numbers in R we use the function rpois\nEvery statistical distribution that is in R (which is a lot! almost all! ) has a distirbution has four functions. of the distribution is called dist, then they are:\nrdist = the distribution functions qdist = the quantile functions pdist = the probability density function ddist the density function\n\n\n\n\n\n\nNote\n\n\n\nIn these simulations we can see that we are expressing ourselves with a sort of great, big shrug: we have no idea what these numbers might be, and so we are working to just make up fake ones. we start with very little – with literally no information – about what our data might be when we see it. for a bayeisan, there is no practical difference between data and parameters in a model\nwhat makes a person and approach Bayesian is not the use of Bayes rule. Bayes rule is just a fact about conditional probability, which everyone uses regardless of their framework for scientific inference. no, to be bayesian is to use probability to measure uncertainty.\n\n\n.. plot it.."
  },
  {
    "objectID": "topics/01_simulation/index.html#simulating-data-in-stan",
    "href": "topics/01_simulation/index.html#simulating-data-in-stan",
    "title": "Data simulation",
    "section": "Simulating data in Stan",
    "text": "Simulating data in Stan\nwrite model walk through it"
  },
  {
    "objectID": "topics/01_simulation/index.html#parameter-recovery",
    "href": "topics/01_simulation/index.html#parameter-recovery",
    "title": "Data simulation",
    "section": "parameter recovery",
    "text": "parameter recovery\nOk so now we can see our first simple stan model! we are using this powerful tool to draw forty random numbers.\nin R : fitdistr, glm\nNow we can move on to the second step outlined above: we can just fit the same data to our model, vice versa, and see if we can recover that parameter.\nIn R, there are several ways to do it: first, we can use R do it in two ways: fitdistr, and glm.\nThen we do the same thing in Stan.\nThen we look to see if we have recovered our parameter.\nThe next steop in visualization, which we also do with this simple model."
  },
  {
    "objectID": "topics/day_1.html#content",
    "href": "topics/day_1.html#content",
    "title": "Day 1",
    "section": "Content",
    "text": "Content\nThe Secret Weapon\nregression with discrete predictors\n\nAfternoon practical exercises"
  },
  {
    "objectID": "topics/day_1.html#course-setup-information",
    "href": "topics/day_1.html#course-setup-information",
    "title": "Day 1",
    "section": "Course setup information",
    "text": "Course setup information\n\nsite information\nplagiarism"
  },
  {
    "objectID": "topics/day_1.html#simulation",
    "href": "topics/day_1.html#simulation",
    "title": "Day 1",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "topics/day_1.html#quantifying-uncertainty",
    "href": "topics/day_1.html#quantifying-uncertainty",
    "title": "Day 1",
    "section": "Quantifying uncertainty",
    "text": "Quantifying uncertainty"
  },
  {
    "objectID": "topics/day_1.html#resampling",
    "href": "topics/day_1.html#resampling",
    "title": "Day 1",
    "section": "Resampling",
    "text": "Resampling\nIn frequentist models, we can use the variance covariance matrix of parameters to resample new parameters values. This lets us propagate uncertainty from the estimated parameters to the predicted relationship.\nLet’s demonstrate this with one specific mite:\n\nlrug_water <- mite_water |> \n  filter(sp == \"LRUG\")\n\nlrug_glm <- glm(pa ~ water, data = lrug_water, family = \"binomial\")\n\nNow, with our model object, we can create the resampling distribution of the model predicitons:\n\n# Set seed\nset.seed(42) # The answer !\n\n# a sequence along the range of water values in the data\npredVal <- seq(from = min(lrug_water$water),\n               to = max(lrug_water$water),\n               length.out = 30)\n\nn_resamp <- 500\n\n# Result object\nresampModel <- array(NA_real_,\n                   dim = c(length(predVal), n_resamp))\n\n# Resample model parameters and calculate model predictions\nparamMean <- summary(lrug_glm)$coefficients[,1]\nparamCov <- summary(lrug_glm)$cov.unscaled\n\n# Resample model parameters\nparamSmpl <- MASS::mvrnorm(n_resamp, paramMean, paramCov)\n\n# Calculate model predictions using the resampled model parameters\nfor(j in 1:n_resamp){\n  resampModel[,j] <- binomial(link = \"logit\")$linkinv(\n    paramSmpl[j,1] + paramSmpl[j,2] * predVal)\n}\n\n# make a plot of these predictions\nmatplot(predVal, resampModel, type = \"l\", col = \"grey\", lty = 1)\n\n\n\n\nIf we want to find some kind of confidence interval for this line, we can take the quantiles of this resampling:\n\nlow <- apply(resampModel, 1, quantile, probs = .015)\nhigh <- apply(resampModel, 1, quantile, probs = .985)\n\n# plot\nwith(lrug_water, plot(pa ~ water, pch = 21, bg = \"lightblue\"))\npolygon(c(predVal,rev(predVal)),\n        c(low,rev(high)), col=\"thistle\", border=NA)\nlines(predVal, \n      predict(lrug_glm, newdata = list(water = predVal), type = \"response\")\n      )\n\n\n\n\nWe can also do this in a tidyverse style, if you are more comfortable with that:\n\ntibble(predVal) |> \n  rowwise() |> \n  mutate(intercept = list(paramSmpl[,1]),\n         slope = list(paramSmpl[,2]),\n         prediction = list(intercept + slope*predVal),\n         prediction_probability = list(plogis(prediction)),\n         low  = quantile(prediction_probability, .015),\n         high = quantile(prediction_probability, .985)) |> \n  ggplot(aes(x = predVal, ymin = low, ymax = high)) + \n  geom_ribbon(fill = \"thistle\") + \n  theme_bw() + \n  ylim(c(0,1))"
  },
  {
    "objectID": "topics/day_1.html#bayesian-approach",
    "href": "topics/day_1.html#bayesian-approach",
    "title": "Day 1",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nhere is a simple bayesian model to generate the same inference:\n\\[\n\\begin{align}\ny &\\sim \\text{Bernoulli}(p)\\\\\n\\text{logit}(p) &= \\alpha + X\\beta\\\\\n\\alpha &\\sim \\text{Normal}(-2.5, .5)\\\\\n\\beta &\\sim \\text{Normal}(0, .5)\\\\\n\\end{align}\n\\]\nnormally we would go through a careful process of checking our priors here. At this time we won’t because the point here is to show how the bayesian posterior includes uncertainty, not to demonstrate a full Bayes workflow.\nFirst we compile the model, then we’ll look at the Stan code:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Utilisateur/Documents/.cmdstan/cmdstan-2.30.1\n\n\n- CmdStan version: 2.30.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\nlogistic_glm_stan <- cmdstan_model(stan_file = \"stan/logistic_bern_logit.stan\", \n                               pedantic = TRUE)\n\nlogistic_glm_stan\n\ndata {\n  int<lower=0> n;\n  vector[n] x;\n  array[n] int<lower=0,upper=1> y;\n}\nparameters {\n  real intercept;\n  real slope;\n}\nmodel {\n  y ~ bernoulli_logit(intercept + slope * x);\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n}\n\n\nHere we see the same three parts of a Stan model that we have reviewed already:\n\ndata\nparameters\nprobability statements\n\nAs you can see, we are using a handy Stan function called bernoulli_logit. This function expects our prediction for the average to be on the logit scale, then applies the logit link function for us.\n\n\nAs a quick review, the logit equation, or inverse-log-odds, is written as \\[\n\\frac{e^\\mu}{1 + e^\\mu}\n\\] Which is also written as\n\\[\n\\frac{1}{1 + e^{-\\mu}}\n\\]\nStan expects our data as a list.\n\nlogistic_glm_stan_samples <- logistic_glm_stan$sample(\n  data = list(n = nrow(lrug_water),\n              y = lrug_water$pa,\n              x = lrug_water$water),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.8 seconds.\n\ncoef(lrug_glm)\n\n(Intercept)       water \n-2.48153943  0.00874349 \n\nlibrary(tidybayes)\n\nspread_rvars(logistic_glm_stan_samples, intercept, slope[]) |> \n  bind_cols(predVal = predVal) |> \n  mutate(pred = posterior::rfun(plogis)(predVal * slope + intercept)) |> \n  ggplot(aes(x = predVal, ydist = pred)) + \n  stat_dist_lineribbon() + \n  guides(fill = \"none\") + \n  ylim(c(0,1))\n\n\n\n\n\nAlternative parameterization\nStan contains many functions intended to facilitate writing statistical models. Above, we used the function bernoulli_logit so that we could provide the expression for the average on the logit scale.  Stan also provides an even more efficient function that we can use; it is especially good when we have more than one predictor variable and a vector of slopes:This idea is the core concept of a GLM, or generalized linear model. Statistical distributions have parameters, but for most distributions these have constraints – only some values are “allowed”. For example, the only parameter of a Bernoulli distribution is \\(p\\), the probability of success. We respect this constraint by using a link function: we write an expression for the average of a distribution that can be any real number, and put it through a link function to get the value for \\(p\\).\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE NOTE below you will see the relative path to the stan file (stan/logistic.stan). Immediately below you will see the Stan file content. You can copy and paste this to your own computer!\n\n\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm <- cmdstan_model(stan_file = \"stan/logistic.stan\", \n                               pedantic = TRUE)\n\nlogistic_bern_glm\n\ndata {\n  int<lower=0> N;\n  matrix[N, 1] x;\n  array[N] int<lower=0,upper=1> y;\n}\nparameters {\n  real intercept;\n  vector[1] slope;\n}\nmodel {\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n  y ~ bernoulli_logit_glm(x, intercept, slope);\n}"
  },
  {
    "objectID": "topics/day_2.html#convergence-diagnostics",
    "href": "topics/day_2.html#convergence-diagnostics",
    "title": "Day 2",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics"
  },
  {
    "objectID": "topics/day_2.html#model-validation-with-simulation",
    "href": "topics/day_2.html#model-validation-with-simulation",
    "title": "Day 2",
    "section": "Model validation with simulation",
    "text": "Model validation with simulation"
  },
  {
    "objectID": "topics/day_2.html#dags",
    "href": "topics/day_2.html#dags",
    "title": "Day 2",
    "section": "DAGs",
    "text": "DAGs\n\nforks, pipes and colliders\nsimulation from a DAG\ndemonstration of errors from disregarding DAGs"
  },
  {
    "objectID": "topics/discrete_predictor/index.html",
    "href": "topics/discrete_predictor/index.html",
    "title": "Palmer penguins and discrete predictors",
    "section": "",
    "text": "Let’s start by taking a look at the Palmer Penguin dataset. Let’s look at the distribution of observations of bill size.\nThere’s quite a lot of variation in these measurements, with a suggestion of perhaps more than one peak in this distribution."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#a-simple-model",
    "href": "topics/discrete_predictor/index.html#a-simple-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "A simple model",
    "text": "A simple model\n\\[\n\\begin{align}\n\\text{Bill depth} &\\sim \\text{Normal}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\text{Normal}(17.5, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n\\]\nlet’s express the same model in Stan:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Utilisateur/Documents/.cmdstan/cmdstan-2.30.1\n\n\n- CmdStan version: 2.30.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\nnormal_dist <- cmdstan_model(\"topics/discrete_predictor/normal_dist.stan\")\nnormal_dist\n\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu, sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\n\n\nThe model section looks very much like the approach shown above. I want you to notice especially how the bottom chunk has three lines, each describing a probability distribution. These are for all the probability distribution of all the quantities in the model, both observed and unobserved. Above, we state which is which. Models are devices for putting together the probability of all the quantities we are looking for. Again, a Bayesian defines the world as umeasured or measured quantities – and above we state which are observed (the data block) and which are unobserved (the parameters block).\nWe can fit this model to data and see the result:\n\n# first we drop all NA values\npenguins_nobillNA <- penguins |> \n  #drop NA values\n  filter(!is.na(bill_depth_mm))\n\n## then we assemble the data as a list.\n## I'm using the base function with()\n##  it lets me use the variable name directly \n## without writing penguins_nobillNA$bill_depth_mm\n\nlist_bill_dep <- with(penguins_nobillNA,\n     list(N = length(bill_depth_mm),\n          measurements = bill_depth_mm))\n     \n## sample 4 chains, suppress counting iterations\nsamp_bill_dep <- normal_dist$sample(data = list_bill_dep, \n                                    parallel_chains = 4,\n                                    refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\n## summarize the samples for each parameter into a nice table\nsamp_bill_dep |> \n  posterior::summarise_draws() |> \n  flextable::flextable()\n\n\nvariablemeanmediansdmadq5q95rhatess_bulkess_taillp__-405.542769-405.233501.029882680.74426520-407.550150-404.5700000.99995832,022.4002,314.258mu17.15103017.147900.108644750.1089711016.97339517.3298151.00084203,014.1192,613.629sigma1.9758491.972450.076562390.074315321.8544072.1073211.00023193,436.3002,574.980"
  },
  {
    "objectID": "topics/discrete_predictor/index.html#plotting-parameters.",
    "href": "topics/discrete_predictor/index.html#plotting-parameters.",
    "title": "Palmer penguins and discrete predictors",
    "section": "Plotting parameters.",
    "text": "Plotting parameters.\nWe don’t have one distribution for each of our unknown numbers: we have thousands. We need to get a sense of what these possible values mean scientifically. An excellent way to do this is by making as many pictures as possible. We will start with making plots of specific parameters.\nWe can look at the distributions easily using the bayesplot package.\n\ndraws <- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\n\nbayesplot::mcmc_hist(draws, pars = \"mu\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nbayesplot::mcmc_hist(draws, pars = \"sigma\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the distributions do not have the same shape as the prior– this is particularly true for \\(\\sigma\\). This shows an important point: the prior distribution does not determine what the posterior looks like. should I sample from the prior and show them that?\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndraws |>  \n  posterior::as_draws_df() |> \n  ggplot(aes(x = sigma)) + \n  stat_dotsinterval()\n\n\n\n\n\nFigure 1: the package ggdist has many fun & useful ways to draw pictures of posterior distributions. Here is one called stats_dotsinterval()"
  },
  {
    "objectID": "topics/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "href": "topics/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "Posterior predictions: the easy way to check your model",
    "text": "Posterior predictions: the easy way to check your model\nPeople care so much about model diagnostics. And with good reason: you need to know how much to trust a model before using it to make a scientific claim. One way to find out who’s model is best would be to use them to make a prediction, and see how right you are. Nobody has the time for that. so instead the best choice is to see how well the data fit your sample.\n\n# just get some draws\ndraws <- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\ndraws_matrix <- posterior::as_draws_matrix(draws)\n\n## set up a matrix. for every posterior sample, \n## (that is, for a value of mu and a value of sigma) \n## draw a whole fake dataset from a normal distribution with that mean and sd. \nnsamples <- 50\nyrep <- matrix(0, ncol = list_bill_dep$N, nrow = nsamples)\n\n# pick some random rows\nset.seed(1234)\nchosen_samples <- sample(1:nrow(draws_matrix), replace = FALSE, size = nsamples)\nsubset_draws <- draws_matrix[chosen_samples,]\n\nfor (r in 1:nsamples){\n yrep[r,] <- rnorm(n = list_bill_dep$N, \n                   mean = subset_draws[r, \"mu\"], \n                   sd = subset_draws[r, \"sigma\"])\n}\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = yrep)\n\n\n\n\n\nPosterior predictions in Stan\nWe can simulate our own data in R if we are comfortable translating between R and Stan. However, if you want, you can do the same process in Stan. Just combine the section we just looked at with the previous work on data simulation we started with:\n\nnormal_dist_rng <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng.stan\")\n\nnormal_dist_rng\n\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu, sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu, sigma);\n  }\n}\n\n\nHere we have a handy random number generator inside Stan.\n\nsamp_bill_dep_rng <- normal_dist_rng$sample(\n  data = list_bill_dep,\n  refresh = 0,\n  parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.8 seconds.\nTotal execution time: 1.0 seconds.\n\ndraws <- samp_bill_dep_rng$draws(variables = c(\"yrep\"))\ndraws_matrix <- posterior::as_draws_matrix(draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(draws_matrix, 50))\n\n\n\n\nThe code is much shorter, because there is less to do in R. Both of these gives the same outcome: the posterior predictive distribution. This gives us a straightfoward way to test our model’s performance:\n\nwe use the model to generate fake observations.\nplot these on top of the real data\nif the data is a really poor match, we know our model has a distorted view of the world."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#different-groups-are-different",
    "href": "topics/discrete_predictor/index.html#different-groups-are-different",
    "title": "Palmer penguins and discrete predictors",
    "section": "Different groups are different",
    "text": "Different groups are different\nlet’s add in differences among species\n\npenguins |> \n  ggplot(aes(x = bill_depth_mm, fill = species))+ \n  geom_histogram(binwidth = .5) + \n  scale_fill_brewer(palette = \"Dark2\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nNow we can see that the distribution is in fact three different shapes, all placed together.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes scientists will plot histograms of data at the beginning of a research project, and use the histogram to decide if their data are “normally distributed” or not. This is not helpful! Instead, decide on a model first, and ask yourself what kind of data you expect."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#stan-code-for-species-differences",
    "href": "topics/discrete_predictor/index.html#stan-code-for-species-differences",
    "title": "Palmer penguins and discrete predictors",
    "section": "Stan code for species differences",
    "text": "Stan code for species differences\n\\[\n\\begin{align}\n\\text{Bill depth}_{\\text{sp}[i]} &\\sim \\text{Normal}(\\mu_{\\text{sp}[i]}, \\sigma) \\\\\n\\mu &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(2) \\\\\n\\end{align}\n\\]\n\nnormal_dist_rng_spp_forloop <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp_forloop.stan\")\n\nnormal_dist_rng_spp_forloop\n\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int<lower=1,upper=3> spp_id;\n}\nparameters {\n  vector[3] mu;\n  real<lower=0> sigma;\n}\nmodel {\n  for (i in 1:N){\n    measurements[i] ~ normal(mu[spp_id[i]], sigma);\n  }\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n\n\nThere’s a few differences to notice here:\n\nin the data block: We have a new input! A declaration of the array of integers at the top, saying if this is “species 1”, “species 2”, or “species 3”\nmu is a vector now. why?\nnotice the for-loop.\n\nDo we maybe add an illustration here of how vector indexing works?\nWe can write this model a different way as well:\n\nnormal_dist_rng_spp <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp.stan\")\n\nnormal_dist_rng_spp\n\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int<lower=1,upper=3> spp_id;\n}\nparameters {\n  vector[3] mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu[spp_id], sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n\n\nThe only difference to the previous model is in the line with the for-loop, which is now replaced with a vectorized expression. This is faster to write and will run faster in Stan. However its not possible in every case. add a link to the Stan forum\n\nSampling the species model\nWe have to make a new data list, since we’ve added a new input: a vector of numbers 1, 2, or 3 that tells us if we are working with the first, second, or third species.\nwhich model to sample? here i’m doing the vectorized one just because\n\nlist_bill_dep_spp <- with(penguins_nobillNA,\n     list(\n       N = length(bill_depth_mm),\n       measurements = bill_depth_mm,\n       spp_id = as.numeric(as.factor(species))\n     )\n)\n     \nsamp_normal_dist_rng_spp <- normal_dist_rng_spp$sample(\n  data = list_bill_dep_spp, \n  parallel_chains = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.1 seconds.\nChain 2 finished in 1.0 seconds.\nChain 3 finished in 1.1 seconds.\nChain 4 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.1 seconds.\nTotal execution time: 1.2 seconds.\n\nsamp_normal_dist_rng_spp$draws(variables = c(\"mu\", \"sigma\")) |> \n  posterior::summarise_draws() |> \n  flextable::flextable()\n\n\nvariablemeanmediansdmadq5q95rhatess_bulkess_tailmu[1]18.34290818.3420000.092030370.0945898818.19057518.495401.00001884,258.3992,837.095mu[2]18.41304518.4107500.138196530.1289862018.18108518.643911.00042384,622.5703,058.643mu[3]14.98449614.9853500.100290490.0985187714.81769015.150421.00085174,849.7563,079.081sigma1.1228291.1219250.042074220.042528381.0554541.192230.99950634,796.7263,304.838\n\n\nand we can repeat the posterior checking from before:\n\nspp_yrep_draws <- samp_normal_dist_rng_spp$draws(variables = c(\"yrep\"))\nspp_draws_matrix <- posterior::as_draws_matrix(spp_yrep_draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(spp_draws_matrix, 50))\n\n\n\n\nThe predicted distribution is now much more like the real data\nWe can also make figures for each individual species. Here we will move away from using bayesplot and try to visualize our posterior using the handy functions in the tidybayes package add a link\n\nlibrary(tidybayes)\nspp_draws_df <- posterior::as_draws_df(spp_yrep_draws)\n\nnormal_dist_post_samp <- tidybayes::gather_draws(samp_normal_dist_rng_spp,\n                        yrep[row_id], \n                        ndraws = 50)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the tidybayes package.\n  Please report the issue at <https://github.com/mjskay/tidybayes/issues/new>.\n\nnormal_dist_post_samp |> \n  mutate(species = penguins_nobillNA$species[row_id]) |> \n  ggplot(aes(x = .value, colour = species)) + \n  geom_density(aes(group = .iteration), alpha = .1) + \n  facet_wrap(~species) + \n  geom_density(aes(x = bill_depth_mm),\n               data = penguins_nobillNA,\n               colour = \"black\") + \n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\nExercises\n\nLevel 1\n\nrepeat this experience for another variable in the dataset. Does the same code work on bill length? What about body size? What would you change about the model (if anything)\nuse bayesplot to examine the fit of body size to these data.\n\n\n\nLevel 2\n\ngenerate some random groups of your own, with known means. How well does the model fit these data\nThe present model is fixed for exactly 3 groups. how would you change it for any number of groups?\n\n\n\nLevel 3\n\nthe function tidybayes::compose_data is a convenient way to set up your data for passing it into R. Try out this function. What does it produce for our dataset? How do you need to modify our Stan program so that it works for the output of tidybayes::compose_data?\nAs you can see, the model assumes the same sigma for all species. what if you relax this?\n\n\n\n\nOptional!\nTry this on your own data!"
  },
  {
    "objectID": "topics/intercept_only/index.html",
    "href": "topics/intercept_only/index.html",
    "title": "Modèles hiérarchiques pour les sciences de la vie",
    "section": "",
    "text": "Understanding a hierarchical model as a model for parameters\nIntroduce\nOne Weird Trick (you Supervisor will be Amazed)— mean centering and group mean centering\n\na model for parameters\nstart withglobal aerages of seach tgroup., our question,is, what create this variation in averages?\nshow distribution of sample averages Do this for the average of the distributions, showing their interactions with each other, their distribution oacross all values.\nStarting with just the distribution of abundances across the species and plots.\nThen move to an intercept-only poisson model : distribution across plots\nreally take time: build up a model of multiple averages per species and multiple per plot\nor would the penguins be better for this? Consider the different islands and the different species\nhonestly could do it with both.\nThen try the following\nmodelling an average modelling group means modelling more than one group mean\na process: * Look at your data * think about “0” – where should it be? what does it mean? * think about units – how much of a difference in your “X” matters to your “y”?\n\nmodel the average and standard deviation of bill size (or of species abudance)\n\n\\[\n\\begin{align}\nY_s &\\sim \\text{Poisson}(\\lambda_s) \\\\\n\\log{\\lambda_s} &\\sim \\text{Normal}(300, 100)  \\\\\n\\end{align}\n\\]\n\na quick note – centering the response, i.e. taking the average out of the prior on the averages\nlook at the averages\nhow could we describe these averages? a simple model: mean and standard deviation\n\n\ndata(\"mite\", package = \"vegan\")\n\nspp_names <- colnames(mite)\nspp_names <- setNames(1:ncol(mite), colnames(mite))\n\n\nmite_long <- mite |> \n  tidyr::pivot_longer(dplyr::everything(), names_to = \"spp\", values_to = \"abd\") |> \n  dplyr::mutate(group_id = spp_names[spp])\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Utilisateur/Documents/.cmdstan/cmdstan-2.30.1\n\n\n- CmdStan version: 2.30.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\ngroup_avg_nopool <- cmdstan_model(\"topics/intercept_only/group_avg_nopool.stan\", \n                                  pedantic = TRUE)\n\nWarning in readLines(stan_file): incomplete final line found on\n'topics/intercept_only/group_avg_nopool.stan'\n\nsamp_group_avg_nopool <- group_avg_nopool$sample(data = list(\n  N = nrow(mite_long),\n  N_groups = ncol(mite),\n  group_id = mite_long$group_id,\n  abd = mite_long$abd\n))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.9 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.8 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 4.4 seconds.\n\n\n\nlibrary(tidyverse)\nlibrary(tidybayes)\nrvars_group_means_unpooled <- samp_group_avg_nopool |> \n  tidybayes::gather_rvars(group_mean[group_id]) |> \n  dplyr::mutate(spp = names(spp_names)[group_id],\n                spp = fct_reorder(spp, .value, .fun = median))\n\nrvars_group_means_unpooled |> \n  ggplot2::ggplot(aes(y = spp, dist = .value)) + \n  tidybayes::stat_halfeye()\n\n\n\nrvars_group_means_unpooled |> \n  ggplot2::ggplot(aes(y = spp, dist = exp(.value))) + \n  tidybayes::stat_halfeye()\n\n\n\ncolMeans(mite)\n\n    Brachy       PHTH       HPAV       RARD       SSTR    Protopl       MEGR \n 8.7285714  1.2714286  8.5142857  1.2142857  0.3142857  0.3714286  2.1857143 \n      MPRO       TVIE       HMIN      HMIN2       NPRA       TVEL       ONOV \n 0.1571429  0.8285714  4.9142857  1.9571429  1.8857143  9.0571429 17.2714286 \n      SUCT       LCIL   Oribatl1   Ceratoz1       PWIL   Galumna1   Stgncrs2 \n16.9571429 35.2571429  1.8857143  1.2857143  1.0857143  0.9571429  0.7285714 \n      HRUF   Trhypch1       PPEL       NCOR       SLAT       FSET   Lepidzts \n 0.2285714  2.6142857  0.1714286  1.1285714  0.4000000  1.8571429  0.1714286 \n  Eupelops   Miniglmn       LRUG      PLAG2   Ceratoz3   Oppiminu   Trimalc2 \n 0.6428571  0.2428571 10.4285714  0.8000000  1.3000000  1.1142857  2.0714286 \n\n\nI could easily add these colmeans to the second plot above\n\nrvars_group_means_unpooled |> \n  ggplot2::ggplot(aes(y = spp, dist = exp(.value))) + \n  tidybayes::stat_halfeye() +\n  geom_point(aes(y = spp, x = value), \n             inherit.aes = FALSE,\n             data = enframe(colMeans(mite), name = \"spp\"),\n             col = \"red\")\n\n\n\n\nBefore we jump to considering this as a hierarchical model, let’s look at another way of writing this Stan code\nFirst of all, I could have written this as a for-loop. I chose to vectorize it, and you can read more about it in the Stan user manual here.\n\n# for-loop version\n\nYou migth prefer seeing it this way because this lets you keep the data in the format you found it. you might also find it more readable. For models of this size, there will be no difference in speed between the two. But in bigger models, the first way I showed will be faster. this is because of the way that th Stan algorithm works when evaluatint the likelihood section. You can read more about it here [TK]\nWhether you choose a vectorized or for-loop approach to writing the likelihood, there is another, much more important alternative to writing a likelihood."
  },
  {
    "objectID": "topics/intercept_only/index.html#rewriting-the-normal-distribution",
    "href": "topics/intercept_only/index.html#rewriting-the-normal-distribution",
    "title": "Modèles hiérarchiques pour les sciences de la vie",
    "section": "rewriting the Normal distribution",
    "text": "rewriting the Normal distribution\n\\[\n\\text{Normal}(\\mu, \\sigma) = \\mu + z \\times \\sigma\n\\]\nif\n\\[\nz \\sim \\text{Normal}(0, 1)\n\\]\nyou can choose to write a normal distribution two ways. In the first, you consider the mean and standard devation as parameters “inside” the distribution, in the other, you start with a standard normal distribution and first scale it (multiply by the standard deviation) and then shift it (add in the average)\nThe result is a distribution that slides around the number line, like this:\n\n## tk animation\n\nWe can rewrite the first model in exactly this syntax\n\ngroup_avg_nopool_nc <- cmdstan_model(\"topics/intercept_only/group_avg_nopool_nc.stan\", \n                                  pedantic = TRUE)\n\nsamp_group_avg_nopool_nc <- group_avg_nopool_nc$sample(data = list(\n  N = nrow(mite_long),\n  N_groups = ncol(mite),\n  group_id = mite_long$group_id,\n  abd = mite_long$abd)\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.7 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.6 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.8 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 7.4 seconds.\n\n\nThis is an interesting fact that we will use later, but which I wanted to show now, separately from studying hierarchical modesls\n\nA model for parameters\nlet’s go back to the original differences between groups.. this time using species data\nCan also run the above model with row numbers\n\nmite_long_group <- mite |> \n  tibble::rowid_to_column() |> \n  pivot_longer(-rowid, names_to = \"spp\", values_to = \"abd\") |> \n  rename(group_id = rowid)\n\nsamp_group_avg_nopool_nc <- group_avg_nopool_nc$sample(data = list(\n  N = nrow(mite_long_group),\n  N_groups = max(mite_long_group$group_id),\n  group_id = mite_long_group$group_id,\n  abd = mite_long_group$abd))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 2.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.9 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 2.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 8.6 seconds.\n\nrvars_group_means_unpooled <- samp_group_avg_nopool_nc |> \n  tidybayes::gather_rvars(group_mean[group_id]) |> \n  dplyr::mutate(spp = names(spp_names)[group_id],\n                spp = fct_reorder(spp, .value, .fun = median))\n\nrvars_group_means_unpooled |> \n  ggplot2::ggplot(aes(y = group_id, dist = .value)) + \n  tidybayes::stat_halfeye()\n\n\n\n\nnow, let’s stop for a second and look at the point estimates for each plot in the dataset:\n\nrvars_group_means_unpooled |> \n  mutate(site_average = median(.value)) |> \n  ggplot(aes(x = site_average)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nAre you using the median to calculate something you’re calling an average ? what is going on?! A: I did this on purpose to show this distinction. the PARAMETER we’re talking about is the average abundance in each site. We don’t have a single value for this average, instead we have 2000 possible values, according to our model. We could choose to summarize those numbers any way we want – often, the median is a good choice.\n\nWhere do these differences in group mean come from? The simplest possible model might be that there is some average mite abundance, and some plots have more or less than this average. In other words, a normal distribution.\nLet’s add this model into our code\n\ngroup_avg_partpool <- cmdstan_model(\"topics/intercept_only/group_avg_partpool.stan\",\n                                    pedantic = TRUE)\n\nsamp_group_avg_partpool <- group_avg_partpool$sample(data = list(\n  N = nrow(mite_long_group),\n  N_groups = max(mite_long_group$group_id),\n  group_id = mite_long_group$group_id,\n  abd = mite_long_group$abd))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/UTILIS~1/AppData/Local/Temp/RtmpgT7jTq/model-62a0151f72ab.stan', line 14, column 2 to column 33)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/UTILIS~1/AppData/Local/Temp/RtmpgT7jTq/model-62a0151f72ab.stan', line 14, column 2 to column 33)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/UTILIS~1/AppData/Local/Temp/RtmpgT7jTq/model-62a0151f72ab.stan', line 14, column 2 to column 33)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/UTILIS~1/AppData/Local/Temp/RtmpgT7jTq/model-62a0151f72ab.stan', line 14, column 2 to column 33)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.1 seconds.\nTotal execution time: 4.7 seconds.\n\nsamp_group_avg_partpool |> \n  gather_rvars(group_mean[spp]) |> \n  arrange(median(.value))\n\n# A tibble: 70 × 3\n     spp .variable         .value\n   <int> <chr>         <rvar[1d]>\n 1    57 group_mean  -0.88 ± 0.25\n 2    62 group_mean  -0.62 ± 0.22\n 3    61 group_mean   0.25 ± 0.14\n 4    54 group_mean   0.39 ± 0.14\n 5    55 group_mean   0.54 ± 0.13\n 6    44 group_mean   0.57 ± 0.12\n 7    24 group_mean   0.84 ± 0.11\n 8    23 group_mean   0.85 ± 0.11\n 9    41 group_mean   0.85 ± 0.11\n10    63 group_mean   0.91 ± 0.11\n# … with 60 more rows\n\n\n\nsamp_group_avg_partpool |> \n  gather_rvars(mu, sigma)\n\n# A tibble: 2 × 2\n  .variable        .value\n  <chr>        <rvar[1d]>\n1 mu         1.33 ± 0.070\n2 sigma      0.57 ± 0.054"
  },
  {
    "objectID": "topics/intercept_only/index.html#regularization-a-simple-simulation",
    "href": "topics/intercept_only/index.html#regularization-a-simple-simulation",
    "title": "Modèles hiérarchiques pour les sciences de la vie",
    "section": "Regularization: a simple simulation",
    "text": "Regularization: a simple simulation\none of the best and most useful aspects of hierarchical models is one which is not easy to see in our chosen datasets! Instead, I’m going to simulate some data to demonstrate it.\n\nset.seed(1234)\nfake_clutch_size_data <- tibble::tibble(\n  site_id = 1:42,\n  n_nests_per_site = sample(size = max(site_id),\n                            x = c(33, 13, 3),\n                            prob = c(.1, .2, .7),\n                            replace = TRUE),\n  site_mean = rnorm(n = max(site_id),\n                    mean = log(10), \n                    sd = .7)) |> \n  rowwise() |> \n  mutate(clutch_size = list(rpois(n = n_nests_per_site, \n                                  lambda = exp(site_mean))))\n\n\nfake_clutch_size_data |> \n  # unnest(clutch_size) |> \n  mutate(mean_cs = mean(clutch_size)) |> \n  ggplot(aes(x = exp(site_mean), y = mean_cs,\n             fill = as.factor(n_nests_per_site))) + \n  geom_point(pch = 21, size = 5) + \n  scale_fill_brewer(palette = \"Dark2\") + \n  geom_abline(slope = 1, intercept = 0)\n\n\n\n\ncan also look at it going through 0\n\nnopool_fig <- fake_clutch_size_data |> \n  # unnest(clutch_size) |> \n  mutate(mean_cs = mean(clutch_size)) |> \n  ggplot(aes(x = exp(site_mean), y = mean_cs - exp(site_mean),\n             fill = as.factor(n_nests_per_site))) + \n  geom_point(pch = 21, size = 5) + \n  scale_fill_brewer(palette = \"Dark2\") + \n  geom_abline(slope = 0, intercept = 0)\n\nin both cases you can see that the green values are off of the true value – just by chance they ended up above or below the real value\nlet’s fit our model from above to this!\n\nunnest_fake_cs <- unnest(fake_clutch_size_data, cols = \"clutch_size\")\n\ngroup_avg_partpool_nests_samp <- group_avg_partpool$sample(\n  data = with(unnest_fake_cs, \n              list(N = length(clutch_size),\n                   N_groups = max(site_id),\n                   group_id = site_id,\n                   abd = clutch_size\n              ))\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/UTILIS~1/AppData/Local/Temp/RtmpgT7jTq/model-62a0151f72ab.stan', line 14, column 2 to column 33)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.4 seconds.\n\n\n\ntruth_posterior_combined <- group_avg_partpool_nests_samp |> \n  gather_rvars(group_mean[site_id]) |> \n  mutate(mean_cs = median(.value)) |> \n  select(-.value) |> \n  left_join(fake_clutch_size_data |> select(site_id, n_nests_per_site, site_mean))\n\nJoining, by = \"site_id\"\n\ntruth_posterior_combined |> \n  ggplot(\n    aes(\n      x = site_mean,\n      y = mean_cs - site_mean,\n      fill = as.factor(n_nests_per_site))) + \n  geom_point(pch = 21, size = 5) + \n  scale_fill_brewer(palette = \"Dark2\") + \n  geom_abline(slope = 0, intercept = 0)\n\n\n\nnopool_fig + \n  geom_point(\n    aes(\n      x = site_mean,\n      y = mean_cs - site_mean),\n    data = truth_posterior_combined) + \n  facet_wrap(~n_nests_per_site)\n\n\n\ntruth_posterior_combined |> \n  filter(exp(site_mean)>50)\n\n# A tibble: 1 × 5\n  site_id .variable  mean_cs n_nests_per_site site_mean\n    <int> <chr>        <dbl>            <dbl>     <dbl>\n1      41 group_mean    3.98                3      4.09\n\nfake_clutch_size_data |> \n  filter(site_id == 41) |> \n  pull(clutch_size)\n\n[[1]]\n[1] 64 54 46\n\n\nSmall exercise for you: demonstrate that all 3 models described here produce answers that are exactly the same.\n\n\n\nIn the ohter, you have standard normal variation\nAnimation showing overa ll mean and error, then group means and error\nprogrssion later to negative binomial perhaps??\nanimation showing averages – group averages – species averages\nreally want to show my own style here! I think I can see how this would build together.\nI wonder, if there is a way to simulate predator-prey dynamics in space, in a model where the predator needs to have both prey and environment conditions, but they prey need only the environment. what would happen if we use causal models (or the wrong causal model) on those data, measuring predators affecting prey when there is nothing\ngroup-mean-centering as a main topic Not as a new topic but as an interesting way to describe multilevel, slope models.\nanimation of points moving together, after group-mean-centering"
  },
  {
    "objectID": "topics/secret_weapon.html",
    "href": "topics/secret_weapon.html",
    "title": "Summarizing many univariate models",
    "section": "",
    "text": "We’ve already looked at univariate models. When we fit the same model to multiple different groups, we don’t expect the same values for all the coefficients. Each thing we are studying will respond to the same variable in different ways.\nHierarchial models represent a way to model this variation, in ways that range from simple to complex.\nBefore we dive in with hierarchical structure, let’s build a bridge between these two approaches.\nThis is useful to help us understand what a hierarchical model does.\nHowever it is also useful from a strict model-building perspective – so useful that Andrew Gelman calls it a “Secret Weapon” tk link\nTo keep things simple and univariate, let’s consider only water:\nFirst, a quick word about centering and scaling a predictor variable:\nsome things to notice about this figure:\nAs you can see, some of these estimates are high, others low. We could also plot these as histograms to see this distribution.\nOnce again, the two parameters of this model represent:"
  },
  {
    "objectID": "topics/secret_weapon.html#say-it-in-stan",
    "href": "topics/secret_weapon.html#say-it-in-stan",
    "title": "Summarizing many univariate models",
    "section": "Say it in Stan",
    "text": "Say it in Stan\nThe above tidyverse approach is very appealing and intuitive, but we can also do the same procedure in Stan."
  },
  {
    "objectID": "topics/secret_weapon.html#modelling-variation-in-slopes",
    "href": "topics/secret_weapon.html#modelling-variation-in-slopes",
    "title": "Summarizing many univariate models",
    "section": "Modelling variation in slopes",
    "text": "Modelling variation in slopes\nClearly there is variation among species in the values of these parameters. Like all variation, we can develop a scientific model to describe it. The simplest model we’ll consider is a simple univariate distribution.\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm <- cmdstan_model(\n  stan_file = here::here(\"topics/secret_weapon_univariate.stan\"), \n  pedantic = TRUE)\n\nmite_bin <- mite\nmite_bin[mite_bin>0] <- 1\n\nlogistic_bern_glm$sample(data = list(\n  Nsites = nrow(mite_bin),\n  K = 2,\n  S = ncol(mite_bin),\n  x = cbind(1, with(mite.env, (WatrCont - mean(WatrCont))/100)),\n  y = as.matrix(mite_bin)\n))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 9.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 9.3 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 10.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 10.3 seconds.\nTotal execution time: 42.0 seconds.\n\n\n variable     mean   median   sd  mad       q5      q95 rhat ess_bulk ess_tail\n   lp__   -1171.18 -1170.78 8.32 8.07 -1185.34 -1157.97 1.00      643     1511\n   z[1,1]     1.67     1.65 0.36 0.36     1.11     2.29 1.00     1214     2032\n   z[2,1]     0.04     0.04 0.39 0.37    -0.62     0.67 1.00     2184     2490\n   z[1,2]    -0.41    -0.40 0.26 0.25    -0.86     0.00 1.00      839     1684\n   z[2,2]    -0.92    -0.90 0.43 0.41    -1.68    -0.26 1.00     2123     2361\n   z[1,3]     2.06     2.04 0.42 0.41     1.41     2.80 1.00     1256     2335\n   z[2,3]    -0.10    -0.09 0.42 0.40    -0.81     0.58 1.00     2376     2473\n   z[1,4]    -0.62    -0.61 0.28 0.28    -1.09    -0.17 1.00      706     1606\n   z[2,4]    -1.04    -1.02 0.45 0.44    -1.80    -0.36 1.00     1918     2508\n   z[1,5]    -1.07    -1.06 0.32 0.32    -1.61    -0.57 1.01      770     1442\n\n # showing 10 of 145 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nlet’s take a look at these values:\n\ndata(\"mite.xy\", package = \"vegan\")\nmite.xy\n\n      x   y\n1  0.20 0.1\n2  1.00 0.1\n3  1.20 0.3\n4  1.40 0.5\n5  2.40 0.7\n6  1.80 0.9\n7  0.05 1.1\n8  2.00 1.3\n9  2.00 1.5\n10 1.20 1.7\n11 2.40 1.9\n12 0.20 2.1\n13 0.40 2.1\n14 2.00 2.3\n15 2.20 2.3\n16 0.05 2.7\n17 0.20 2.7\n18 2.20 2.7\n19 2.40 2.7\n20 1.20 2.9\n21 0.05 3.1\n22 1.40 3.1\n23 2.40 3.1\n24 0.20 3.5\n25 1.20 3.7\n26 0.80 3.9\n27 1.60 3.9\n28 0.20 4.1\n29 0.80 4.1\n30 1.80 4.5\n31 0.20 4.7\n32 1.40 4.7\n33 0.60 5.3\n34 1.00 5.3\n35 2.40 5.3\n36 1.40 5.5\n37 1.80 5.5\n38 0.40 5.9\n39 1.00 5.9\n40 1.80 5.9\n41 2.00 5.9\n42 0.05 6.1\n43 0.20 6.1\n44 0.40 6.1\n45 1.20 6.1\n46 1.60 6.1\n47 1.60 6.3\n48 0.40 6.5\n49 1.80 6.7\n50 0.60 6.9\n51 2.00 7.1\n52 0.05 7.3\n53 0.40 7.3\n54 1.40 7.5\n55 2.20 7.5\n56 0.20 7.9\n57 1.60 7.9\n58 2.40 7.9\n59 0.05 8.1\n60 1.20 8.1\n61 1.40 8.1\n62 2.00 8.1\n63 1.60 8.5\n64 1.60 8.7\n65 1.00 8.9\n66 1.60 8.9\n67 2.40 9.1\n68 2.20 9.3\n69 1.80 9.5\n70 0.40 9.7\n\nnrow(mite)\n\n[1] 70"
>>>>>>> 62de3affae8f7afb1b1017475a65579e2690017d
  }
]