---
title: "Matrix algebra"
title-slide-attributes: 
  data-background-image: ../img/bg.jpg
  data-background-size: full
author: "Andrew MacDonald and Guillaume Blanchet and Vincent Tolon"
date: "2023-02-10"
execute:
  echo: true
format: 
  revealjs:
    theme: [default]
    logo: ../img/UdeS_logo_h_rgbHR.png
    transition: slide
    background-transition: fade
---

## A general way to write matrices
$$
\mathbf{A} = \begin{bmatrix}
				A_{11} & A_{12} & \dots & A_{1j} & \dots & A_{1n}\\
				A_{21} & A_{22} & \dots & A_{2j} & \dots & A_{2n}\\
				\vdots & \vdots & \ddots & \vdots & & \vdots\\
			  A_{i1} & A_{i2} & \dots & A_{ij} & \dots & A_{in}\\
		    \vdots & \vdots & & \vdots & \ddots & \vdots\\
		    A_{m1} & A_{m2} & \dots & A_{mj} & \dots & A_{mn}\\
		\end{bmatrix}
$$
$$A = \left[a_{ij}\right]=\left[a_{ij}\right]_{m\times n}$$

# Basic matrix operations

## The transpose of a matrix

::::: { style="font-size: 0.65em"}
:::: {.columns}
::: {.column width="50%"}

$$A = \begin{bmatrix}
          5 & -6 & 4 & -4\\
        \end{bmatrix}
$$

$$B = \begin{bmatrix}
          -8\\
          9\\
          -2\\
        \end{bmatrix}
$$

$$C = \begin{bmatrix}
          -4 & 1\\
          2 & -5\\
        \end{bmatrix}
$$
:::

::: {.column width="50%"}

$$A^t=\begin{bmatrix}
          5\\
          -6\\
          4\\
          -4\\
        \end{bmatrix}
$$
    
 
$$B^t =\begin{bmatrix}
          -8 & 9 & -2\\
        \end{bmatrix}
$$
    

$$C^t =\begin{bmatrix}
          -4 & 2\\
          1 & -5\\
        \end{bmatrix}
$$
:::
::::

In R

```{r}
A <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)
A
t(A)
```
:::::
## Addition and Substraction

$$\mathbf{C} = \mathbf{A}\pm \mathbf{B}$$
$$C_{ij} = A_{ij} \pm B_{ij}$$

::: {style="color: blue;"}
$$\begin{bmatrix}
				3 & 5\\
				1 & -2\\
			\end{bmatrix} + 
			\begin{bmatrix}
				2 & 1\\
				4 & -2\\
			\end{bmatrix} = 
			\begin{bmatrix}
				3+2 & 5+1\\
				1+4 & -2-2\\
			\end{bmatrix} = 
			\begin{bmatrix}
				5 & 6\\
				5 & -4\\
			\end{bmatrix}$$
:::

In R

```{r}
A <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)
B <- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)
A + B
```

## Multiplying a matrix by a scalar
	
$$\mathbf{B} = c\mathbf{A}$$
$$B_{ij} = cA_{ij}$$
	
::: {style="color: blue;"}
$$
			0.3 \begin{bmatrix}
				3 & 5\\
				1 & -2\\
			\end{bmatrix} =  
			\begin{bmatrix}
				0.9 & 1.5\\
				0.3 & -0.6\\
			\end{bmatrix}
$$
:::		

In R

```{r}
A <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)
c <- 0.3
c*A 
```

## Matrix multiplications (not divisions!)
	
$$\mathbf{C} = \mathbf{A} \cdot \mathbf{B}$$

$$C_{ik} = \sum^{n}_{j=1}A_{ij}B_{jk}$$

**Rules**
	
Associative: $\mathbf{A}(\mathbf{B}\mathbf{C}) = (\mathbf{A}\mathbf{B})\mathbf{C}$ 
	
Distributive: $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}$
	
Not commutative: $\mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}$


## Inner product
$$(\mathbf{Ax})_i=\sum_{j=1}^{n}A_{ij}x_j$$

:::{style="color: blue;"}
$$
  \begin{bmatrix}
    3 & 5\\
    1 & -2\\
  \end{bmatrix}
  \begin{bmatrix}
    2\\ 5\\
  \end{bmatrix} = 
  \begin{bmatrix}
    (3, 5) \cdot (2, 5)\\
    (1, -2) \cdot (2, 5) \\
  \end{bmatrix} = 
  \begin{bmatrix}
    3 \times 2 + 5 \times 5\\
    1 \times 2 -2 \times 5\\
  \end{bmatrix} = 
  \begin{bmatrix}
    31\\
    -8\\
  \end{bmatrix}
$$
:::

In R

```{r}
A <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)
x <- matrix(c(2,5), nrow = 2, ncol = 1)
A %*% x
```

# Some important matrices


## Identity matrix

The identity matrix is a square matrix where all values of its diagonal are 0 except  the diagonal values which are all 1s.

:::: { style="font-size: 0.8em"}
::: {style="color: blue;"}
$$
\mathbf{I}=\begin{bmatrix}
				1 & 0 & 0\\
				0 & 1 & 0\\
				0 & 0 & 1\\
		\end{bmatrix}
$$
:::

The identity matrix is important because 

$$\mathbf{A} \cdot \mathbf{I}_n = \mathbf{A}$$
or 

$$\mathbf{I}_m \cdot \mathbf{A} = \mathbf{A}$$
::::


## Diagonal matrix

The diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.

:::: {style="font-size: 0.8em"}
$$D=
      \begin{bmatrix}
        d_1 & 0 & \dots & 0\\
        0 & d_2 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & d_n\\
\end{bmatrix}$$

An example

::: {style="color: blue;"}
$$
\begin{bmatrix}
				-1 & 0 & 0\\
				0 & 0 & 0\\
				0 & 0 & 6\\
		\end{bmatrix}
$$
:::
::::

## Triangular matrix

:::: {.columns}
::: {.column width="50%"}
#### Lower triangular matrix
$$\begin{bmatrix}
        -10 & 0 & 0 & 0\\
        3 & 0 & 0 & 0\\
        0 & 4 & 3 & 0\\
        9 & -5 & 4 & 3\\
\end{bmatrix}$$
:::
::: {.column width="50%"}
#### Upper triangular matrix
$$\begin{bmatrix}
        -10 & 0 & -5 & 0\\
        0 & 0 & 5 & 6\\
        0 & 0 & 3 & 3\\
        0 & 0 & 0 & 3\\
      \end{bmatrix}$$
:::
::::

## Symmetric matrix

The values on the  above and below the diagonal are match so that $A = A^t$

::: {style="color: blue;"}
$$
\begin{bmatrix}
				3 & 4 & -10\\
				4 & 5 & 7\\
				-10 & 7 & -6\\
		\end{bmatrix}
$$
:::

## Matrix inversion

::: { style="font-size: 0.8em"}
In matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix $\mathbf{A}$ is defined as $\mathbf{A}^{-1}$

As such,
$$\mathbf{A}\cdot \mathbf{A}^{-1} = \mathbf{I}$$ 
In R

```{r}
A <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)
(Ainv <- solve(A))
A %*% Ainv
```
:::

## Matrix inversion
### Inverting a diagonal matrix

$$D^{-1}=
      \begin{bmatrix}
        1/d_1 & 0 & \dots & 0\\
        0 &  1/d_2 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots &  1/d_n\\
\end{bmatrix}$$



## Cholesky decomposition

::: { style="font-size: 0.8em"}
The Cholesky decomposition allows to decompose a matrix in a triangular, which, when multiplied by its transposed will allow us to recover the initial matrix. 

In coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice

In math terms the Cholesky decomposition is defined as 
$$\mathbf{A} = \mathbf{L}\mathbf{L}^t$$
Example

$$
		\begin{bmatrix}
			1 & 1 & 1\\
			1 & 5 & 5\\
			1 & 5 & 14\\
		\end{bmatrix}=
		\begin{bmatrix}
			1 & 0 & 0\\
			1 & 2 & 0\\
			1 & 2 & 3 \\
		\end{bmatrix}
		\begin{bmatrix}
			1 & 1 & 1\\
			0 & 2 & 2\\
			0 & 0 & 3 \\
		\end{bmatrix}
$$
:::


## Cholesky decomposition

### Why is it useful ?

There are actually two main reasons :

1. Working with triangular matrices is computationally more efficient
2. It can be used to rescale matrices and make MCMC algorithms converge more easily
