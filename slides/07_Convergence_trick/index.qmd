---
title: "Convergence trick"
title-slide-attributes: 
  data-background-image: ../img/bg.jpg
  data-background-size: full
author: "Guillaume Blanchet -- Andrew MacDonald -- Vincent Tolon"
date: "2023-05-10"
execute:
  echo: true
format: 
  revealjs:
    theme: [default]
    logo: ../img/UdeS_logo_h_rgbHR.png
    transition: slide
    background-transition: fade
---

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

When estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure. 

However, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this

```{r, echo = FALSE, fig.width = 25,fig.height = 5,fig.align="center"}
set.seed(42)
auto <- as.vector(arima.sim(n=5000,
                            list(ar=c(0.999)),
                            mean=0.1))
plot(auto,
     type="l",
     ylab="",
     xlab="Iterations",
     las=1,
     cex.axis=2,
     cex.lab = 5,
     col= "blue",
     lwd = 4,
     xaxt = "n")
```
::: 

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

Even if you run the model for many, many (many !) iterations, it never seems to converge.

What should we do ?

![](https://www.i2symbol.com/pictures/emojis/a/3/6/b/a36b215220fc0153c107ee9d022cb75e_384.png){fig-align="center" width=20%}
::: 

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

There is a very cool trick that can help us here. 

Before we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution ($\mathcal{N}(0,1)$) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.

### The convergence trick

If we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.

:::: {style="text-align: center; font-size: 2em; color: red "}
Any ideas how to do this ?
::::
:::

::: {style="font-size: 0.9em"}
## The convergence trick

### Translation

```{r, echo=FALSE, fig.width=20,  fig.height=8, fig.align='center'}
val <- seq(-3.5, -0.75, length = 200)
marginal <- dnorm(val, mean = -2, sd = 0.2)

par(mar = c(0.1, 0.1, 0.1, 0.1))
plot(val,marginal,
     type = "n",
     xlim = c(-3,3),
     axes = FALSE,
     xlab = "",
     ylab = "") 

axis(1, tick = NA)

# Before
polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0, 0, 1, 0.5),
        border = NA)

lines(val, marginal,lwd = 3)

# After
polygon(x = c(val, val[1]) + 4,
        y = c(marginal,marginal[1]),
#        col = rgb(1, 165/ 255, 0, 0.5),
        col = rgb(0, 0, 1, 0.5),
        border = NA)

lines(val + 4, marginal,lwd = 3)

arrows(x0 = -1 ,
       y0 = 1, 
       x1 = 1,
       y1 = 1,
       length = 0.75,
       angle = 20,
       code = 2,
       col = "red",
       lwd = 10,
       lend = "round",
       ljoin = "mitre")

```
:::

## The convergence trick

### Translation

Mathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.

This means that 

$$\mathcal{N}(\mu, \sigma^2)$$
is exactly the same as 

$$\mathcal{N}(0, \sigma^2) + \mu$$

## The convergence trick

### Scaling

```{r, echo = FALSE, fig.width=10, fig.height=5, fig.align='center'}
par(mar=c(0.5,5,0.5,0.5))
couleur<-rainbow(5)
curve(dnorm, -20, 20, n = 10000, col = couleur[1], xaxt = "n",
      xlab = "", ylab = "Density", las = 1, cex.lab=3, lwd = 5, cex.axis = 1.5)
curve(dnorm(x,mean = 0, sd = 2), -1000, 1000, add=TRUE,
      n=10000, col="orange", lwd = 5)
curve(dnorm(x,mean = 0, sd = 5), -1000, 1000, add=TRUE,
      n=10000,col=couleur[3], lwd = 5)
curve(dnorm(x,mean = 0, sd = 8), -1000, 1000, add=TRUE,
      n=10000,col=couleur[4], lwd = 5)
curve(dnorm(x,mean = 0, sd = 10), -1000, 1000, add=TRUE,
      n=10000,col=couleur[5], lwd = 5)
curve(dnorm(x,mean = 0, sd = 50), -1000, 1000, add=TRUE,
      n=10000, lwd = 5)
legend("topright",legend=c(expression(sigma==1),
                           expression(sigma==2),
                           expression(sigma==5),
                           expression(sigma==8),
                           expression(sigma==10),
                           expression(sigma==50)),col=c(couleur[1],
                                                   "orange",
                                                   couleur[3:5],
                                                   "black"),
       lty=1, lwd=5, cex = 1.8)
```

## The convergence trick

### Scaling

Mathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.

This means that 

$$\mathcal{N}(\mu, \sigma^2)$$
is exactly the same as 

$$\mathcal{N}(\mu, 1) \times \sigma^2$$

## The convergence trick

The convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from **outside** the distribution

$$\mathcal{N}(0, 1) \times \sigma^2 + \mu$$
When implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient.

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.
:::

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.
:::

To do this, we need to work with a multivariate Gaussian distribution.

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.

To do this, we need to work with a multivariate Gaussian distribution.

The good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.
:::

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.

To do this, we need to work with a multivariate Gaussian distribution.

The good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.

To show how our convergence trick works for a multivariate Gaussian distribution, let's first visualize the two dimensional version of this distribution.
:::

## Bivariate Gaussian distribution

$$\mathcal{MVN}\left(
\begin{bmatrix}
  0\\
  0\\
\end{bmatrix},
\begin{bmatrix}
  2 & -1\\
  -1 & 2\\
\end{bmatrix}\right)$$
```{r, echo = FALSE, fig.align="center"}
library(mnormt)

#make this example reproducible
set.seed(42)

#create bivariate normal distribution
x     <- seq(-3, 3, 0.1) 
y     <- seq(-3, 3, 0.1)
mu    <- c(0, 0)
sigma <- matrix(c(2, -1, -1, 2), nrow=2)
f     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     <- outer(x, y, f)


par(mfrow = c(1,2), mar = c(2,2,2,2))
#create contour plot
contour(x, y, z, 
        asp =1, 
        las = 1, 
        col ="blue",
        lwd = 3)

#create surface plot
persp(x, y, z, 
      theta=-30,
      phi=25,
      expand=0.6,
      ticktype='detailed', 
      col = "blue")
```

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

#### Translation

For a multivariate distribution, a translation amounts to adding a **vector** of values to make the translation. 

Mathematically, this means that

$$\mathcal{MVN}\left(
\begin{bmatrix}
  \mu_1\\
  \vdots\\
  \mu_n\\
\end{bmatrix},
\mathbf{\Sigma}\right)=\mathcal{MVN}\left(
\begin{bmatrix}
  0\\
  \vdots\\
  0\\
\end{bmatrix},
\mathbf{\Sigma}\right) + \begin{bmatrix}
  \mu_1\\
  \vdots\\
  \mu_n\\
\end{bmatrix}$$

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

#### Scaling

Unlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform... But mathematician and statistician have worked hard to figure out how to do this properly.

However, we need to delve a little deeper into matrix algebra to understand how to scale a multivariate Gaussian distribution.

# Matrix algebra interlude (part 2!)

## Scaling a covariance matrix {auto-animate="true"}

::: {style="font-size: 0.8em"}
First recall that an covariance matrix $\mathbf{\Sigma}$ is a square matrix (i.e. it is an $n\times n$ matrix).

To scale $\mathbf{\Sigma}$, we cannot only multiply it by a scalar or even by a single matrix, we need to use the following matrix multiplication

$$\mathbf{A}\mathbf{\Sigma}\mathbf{A}^t$$
where $\mathbf{A}$ is a $p\times n$ matrix of weight to be used for the scaling (a "scaling" matrix) and $\mathbf{A}^t$ is its tranpose.

The technical reason why we **need** to use the equation above is to ensure that the resulting scaled covariance matrix also has an $n \times n$ dimension. 

If only
$$\mathbf{A}\mathbf{\Sigma}$$
is used the dimension of the resulting matrix also would be $p \times p$.
:::

## Square-root of a matrix

::: {style="font-size: 0.8em"}
Because in our problem weighting (or scaling) matrices are usually other covariance matrices, to apply the matrix scaling operation described previously, we need to find a way square-root a matrix.

This where the genious of André-Louis Cholesky comes to the rescue.
:::

![](https://upload.wikimedia.org/wikipedia/commons/5/5f/Andre_Cholesky.jpg){fig-align="center" width=20%}

## Square-root of a matrix

### Cholesky decomposition

::: { style="font-size: 0.8em"}
André-Louis Cholesky discovered a matrix decomposition approach probably around 1902 (so when he was 27 years old!), although it was attributed to him a few years after his death.

The Cholesky decomposition allows to decompose a square matrix in a triangular matrix, which, when multiplied by its transposed will allow us to recover the initial matrix. 

In coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice

In math terms the Cholesky decomposition is defined as 
$$\mathbf{A} = \mathbf{L}\mathbf{L}^t$$
:::

## Square-root of a matrix

### Cholesky decomposition

#### Example
$$\mathbf{A} = \mathbf{L}\mathbf{L}^t$$

$$
		\begin{bmatrix}
			1 & 1 & 1\\
			1 & 5 & 5\\
			1 & 5 & 14\\
		\end{bmatrix}=
		\begin{bmatrix}
			1 & 0 & 0\\
			1 & 2 & 0\\
			1 & 2 & 3 \\
		\end{bmatrix}
		\begin{bmatrix}
			1 & 1 & 1\\
			0 & 2 & 2\\
			0 & 0 & 3 \\
		\end{bmatrix}
$$

# End of matrix algebra interlude

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

#### Scaling

::: { style="font-size: 0.7em"}
To scale the following multivariate Gaussian distribution
$$\mathcal{MVN}\left(\boldsymbol{\mu},\mathbf{\Sigma}\right),$$ 

The following steps need to be applied 

1. Apply the Cholesky decomposition on the scaling matrix, here $\mathbf{\Sigma}$
$$\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^t$$
2. Multiply the $\mathbf{L}$ matrix to a standard variance multivariate Gaussian distribution

$$\mathbf{L}\cdot \mathcal{MVN}\left(\boldsymbol{\mu},\mathbf{I}\right)\cdot\mathbf{L}^t.$$

Recall, that $\mathbf{I}$ is the identity matrix.
:::

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

If we apply translation and scaling together on a multivariate Gaussian distribution, we get

$$\mathbf{L}\cdot \mathcal{MVN}\left(\mathbf{0},\mathbf{I}\right)\cdot\mathbf{L}^t + \boldsymbol{\mu}$$
When implementing in Stan some of the models we will discuss in this course, this convergence trick becomes very practical because it can lead a model to convergence much faster than without using this trick.