---
title: "Complex hierarchical models"
title-slide-attributes: 
  data-background-image: ../img/bg.jpg
  data-background-size: full
author: "Guillaume Blanchet -- Andrew MacDonald -- Vincent Tolon"
date: "2023-05-10"
execute:
  echo: true
format: 
  revealjs:
    theme: [default]
    logo: ../img/UdeS_logo_h_rgbHR.png
    transition: slide
    background-transition: fade
---

## "Complex" hierarchical model

By "complex" we refer to hierarchical models for which more than one parameters are accounted for in a parameter hierarchy. 

As we will see, there are a number of ways this can complexify the structure of a model in ways that are not always obvious.

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

When estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure. 

However, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this

```{r, echo = FALSE, fig.width = 25,fig.height = 5,fig.align="center"}
set.seed(42)
auto <- as.vector(arima.sim(n=5000,
                            list(ar=c(0.999)),
                            mean=0.1))
plot(auto,
     type="l",
     ylab="",
     xlab="Iterations",
     las=1,
     cex.axis=2,
     cex.lab = 5,
     col= "blue",
     lwd = 4,
     xaxt = "n")
```
::: 

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

Even if you run the model for many, many (many !) iterations, it never seems to converge.

What should we do ?

![](https://www.i2symbol.com/pictures/emojis/a/3/6/b/a36b215220fc0153c107ee9d022cb75e_384.png){fig-align="center" width=20%}
::: 

::: {style="font-size: 0.9em"}
## Playing with the Gaussian distribution

There is a very cool trick that can help us here. 

Before we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution ($\mathcal{N}(0,1)$) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.

### The convergence trick

If we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.

:::: {style="text-align: center; font-size: 2em; color: red "}
Any ideas how to do this ?
::::
:::

::: {style="font-size: 0.9em"}
## The convergence trick

### Translation

```{r, echo=FALSE, fig.width=20,  fig.height=8, fig.align='center'}
val <- seq(-3.5, -0.75, length = 200)
marginal <- dnorm(val, mean = -2, sd = 0.2)

par(mar = c(0.1, 0.1, 0.1, 0.1))
plot(val,marginal,
     type = "n",
     xlim = c(-3,3),
     axes = FALSE,
     xlab = "",
     ylab = "") 

axis(1, tick = NA)

# Before
polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0, 0, 1, 0.5),
        border = NA)

lines(val, marginal,lwd = 3)

# After
polygon(x = c(val, val[1]) + 4,
        y = c(marginal,marginal[1]),
#        col = rgb(1, 165/ 255, 0, 0.5),
        col = rgb(0, 0, 1, 0.5),
        border = NA)

lines(val + 4, marginal,lwd = 3)

arrows(x0 = -1 ,
       y0 = 1, 
       x1 = 1,
       y1 = 1,
       length = 0.75,
       angle = 20,
       code = 2,
       col = "red",
       lwd = 10,
       lend = "round",
       ljoin = "mitre")

```
:::

## The convergence trick

### Translation

Mathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.

This means that 

$$\beta \sim \mathcal{N}(\mu, \sigma^2)$$
is exactly the same as 

$$\beta \sim \left(\mathcal{N}(0, \sigma^2) + \mu\right)$$

## The convergence trick

### Scaling

```{r, echo = FALSE, fig.width=10, fig.height=5, fig.align='center'}
par(mar=c(0.5,5,0.5,0.5))
couleur<-rainbow(5)
curve(dnorm, -20, 20, n = 10000, col = couleur[1], xaxt = "n",
      xlab = "", ylab = "Density", las = 1, cex.lab=3, lwd = 5, cex.axis = 1.5)
curve(dnorm(x,mean = 0, sd = 2), -1000, 1000, add=TRUE,
      n=10000, col="orange", lwd = 5)
curve(dnorm(x,mean = 0, sd = 5), -1000, 1000, add=TRUE,
      n=10000,col=couleur[3], lwd = 5)
curve(dnorm(x,mean = 0, sd = 8), -1000, 1000, add=TRUE,
      n=10000,col=couleur[4], lwd = 5)
curve(dnorm(x,mean = 0, sd = 10), -1000, 1000, add=TRUE,
      n=10000,col=couleur[5], lwd = 5)
curve(dnorm(x,mean = 0, sd = 50), -1000, 1000, add=TRUE,
      n=10000, lwd = 5)
legend("topright",legend=c(expression(sigma==1),
                           expression(sigma==2),
                           expression(sigma==5),
                           expression(sigma==8),
                           expression(sigma==10),
                           expression(sigma==50)),col=c(couleur[1],
                                                   "orange",
                                                   couleur[3:5],
                                                   "black"),
       lty=1, lwd=5, cex = 1.8)
```

## The convergence trick

### Scaling

Mathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.

This means that 

$$\beta \sim \mathcal{N}(\mu, \sigma^2)$$
is exactly the same as 

$$\beta \sim \left(\mathcal{N}(\mu, 1) \times \sigma^2\right)$$

## The convergence trick

The convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from **outside** the distribution

$$\beta \sim \left(\mathcal{N}(0, 1) \times \sigma^2 + \mu\right)$$
When implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient.

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.
:::

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.
:::

To do this, we need to work with a multivariate Gaussian distribution.

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.

To do this, we need to work with a multivariate Gaussian distribution.

The good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.
:::

## The convergence trick {auto-animate="true"}

:::{style="font-size: 0.95em"}
The example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.

To do this, we need to work with a multivariate Gaussian distribution.

The good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.

To show how our convergence trick works for a multivariate Gaussian distribution, let's first visualize the two dimensional version of this distribution.
:::

## Bivariate Gaussian distribution

$$\mathcal{MVN}\left(
\begin{bmatrix}
  0\\
  0\\
\end{bmatrix},
\begin{bmatrix}
  2 & -1\\
  -1 & 2\\
\end{bmatrix}\right)$$
```{r, echo = FALSE, fig.align="center"}
library(mnormt)

#make this example reproducible
set.seed(42)

#create bivariate normal distribution
x     <- seq(-3, 3, 0.1) 
y     <- seq(-3, 3, 0.1)
mu    <- c(0, 0)
sigma <- matrix(c(2, -1, -1, 2), nrow=2)
f     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     <- outer(x, y, f)


par(mfrow = c(1,2), mar = c(2,2,2,2))
#create contour plot
contour(x, y, z, 
        asp =1, 
        las = 1, 
        col ="blue",
        lwd = 3)

#create surface plot
persp(x, y, z, 
      theta=-30,
      phi=25,
      expand=0.6,
      ticktype='detailed', 
      col = "blue")
```

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

#### Translation

For a multivariate distribution, a translation amounts to adding a **vector** of values to make the translation. 

Mathematically, this means that

$$\mathcal{MVN}\left(
\begin{bmatrix}
  \mu_1\\
  \vdots\\
  \mu_n\\
\end{bmatrix},
\mathbf{\Sigma}\right)=\mathcal{MVN}\left(
\begin{bmatrix}
  0\\
  \vdots\\
  0\\
\end{bmatrix},
\mathbf{\Sigma}\right) + \begin{bmatrix}
  \mu_1\\
  \vdots\\
  \mu_n\\
\end{bmatrix}$$

## The convergence trick {auto-animate="true"}

### Multivariate Gaussian distribution

#### Scaling

Unlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform... But mathematician and statistician have worked hard to figure out how to do this properly.

However, we need to delve deeper into matrix algebra.

ICICI

# "Complex" hierarchy on the intercept

## Interacting hierarchies

::: {style="font-size: 0.7em"}
`lme4` notation : `y ~ (1 | f:g)`

This model assumes that factors `f` and `g` interact to make a hierarchy.

Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{f[l_f]\times g[l_g] },\sigma^2\mathbf{I}) \quad\forall\quad l_f = 1\dots k_f\,\,\,\,\text{and}\,\,\,l_g = 1\dots k_g$$
or 

$$y_i = b_{f[l_f]\times g[l_g]} + \varepsilon \quad\forall\quad l_f = 1\dots k_f\,\,\,\,,\,\,\,l_g = 1\dots k_g\,\,\,\,\text{and}\,\,\,i = 1\dots n$$
In words, a multi-factor hierarchy can be constructed my multiplying the levels of individual factors to account for a more complexe hierarchy. Another way to think about this structure is to construct a complexe hierarchy using multiple simpler hierarchies. 

This is because in this model

$$\mathbf{b} \sim \mathcal{N}\left(0, \sigma^2_{f\times g}\right)$$
:::

## Interacting hierarchies

```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}
zones=matrix(c(1,2), ncol=2, byrow=TRUE)
layout(zones, widths=c(1/5,4/5), heights=c(1/5,4/5))

val <- seq(-1, 3, length = 200)
marginal <- dnorm(val, mean = 1, sd = 0.5)

par(mar = c(0.1, 0.1, 0.1, 0.1))
plot(-marginal,val,
     type = "n",
     ylim = c(-3,3),
     axes = FALSE,
     xlab = "",
     ylab = "") 

polygon(x = c(-marginal,-marginal[1]),
        y = c(val, val[1]),
        col = rgb(170/255, 103/255, 57/255, 0.5),
        border = NA)

lines(-marginal,val, lwd = 3)

plot(0,0,
     type = "n",
     xlim = c(0,5),
     ylim = c(-3,3),
     bty="L",
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",) 

set.seed(40)
abline(h = rnorm(30, mean = 1, sd = 0.5),
       lwd = 3,
       col = rgb(170/255, 103/255, 57/255))

legend("topright",
       legend = c("f", "g"),
       fill = c("blue", "orange"),
       cex = 2.5)
```


## Hierarchies within hierarchies

::: {style="font-size: 0.7em"}
`lme4` notation : `y ~ (1 | f/g)` or `y ~ (1 | f) + (1 | f:g)`

This model assumes there is a hierarchy that varies among the levels of factor `f` and among the levels of factor `g` but only within the levels of factor `f`.

Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{g[l_g]\in f[l_f]},\sigma^2\mathbf{I}) \quad\forall\quad l_f = 1\dots k_f\,\,\,\,\text{and}\,\,\,l_g = 1\dots k_g$$
or 

$$y_i = b_{g[l_g]\in f[l_f]} + \varepsilon \quad\forall\quad l_f = 1\dots k_f\,\,\,\,,\,\,\,l_g = 1\dots k_g\,\,\,\,\text{and}\,\,\,i = 1\dots n$$
In words, the model parameter $b$ will change for a sample $i$ only when the level $l_g$ of the factor $g$ **within** the level $l_f$ of the factor $f$ changes. 

This is because in this model

$$\mathbf{b} \sim \mathcal{N}\left(0,\sigma^2_{g\in f}
                                \right)$$
:::

## Hierarchies within hierarchies

ICI



## `y ~ (1 | f/g)`
### Stan code for this model

::: {style="margin-top: 200px; font-size: 2.5em; color: red;"}
For Andrew?
:::

## `y ~ (1 | f/g)`
```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}
# Stuff
```

## `y ~ (1 | f) + (1 | g)`

::: {style="font-size: 0.7em"}
Other notation used : `y ~ 1 + (1 | f) + (1 | g)`

This model assumes there is a hierarchy that varies among the two factors.

Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{f[l_f]\times g[l_g]},\sigma^2\mathbf{I}) \quad\forall\quad l_f = 1\dots k_f\,\,\,\,\text{and}\,\,\,l_g = 1\dots k_g$$
or 

$$y_i = b_{f_i[l_f]\times g_i[l_g]} + \varepsilon \quad\forall\quad l_f = 1\dots k_f\,\,\,\,,\,\,\,l_g = 1\dots k_g\,\,\,\,\text{and}\,\,\,i = 1\dots n$$
In words, this means that the model values $b$ will change for a sample $i$ only when the interaction the level $l_f$ of the factor $f$ and the  level $l_g$ of the factor $g$ changes. 

This is because in this model

$$\mathbf{b} \sim \mathcal{N}\left(0, 
                                \begin{bmatrix}
                                  \sigma^2_f & \sigma_f\sigma_g\\
                                  \sigma_f\sigma_g& \sigma^2_g\\
                                \end{bmatrix}
                                \right)$$
:::

## `y ~ (1 | f) + (1 | g)`

This model has a number interesting properties

- It does **not** assumes that the two factors act independent. Actually, if you are interested in such a model, the code to use is not as straight forward to write with these packages.
- If a particular levels is associated to the same samples for the two factors, usually this create technical problems and the model cannot be estimated properly (this is true regardless of how you estimate these parameter)
- This can be generalized to as many factors as we want. We will see how this can be useful later.


## `y ~ x + (x || g)`

## second

::: r-fit-text
Test your model
:::

## 

img:

![](img/bg.jpg)

it is a landscape

##  {auto-animate="true"}

::: {style="margin-top: 100px;"}
check with simulations
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 2.5em; color: red;"}
check with simulations
:::

## choose parameters {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
```

## make up an X variable {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
```

## calculate the average {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
```

## simulate some observations {auto-animate="TRUE"}

```{r}
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
```

## finally, visualize

```{r}
plot(y_obs ~ x)
```

## here it is all on one slide

```{r}
#| output-location: column
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
plot(y_obs, x)
```

## Or we can present the code and results separately

::: panel-tabset
### The code

```{r}
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
```

### The figure

```{r}
plot(y_obs ~ x)
```
:::

## another equation

$$
2 + 4 = 6
$$

## The equation

$$
\begin{align}
y  &\sim \text{N}(\mu, \sigma_{obs}) \\
\mu &= a + bx \\
\end{align}
$$

## The model {auto-animate="TRUE"}

``` stan
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## Declare the data {auto-animate="TRUE"}

``` {.stan code-line-numbers="1-4"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## State parameters {auto-animate="TRUE"}

``` {.stan code-line-numbers="5-8"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## Write the likelihood and priors {auto-animate="TRUE"}

``` {.stan code-line-numbers="9-13"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```
