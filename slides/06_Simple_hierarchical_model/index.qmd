---
title: "'Simple' hierarchical models"
title-slide-attributes: 
  data-background-image: ../img/bg.jpg
  data-background-size: full
author: "Andrew MacDonald and Guillaume Blanchet and Vincent Tolon"
date: "2023-02-10"
execute:
  echo: true
format: 
  revealjs:
    theme: [default]
    logo: ../img/UdeS_logo_h_rgbHR.png
    transition: slide
    background-transition: fade
---

## "Simple" hierarchical model

Here, we use the term "simple" in a rather loose way to discuss hierarchical models without any constrains, whether they are spatial, temporal, phylogenetic or others. 

Futhermore, for most of this lecture, we will focus on models with a Gaussian error term to develop the underlying theory. 

When we will have done this, it will be reasonably straight forward to move to non-Gaussian hierarchical model.


## A (very !) general formulation

::: {style="font-size: 0.68em"}
As discuss yesterday, a linear model can be writen as

$$\mathbf{y}\sim \mathcal{MVN}(\mathbf{X} \boldsymbol{\beta}, \sigma^2\mathbf{I})$$

where

- $\mathbf{y}$ is a vector quantifying a response variable of length $n$
- $\mathbf{X}$ is a matrix of explanatory variables with $n$ rows (samples) and $p$ columns (explanatory varaibles) 
- $\boldsymbol{\beta}$ is a vector $p$ pararameters weighting the importance of each explanatory variables in $\mathbf{X}$
- $\sigma^2$ is a measure of variance of the error in the regression model
- $\mathbf{I}$ is an $n \times n$ identity matrix 
:::

## A (very !) general formulation

::: {style="font-size: 0.8em"}
A hierarchical model is a generalization of the linear model such that 

$$(\mathbf{y}|\mathbf{b} )\sim \mathcal{MVN}(\mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{b}, \sigma^2\mathbf{I})$$
:::
::: {style="font-size: 0.68em"}

where

- $\mathbf{y}$ is a vector quantifying a response variable of length $n$
- $\mathbf{X}$ is a matrix of explanatory variables with $n$ rows (samples) and $p$ columns (explanatory variables) 
- $\boldsymbol{\beta}$ is a vector $p$ pararameters weighting the importance of each explanatory variables in $\mathbf{X}$
- $\sigma^2$ is a measure of variance of the error in the regression model
- $\mathbf{I}$ is an $n \times n$ identity matrix 

:::: {style="color: blue"}
- $\mathbf{Z}$ is another matrix of explanatory variables with $n$ rows (samples) and $q$ columns (explanatory variables) 
- $\mathbf{b}$ is a vector $q$ pararameters weighting the importance of each explanatory variables in $\mathbf{Z}$
::::
:::

## A (very !) general formulation

::: {style="font-size: 0.8em"}
A hierarchical model is a generalization of the linear model such that 

$$(\mathbf{y}|\mathbf{b} )\sim \mathcal{MVN}(\mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{b}, \sigma^2\mathbf{I})$$
What is also noticeable in this model is the conditional relationship between $\mathbf{y}$ and $\mathbf{b}$.

Specifically, in this formulation, 

$$\mathbf{b}\sim \mathcal{MVN}(\mathbf{0}, \mathbf{\Sigma})$$
where $\mathbf{\Sigma}$ is a covariance matrix.

Based on this  general formulation, we can now define all unconstrained hierarchical models.
:::


## The "`|`"

::: {style="font-size: 0.8em"}
Most of you have probably already used the packages `lme4` or `brms` to build hierarchical models and in so, have used the `|` to include a hierachy in your model.

But do you know what the underlying mathematical structure of the model you built look like ? Does it really answer the question you were asking ?
:::

![](https://www.i2symbol.com/pictures/emojis/2/1/4/4/21445641aeaca6c4436579b3fa30772b_384.png){fig-align="center" width=20%}

::: {style="font-size: 0.8em"}
Let's look at different `lme4` models
:::

## A bit of notation

Before we get into writing math, we need to define a bit of notation in addition of the one we have used so far.

Specifically, when define a hierarchy in a model, it is common to do this using at least one factor. Mathematically, we will define the different level of a factor in a model by a subscript. 

We will use square brackets to define the level of interest

### Example

$$\mathbf{Z}_{f[l]}$$
This means that, within $\mathbf{Z}$, we focus on $l^{\text{th}}$ level of factor $f$.

## `y ~ (1 | f)`

::: {style="font-size: 0.8em"}
Other notation used : `y ~ 1 + (1 | f)`

This model assumes there is a hierarchy solely on the intercept.

Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{f[l]},\sigma^2\mathbf{I}) \quad \forall\quad l = 1\dots k$$
or 

$$y_i = b_{{f_i[l]}} + \varepsilon \quad \forall\quad l = 1\dots k$$
In words, this means that the model values $b$ will change for a sample $i$ only when the level $l$ of the factor $f$ changes. 

This is because in this model

$$\mathbf{b} \sim \mathcal{N}(0, \sigma^2_f)$$
:::

## `y ~ (1 | f) + (1 | g)`

::: {style="font-size: 0.7em"}
Other notation used : `1 + (1 | f) + (1 | g)`

This model assumes there is a hierarchy that varies among the two factors.

Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{f[l_f]\times g[l_g]},\sigma^2\mathbf{I}) \quad\forall\quad l_f = 1\dots k_f\,\,\,\,\text{and}\,\,\,l_g = 1\dots k_g$$
or 

$$y_i = b_{f_i[l_f]\times g_i[l_g]} + \varepsilon \quad\forall\quad l_f = 1\dots k_f\,\,\,\,\text{and}\,\,\,l_g = 1\dots k_g$$
In words, this means that the model values $b$ will change for a sample $i$ only when the interaction the level $l_f$ of the factor $f$ and the  level $l_g$ of the factor $g$ changes. 

This is because in this model

$$\mathbf{b} \sim \mathcal{N}\left(0, 
                                \begin{bmatrix}
                                  \sigma^2_f & \sigma_f\sigma_g\\
                                  \sigma_f\sigma_g& \sigma^2_g\\
\end{bmatrix}
\right)$$
:::

## `y ~ (1 | f) + (1 | g)`

This model has a number interesting properties

- It does **not** assumes that the two factors act independent. Actually, if you are interested in such a model, the code to use is not as straight forward to write with these packages.
- If a particular levels is associated to the same samples for the two factors, usually this create technical problems and the model cannot be estimated properly (this is true regardless of how you estimate these parameter)
- This can be generalized to as many factors as we want. We will see how this can be useful later.

## `y ~ (1 | f/g)`

## `y ~ x + (x | g)`

## `y ~ x + (x || g)`


## How many levels ?

A common question that often gets asked is :

"How many level is enough ?"

This is a simple questions that sadly does not have a simple answer.

![](https://www.i2symbol.com/pictures/emojis/7/e/1/8/7e1820e72993db112424b5a92e1d40d7_384.png){fig-align="center" width=20%}

## second

::: r-fit-text
Test your model
:::

## 

img:

![](img/bg.jpg)

it is a landscape

##  {auto-animate="true"}

::: {style="margin-top: 100px;"}
check with simulations
:::

##  {auto-animate="true"}

::: {style="margin-top: 200px; font-size: 2.5em; color: red;"}
check with simulations
:::

## choose parameters {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
```

## make up an X variable {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
```

## calculate the average {auto-animate="TRUE"}

``` r
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
```

## simulate some observations {auto-animate="TRUE"}

```{r}
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
```

## finally, visualize

```{r}
plot(y_obs ~ x)
```

## here it is all on one slide

```{r}
#| output-location: column
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
plot(y_obs, x)
```

## Or we can present the code and results separately

::: panel-tabset
### The code

```{r}
yintercept <- 4
slope <- 1.3
obs_error <- .5
x <- runif(200, min = -1, max = 1)
y_mean <- yintercept + slope * x
y_obs <- rnorm(200, mean = y_mean, sd = obs_error)
```

### The figure

```{r}
plot(y_obs ~ x)
```
:::

## another equation

$$
2 + 4 = 6
$$

## The equation

$$
\begin{align}
y  &\sim \text{N}(\mu, \sigma_{obs}) \\
\mu &= a + bx \\
\end{align}
$$

## The model {auto-animate="TRUE"}

``` stan
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## Declare the data {auto-animate="TRUE"}

``` {.stan code-line-numbers="1-4"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## State parameters {auto-animate="TRUE"}

``` {.stan code-line-numbers="5-8"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```

## Write the likelihood and priors {auto-animate="TRUE"}

``` {.stan code-line-numbers="9-13"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 1);
  sigma ~ exponential(1);
}
```
