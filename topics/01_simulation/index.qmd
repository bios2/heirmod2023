---
title: "Data simulation"
description: |
  Expressing yourself through made-up numbers.
execute:
  freeze: true
format:
  html:
    code-tools: true
---

```{r, message=FALSE}
library(tidyverse)
library(cmdstanr)
```


Before starting work on real data, we are going to begin by learning how to make up some of our own.
There are at least three reasons why this is a good idea:

1.  **Understand your priors.**. For most interesting models in ecology, you will not be able to pick good numbers for your prior parameters just by thinking hard. Should this be $\text{Normal}(2, 1)$ ?? or should the standard deviation be bigger? smaller? As we'll see, simulation will demystify the process 
1. **Validate your model.** Bayesian models are great because they can create datasets by simulation. The _very minimum_ requirement we might have for our models is that, if we use a model to make a dataset dataset, we should be able to fit that model to the simulated data to recover the true parameter values. This lets us know that the sample size and power of our analysis is correct.
1. **Test your understanding.** Perhaps most importantly, simulation helps you to test your own intuition.
If you can write it in Stan, or if you can simulate data from your model, then you really understand it!
If you can't, then you don't know quite how it works yet.

## Simple exercise in simulation

Let's imagine we are taking a walk as a group today at the beautiful SBL. What is the number of birds each of us is going to see on our hike?

### Some questions to ask about simulated data

1. What kind of observations are you going to make? Do they have a minimum or maximum value?
Are they integers, or are they decimal numbers, or something else?
1. Where do the numbers come from? This could be anything, from simple linear approximations (ie the models we're looking at in this course) to ODEs, mathematical models, GAMs, etc. 
1. How many observations will we be making?

### The process

let's try to answer these questions for birds

1. We're going to count birds, so we'll have count data: a number that is either 0 or some positive, round number
2. We'll make a simplifying assumption: everybody has the same chance of seeing a bird (ie no differences in skill or equipment), and everyone is the class is an independent observer (ie nobody is working in pairs, etc) 
3. Everyone in the class makes only one count, so we have 23 (?) numbers.

We're bayesians, so we need to write a probability distribution for all the possible values

$$
\begin{align}
\text{Number of Birds}_{\text{seen by person i}} &\sim \text{Poisson}(\lambda) \\
\lambda &\sim \text{Uniform}(0, 60)
\end{align}
$$

A quick note about notation for models like these:

* We use a subscript $i$ to indicate the "label" for each observation in our dataset. You can think of this as the row number of the data spreadsheet, and imagine sliding your finger down the column of measurements, modelling each value in turn.
* Usually we'll use more general language, such as $y_i$. But for this simple example I wanted to make things as explicit as possible. 
* Notice the symbol $\sim$. This is read as "distributed as", and indicates the probability distribution from which the values might come.  When the values we're talking about are data that we can observe (in this case, ), we call the distribution the likelihood. When the value is something we can't observe (in this case, the average count $\lambda$) we call the distribution the prior.

::: {.callout-warning}
We'll be talking about better ways to model count data in a later exercise! For now, I'm using the Uniform distribution for simplicity. It's not usually a very good choice! 
:::

### Simulation in R

One of the most useful traits of bayesian models is that they are _generative_: they can be used to make a simulated dataset. 
We'll do that now for our bird example.

let's simulate from a poisson distribution:

```{r}
set.seed(525600)
n_people <- 21
avg_birds_per_person <- runif(1, min = 0, max = 30)
bird_count <- rpois(n_people, lambda = avg_birds_per_person)
```

Some things to note in the code above: 

#### R statistical distributions 

Every statistical distribution that is in R (which is a lot! almost all! ) has four different functions. 
If the distribution is called `dist`, then they are:

* `rdist` = the distribution functions 
* `qdist` = the quantile functions 
* `pdist` = the probability density function 
* `ddist` the density function

Let's take a look at our simulated values:

```{r}
#| fig-cap: Histogram of simulated counts of birds

hist(bird_count, col = "lightblue", xlim = c(0, 50))
```

This is pretty great, and represents one possible realization of sampling. 
However, one sample isn't enough to tell us about what our $\text{Uniform}(0, 60)$ prior really means. Let's do a few different simulations:

```{r}
#| fig-cap: Twelve different simulations of a possible bird dataset. Do all of these seem plausible? 

library(tidyverse)

set.seed(525600)

simulate_some_birds <- function() {
  lambda <- runif(1, min = 0, max = 60)
  data.frame(obs = rpois(23, lambda = lambda))
}
  
purrr::rerun(.n = 12, simulate_some_birds()) |> 
  dplyr::bind_rows(.id = "sample") |> 
  ggplot(aes(x = obs)) + 
  geom_histogram() + 
  facet_wrap(~sample) + 
  theme_bw() + 
  labs(x = "Number of birds observed per person")
```

This figure shows different simulations of what, according to our prior, might be reasonable datasets for us to study. Do any of them seem implausible to you? If so, try changing the prior. The goal is to make fake datasets that _seem_ plausible, but which still include the possiblity of suprising observations. 

## Simulating data in Stan

Let's look back at the equation:

$$
\begin{align}
\text{Number of Birds}_{\text{seen by person i}} &\sim \text{Poisson}(\lambda) \\
\lambda &\sim \text{Uniform}(0, 60)
\end{align}
$$

And then translate it into Stan:

```{r}
#| class-output: stan
poisson_simulation <- cmdstan_model(
  stan_file = "topics/01_simulation/poisson_simulation.stan")

poisson_simulation
```

This stan program has two parts

```stan

data {
  int<lower=0> n_people;
}
```

And the generated quantites block

```stan
generated quantities {
  // simulate a population average
  real<lower=0> avg_observed;
  avg_observed = uniform_rng(0, 60);
  // simulate observations with that average
  // an array -- like a list in R
  array[n_people] int<lower=0> observations;
  for (i in 1:n_people){
    observations[i] = poisson_rng(avg_observed);
  }
}
```

Let's look at similarities and differences to R 

similarities: 
* we have a random number generating function for each of our distributions
* 

PINEAPPLE could this be a two-column output so they are side by side?



## parameter recovery

Ok so now we can see our first simple stan model!
we are using this powerful tool to draw forty random numbers.


in R : fitdistr, glm

Now we can move on to the second step outlined above: we can just fit the same data to our model, vice versa, and see if we can recover that parameter.

In R, there are several ways to do it: first, we can use R do it in two ways: fitdistr, and glm.

Then we do the same thing in Stan.

Then we look to see if we have recovered our parameter.

The next steop in visualization, which we also do with this simple model.

## Bonus material: conjugacy

You would never actually do the analysis on this page. In reality, for simple distributions such as the Poisson, we would 

## Exercises

* 