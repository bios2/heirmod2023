{
  "hash": "ce9520376f1498e317917d75d4ed27d4",
  "result": {
    "markdown": "---\ntitle: \"Univariate regression\"\ndescription: |\n  The shortest route to science is a straight line.\nexecute:\n  freeze: true\ncomments:\n  hypothesis: true\nformat:\n  html:\n    code-tools: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Simulation workout! \n\n1) make a histogram of 500 numbers from a distribution! \n   * normal \n   * poisson\n   * ** EXTRA** try a new one, like beta, gamma, lognormal\n   \n2) make a histogram of poisson observations, using the classic **`log()` link function**. \n\n$$\n\\begin{align}\ny &\\sim \\text{Poisson}(e^a) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n$$\n\n:::{.callout-note collapse=\"true\"}\n### TIP\n\n::: {.cell}\n\n```{.r .cell-code}\n## sample poisson variables like this:\nrpois(500, exp(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 23 26 27 16 25 16 14 21 15 18 26 19 22 21 20 23 22 21 19 15 18 19 18 24 23\n [26] 18 20 14 18 15 21 22 24 23 15 19 18 23 18 22 20 15 22 11 25 28 23 19 30 25\n [51] 16 21 20 24 23 19 14 21 16 31 19 16 19 19 20 26 15 15 15 25 29 12 24 19 26\n [76] 16 18 21 20 22 26 27 24 15 26 33 22 23 26 23 22 23 20 28 17 12 14 20 30 20\n[101] 15 24 21 31 20 13 18 17 20 15 27 14 13 13 16 25 24 21 16 21 19 26 14 18 26\n[126] 14 24 18 20 20 23 16 25 24 19 16 15 23 15 17 21 18 16 11 26 22 28 15 20 21\n[151] 19 23 18 18 22 26 23 22 13 27 24 14 21 22 31 13 24 19 26 18 25 30 12 18 15\n[176] 25 17 22 22 17 26 23 19 24 21 16 21 24 17 20 16 18 25 26 18 12 22 21 13 19\n[201] 20 22 19 21 19 13 20 23 12 26 14 19 16 22 19 21 19 29 20 19 26 21 19 28 19\n[226] 17 15 19 22 12 24 15 13 19 23 17 22 18 16 28 23 21 24 23 19 23 19 24 18 22\n[251] 17 17 24 19 18 21 18 24 17 32 26 27 26 28 22 23 17 23  9 15 23 33 16 15 17\n[276] 35 16 19 13 14 18 19 18 14 28 21 16 19 21 19 17 17 12 21 20 17 20 21 13 20\n[301] 17 13 22 18 21 22 14 18 22 18 21 20 12 16 22 15 19 24 16 19 19 13 24 16 20\n[326] 16 12 16 28 23 24 20 13 15 24 19 18 29 17 18 19 22 15 14 26 20 24 26 22 23\n[351] 18 16 18  9 14 24 17 32 29 19 18 22 19 27 16 26 17 23 25 22 23 22 19 20 23\n[376] 15 28 24 10 19 23 27 18 15 23 20 14 20 19 28 13 30 21 14 14 25 17 21 22 21\n[401] 19 16 19 16 15 31 23 27 20 22 12 24 21 11 28 17 21 19 29 29 23 23 21 15 28\n[426] 16 19 15 25 12 12 20 19 15 18 24 24 23 21 21 29 21 19 26 22 21 17 21 17 17\n[451] 24 19 15 26 23 22 25 21 25 16 16 17 16 20 16 29 25 23 13 19 17 24 15 31 16\n[476] 22 23 19 17 27 17 15 15 15 19 16 14 17 17 25 15 29 22 19 28 19 23 23 18 29\n```\n:::\n:::\n\n:::\n\n3)  make a histogram of Binomial observations, using the **inverse logit link function**\n\n$$\n\\begin{align}\ny &\\sim \\text{Binomial}\\left(\\frac{1}{1+e^{-a}}, N \\right) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n$$\n\nHere's a plot of the link function, to help you think about it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(1 / (1 + exp(-x)), xlim = c(-3, 3), ylim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n### TIP\n\n::: {.cell}\n\n```{.r .cell-code}\na <- rnorm(1, mean = 0, 1)\nhist(rbinom(n = 500, size = 50, prob = 1 / (1 + exp(-a))))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n\n\n## Statistical models of Penguin bill morphology.\n\nWe'll be studying the relationship between two numbers about penguin bills. \nSpecifically, we'll ask **\"Are longer bills also deeper?\"**. \nThis question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\n\nLet's begin with plotting the data: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(cmdstanr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is cmdstanr version 0.7.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan path: /home/andrew/software/cmdstan\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan version: 2.34.1\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\npenguins |> \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![Bill depth (mm) as predicted by bill length (mm) across the entire `palmerpenguins` dataset.](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nLet's write a simple statistical model for these data:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n$$\n\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n:::{.callout-warning}\n# WHERE IS ZERO??\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n:::\n\nIf we fit a model like this **without** thinking about the location of zero, we get some pretty silly answers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_line <- coef(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n```\n:::\n\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\n$$\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n$$\n\nBut, if we take the data as we found it, we're going to be talking about $\\beta_0$ as the depth of a penguin's bill _when the bill has 0 length!_ Either way it is the same line. However, from the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\n\nA very common choice is to **subtract the average** from your independent variable, so that it is equal to 0 at the average:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n$$\n\nNow $\\beta_0$ means the average _bill depth_ at the average _bill length_.  It becomes easier to think about priors:\n\n$$\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n$$\n\n:::{.callout-note}\n## Exercise\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc.\n:::\n\n## Prior predictive simulations\n\nArmed with this model, it becomes much easier to think about prior predictions.\n\nWe'll make a bunch of lines implied by the equation above. There's two steps:\n\n1. Center the predictor\n2. Make up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_len_centered <- with(penguins,\n                          bill_length_mm - mean(bill_length_mm,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths <- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n```\n:::\n\n\n:::{.callout-warning}\n## Shortcuts to these common tasks\n\nThese tasks are so common that they are automated in helper functions.\n\nFor centering predictors, see the base R function `?scale`\n\nFor creating a short vector over the range of a predictor, see `modelr::seq_range`. The R package [`modelr`](https://modelr.tidyverse.org/) has many different functions to help with modelling.\n:::\n\nTo simulate, we'll use some matrix algebra, as we saw in lecture:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nknitr::kable(head(X))\n```\n\n::: {.cell-output-display}\n|   | some_bill_lengths|\n|--:|-----------------:|\n|  1|       -11.8219298|\n|  1|        -8.7663743|\n|  1|        -5.7108187|\n|  1|        -2.6552632|\n|  1|         0.4002924|\n|  1|         3.4558480|\n:::\n\n```{.r .cell-code}\nknitr::kable(head(B))\n```\n\n::: {.cell-output-display}\n|       |           |           |           |          |           |           |           |\n|:------|----------:|----------:|----------:|---------:|----------:|----------:|----------:|\n|inters | 19.3267323| 18.7340270| 16.6558349| 17.814553| 14.0237585| 17.8310158| 14.6174138|\n|slopes |  0.2564205| -0.4651482|  0.2435612| -0.278236| -0.1508848|  0.6891655| -0.5906592|\n:::\n\n```{.r .cell-code}\nprior_mus <- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note}\n## Exercise\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n:::\n\n### Simulating Observations\n\nThere are always at least TWO kinds of predictions we can be thinking about: \n\n1. Predicted averages. This is often called a \"confidence\" interval for a regression line.\n2. Predicted observations. This is often called a \"prediction\" interval.\n\nWe can use the full model to simulate observations! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes <- rnorm(7, 0, .5)\ninters <- rnorm(7, 17, 2)\nsigmas <- rexp(7, rate = 0.3)\n\nX <- cbind(1, some_bill_lengths)\nB <- rbind(inters, slopes)\n\nprior_mus <- X %*% B\n\nprior_obs <- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] <- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nTidyverse style for those who indulge:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |> \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |> \n  rowwise() |> \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |> \n  unnest(cols = c(\"x\", \"avg\", \"obs\")) |> \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\n* Experiment with priors that are \"too narrow\" or \"too wide\". \n* Try a different distribution than the one used\n* Instead of bill size, imagine that we are applying this model to YOUR data. What would you change?\n:::\n\n## Linear regression in Stan\n\nNow we write a Stan program for this model. \nWe'll begin with a simple model that has no posterior predictions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_regression_no_prediction <- cmdstan_model(\n  stan_file = \"topics/02_regression/normal_regression_no_prediction.stan\")\n\nnormal_regression_no_prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int<lower=0> N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n}\nparameters {\n  real intercept;\n  real slope;\n  real<lower=0> sigma;\n}\nmodel {\n  bill_dep ~ normal(intercept + slope * bill_len, sigma);\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n  sigma ~ exponential(.7);\n}\n```\n:::\n:::\n\n\nIn order to get the posterior, we need to put our data in Stan. We follow the same steps as previously:\n\n* Remember to remove NAs first!  \n* arrange the data in a list\n* pass the data to a Stan model to estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## drop NAs\npenguins_no_NA <- penguins |> \n  tidyr::drop_na(bill_depth_mm, bill_length_mm) |> \n  dplyr::mutate(\n    bill_length_center = bill_length_mm - mean(bill_length_mm))\n\n## assemble data list\ndata_list <- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm\n          ))\n\n## run the sampler, using the compiled model.\nnormal_reg_no_pred <- normal_regression_no_prediction$sample(\n  data = data_list, \n  refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n\n```{.r .cell-code}\nnormal_reg_no_pred$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 10\n  variable       mean    median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>         <dbl>     <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__      -396.     -395.     1.20   0.998  -398.    -394.      1.00    2081.\n2 intercept   17.2      17.2    0.104  0.103    17.0     17.3     1.00    3849.\n3 slope       -0.0849   -0.0848 0.0192 0.0192   -0.117   -0.0524  1.00    4194.\n4 sigma        1.93      1.93   0.0729 0.0720    1.81     2.06    1.00    4059.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_no_pred$draws() |> \n  bayesplot::mcmc_areas(pars = c(\"slope\", \"intercept\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\n**Discussion** : Look just at the posterior distribution of the slope right above. \nDo we have evidence that there's a relationship between bill length and bill depth.\n:::\n\n## Posterior predictions in R\n\nWe can calculate a posterior prediction line directly in R for these data.\nI'll show each step in this workflow separately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_no_pred |> \n  spread_rvars(slope, intercept, sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n            slope  intercept        sigma\n       <rvar[1d]> <rvar[1d]>   <rvar[1d]>\n1  -0.085 ± 0.019   17 ± 0.1  1.9 ± 0.073\n```\n:::\n:::\n\n\n`tidybayes` helps us extract the posterior distribution of the parameters into a convenient object called an `rvar`. \nLearn more about tidybayes [here](http://mjskay.github.io/tidybayes/articles/tidybayes.html) and about the rvar datatype [here](https://mc-stan.org/posterior/articles/rvar.html)\n\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline <- normal_reg_no_pred |> \n  tidybayes::spread_rvars(slope, intercept) |> \n  expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |> \n  mutate(mu = intercept + slope*x)\n\nknitr::kable(normal_reg_predline)\n```\n\n::: {.cell-output-display}\n|slope          |intercept |     x|mu        |\n|:--------------|:---------|-----:|:---------|\n|-0.085 ± 0.019 |17 ± 0.1  | -15.0|18 ± 0.31 |\n|-0.085 ± 0.019 |17 ± 0.1  |  -7.5|18 ± 0.18 |\n|-0.085 ± 0.019 |17 ± 0.1  |   0.0|17 ± 0.10 |\n|-0.085 ± 0.019 |17 ± 0.1  |   7.5|17 ± 0.18 |\n|-0.085 ± 0.019 |17 ± 0.1  |  15.0|16 ± 0.31 |\n:::\n:::\n\n\nFinally we'll plot these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_predline |> \n  ggplot(aes(x = x, dist = mu)) + \n  stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             inherit.aes = FALSE,\n             data = penguins_no_NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n## Posterior predictions in Stan\n\nWe can also make these posterior predictions in Stan.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_regression <- cmdstan_model(stan_file = \"topics/02_regression/normal_regression.stan\")\n\nnormal_regression\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int<lower=0> N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n  // posterior predictions\n  int<lower=0> npost;\n  vector[npost] pred_values;\n}\nparameters {\n  real intercept;\n  real slope;\n  real<lower=0> sigma;\n}\nmodel {\n  bill_dep ~ normal(intercept + slope * bill_len, sigma);\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n}\ngenerated quantities {\n  vector[npost] post_bill_dep_obs;\n  vector[npost] post_bill_dep_average;\n  \n  // calculate expectation\n  post_bill_dep_average = intercept + slope * pred_values;\n  \n  // make fake observations\n  for (i in 1:npost) {\n    post_bill_dep_obs[i] = normal_rng(intercept + slope * pred_values[i], sigma);\n  }  \n  \n}\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_no_NA <- penguins |> \n  tidyr::drop_na(bill_depth_mm, bill_length_mm) |> \n  dplyr::mutate(\n    bill_length_center = bill_length_mm - mean(bill_length_mm))\n\ndata_list <- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm,\n          npost = 6,\n          pred_values = modelr::seq_range(penguins_no_NA$bill_length_center, n = 6)\n          ))\n\nbill_norm_reg <- normal_regression$sample(data = data_list, \n                                          refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n```\n:::\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nbill_posterior <- bill_norm_reg |> \n  tidybayes::spread_rvars(post_bill_dep_average[i],\n                          post_bill_dep_obs[i]) |>\n  mutate(bill_length = data_list$pred_values[i]) \n\nbill_posterior |> \n  ggplot(aes(x = bill_length, dist = post_bill_dep_average)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1, guide = \"none\") + \n  labs(title = \"Average response\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbill_posterior |> \n  ggplot(aes(x = bill_length, dist = post_bill_dep_obs)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1, guide = \"none\") +\n  labs(title = \"Predicted observations\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n### EXERCISE\nExtend this model to include species. Specifically, let each species have its own value of the `intercept`. This involves combining this regression example with the previous activity on discrete predictors.\n\nWhen you're done, look at the resulting summary of coefficients. What do you notice that's different?\n::: \n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_regression_spp <- cmdstan_model(stan_file = \"topics/02_regression/normal_regression_spp.stan\")\n\nnormal_regression_spp\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int<lower=0> N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n  // species IDs\n  array[N] int spp_id;\n  // posterior predictions\n  int<lower=0> npost;\n  vector[npost] pred_values;\n  array[npost] int pred_spp_id;\n}\nparameters {\n  vector[3] intercept;\n  real slope;\n  real<lower=0> sigma;\n}\nmodel {\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n  sigma ~ exponential(.7);\n  bill_dep ~ normal(intercept[spp_id] + slope * bill_len, sigma);\n}\ngenerated quantities {\n  vector[npost] post_bill_dep_obs;\n  vector[npost] post_bill_dep_average;\n  \n  // calculate expectation\n  post_bill_dep_average = intercept[pred_spp_id] + slope * pred_values;\n  \n  // make fake observations\n  for (i in 1:npost) {\n    post_bill_dep_obs[i] = normal_rng(intercept[pred_spp_id[i]] + slope * pred_values[i], sigma);\n  }  \n  \n}\n```\n:::\n:::\n\n\nWe set up a list for this model just as we did before. \nNote that this time we are using TRIPLE the `pred_values`, because we want to run independent predictions for each species.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbill_vec <- modelr::seq_range(penguins_no_NA$bill_length_center, n = 6)\n\ndata_list_spp <- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm,\n          spp_id = as.numeric(as.factor(species)),\n          npost = 3*6,\n          pred_values = rep(bill_vec, 3),\n          pred_spp_id = rep(1:3, each = 6)\n          ))\n\nnormal_reg_spp_post <- normal_regression_spp$sample(data = data_list_spp, refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n```\n:::\n:::\n\n\nNote that the sign of the slope is different now! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_reg_spp_post$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 42 × 10\n   variable         mean   median     sd    mad       q5      q95  rhat ess_bulk\n   <chr>           <dbl>    <dbl>  <dbl>  <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n 1 lp__         -157.    -157.    1.56   1.35   -161.    -156.    0.999    1892.\n 2 intercept[1]   19.4     19.4   0.117  0.116    19.2     19.6   1.00     1937.\n 3 intercept[2]   17.4     17.4   0.141  0.141    17.2     17.7   1.00     2143.\n 4 intercept[3]   14.3     14.3   0.105  0.105    14.1     14.4   1.00     1949.\n 5 slope           0.198    0.198 0.0171 0.0173    0.170    0.226 1.00     1686.\n 6 sigma           0.956    0.955 0.0360 0.0350    0.899    1.02  1.00     3169.\n 7 post_bill_d…   17.0     17.0   0.966  0.963    15.4     18.6   0.999    3886.\n 8 post_bill_d…   18.1     18.1   0.945  0.953    16.5     19.6   1.00     3833.\n 9 post_bill_d…   19.2     19.2   0.966  0.954    17.6     20.8   1.00     3755.\n10 post_bill_d…   20.3     20.3   0.960  0.941    18.7     21.9   1.00     3841.\n# ℹ 32 more rows\n# ℹ 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\n:::\n\n### Plotting posterior predictions\n\nUsing `stat_lineribbon()`, let's plot the average and predicted intervals for this regression.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nbill_posterior <- normal_reg_spp_post |> \n  tidybayes::spread_rvars(post_bill_dep_average[i],\n                          post_bill_dep_obs[i]) |>\n  mutate(bill_length = data_list_spp$pred_values[i],\n         spp = data_list_spp$pred_spp_id) |> \n  mutate(spp = as.factor(levels(penguins$species)[spp]))\n\nbill_posterior |> \n  ggplot(aes(x = bill_length,\n             ydist = post_bill_dep_average,\n             fill = spp, \n             colour = spp)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center,\n                 y = bill_depth_mm,\n                 fill = species, colour = species),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) +   \n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Dark2\") + \n  labs(title = \"Average response\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbill_posterior |> \n  ggplot(aes(x = bill_length,\n             dist = post_bill_dep_obs,\n             fill = spp,\n             colour = spp)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center,\n                 y = bill_depth_mm,\n                 colour = species),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Dark2\") + \n  labs(title = \"Predicted observations\") + \n  facet_wrap(~spp, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n:::\n\n\n## Exercise! \n\nshow how the $\\sigma$ is different between these two models\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}