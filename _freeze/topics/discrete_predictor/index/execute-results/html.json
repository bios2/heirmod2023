{
  "hash": "0b40c11cee01cb632858a5f57fbf39e1",
  "result": {
    "markdown": "---\ntitle:  Palmer penguins and discrete predictors\ndescription: |\n  fitting a model with discrete predictors.\nexecute:\n  freeze: true\nformat:\n  html:\n    code-tools: true\n---\n\n\nLet's start by taking a look at the Palmer Penguin dataset. Let's look at the distribution of observations of bill size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |> \n  ggplot(aes(x=bill_depth_mm)) + \n  geom_histogram(binwidth = .5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![Histogram of bill depth for all the penguins in the Palmer Penguin dataset.](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThere's quite a lot of variation in these measurements, with a suggestion of perhaps more than one peak in this distribution.\n\n## A simple model\n\n$$\n\\begin{align}\n\\text{Bill depth} &\\sim \\text{Normal}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\text{Normal}(17.5, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n$$\n\nlet's express the same model in Stan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is cmdstanr version 0.7.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan path: /home/andrew/software/cmdstan\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan version: 2.34.1\n```\n:::\n\n```{.r .cell-code}\nnormal_dist <- cmdstan_model(\"topics/discrete_predictor/normal_dist.stan\")\nnormal_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu, sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\n```\n:::\n:::\n\n\nThe model section looks very much like the approach shown above. \nI want you to notice especially how the bottom chunk has three lines, each describing a probability distribution. \nThese are for all the probability distribution of all the quantities in the model, both observed and unobserved. Above, we state which is which. \nModels are devices for putting together the probability of all the quantities we are looking for. Again, a Bayesian separates the world into unmeasured or measured quantities -- and above we state which are observed (the data block) and which are unobserved (the parameters block).\n\nWe can fit this model to data and see the result: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first we drop all NA values\npenguins_nobillNA <- penguins |> \n  #drop NA values\n  filter(!is.na(bill_depth_mm))\n\n## then we assemble the data as a list.\n## I'm using the base function with()\n##  it lets me use the variable name directly \n## without writing penguins_nobillNA$bill_depth_mm\n\nlist_bill_dep <- with(penguins_nobillNA,\n     list(N = length(bill_depth_mm),\n          measurements = bill_depth_mm))\n     \n## sample 4 chains, suppress counting iterations\nsamp_bill_dep <- normal_dist$sample(data = list_bill_dep, \n                                    parallel_chains = 4,\n                                    refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n:::\n\n```{.r .cell-code}\n## summarize the samples for each parameter into a nice table\nsamp_bill_dep |> \n  posterior::summarise_draws() |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|variable |        mean|      median|        sd|       mad|          q5|        q95|     rhat| ess_bulk| ess_tail|\n|:--------|-----------:|-----------:|---------:|---------:|-----------:|----------:|--------:|--------:|--------:|\n|lp__     | -405.543025| -405.229500| 1.0070374| 0.7257327| -407.509050| -404.56895| 1.000171| 1654.792| 2254.549|\n|mu       |   17.146799|   17.147600| 0.1073774| 0.1069696|   16.966655|   17.32120| 1.002203| 3227.272| 2451.274|\n|sigma    |    1.974634|    1.970985| 0.0772860| 0.0784295|    1.854086|    2.10642| 1.000073| 2818.370| 2233.633|\n:::\n:::\n\n\n\n## Plotting parameters. \n\nWe don't have one distribution for each of our unknown numbers: we have thousands. \nWe need to get a sense of what these possible values mean scientifically. \nAn excellent way to do this is by making as many pictures as possible. \nWe will start with making plots of specific parameters. \n\nWe can look at the distributions easily using the `bayesplot` package.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ndraws <- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\n\nbayesplot::mcmc_hist(draws, pars = \"mu\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(draws, pars = \"sigma\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nNotice that the distributions do not have the same shape as the prior-- this is particularly true for $\\sigma$. \nThis shows an important point: the prior distribution does not determine what the posterior looks like. \n[should I sample from the prior and show them that?]{.aside}\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndraws |>  \n  posterior::as_draws_df() |> \n  ggplot(aes(x = sigma)) + \n  stat_dotsinterval()\n```\n\n::: {.cell-output-display}\n![the package `ggdist` has many fun & useful ways to draw pictures of posterior distributions. Here is one called `stats_dotsinterval()`](index_files/figure-html/fig-dotsint-1.png){#fig-dotsint width=672}\n:::\n:::\n\n\n## Posterior predictions: the easy way to check your model\n\nPeople care so much about model diagnostics. \nAnd with good reason: you need to know how much to trust a model before using it to make a scientific claim. \nOne way to find out which model is best would be to use them to make a prediction, and see how right you are. \nAn alternative is to see how well the data fit your sample. \n\n### Posterior prediction in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# just get some draws\ndraws <- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\ndraws_matrix <- posterior::as_draws_matrix(draws)\n\n## set up a matrix. for every posterior sample, \n## (that is, for a value of mu and a value of sigma) \n## draw a whole fake dataset from a normal distribution with that mean and sd. \nnsamples <- 50\nyrep <- matrix(0, ncol = list_bill_dep$N, nrow = nsamples)\n\n# pick some random rows\nset.seed(1234)\nchosen_samples <- sample(1:nrow(draws_matrix), replace = FALSE, size = nsamples)\nsubset_draws <- draws_matrix[chosen_samples,]\n\nfor (r in 1:nsamples){\n yrep[r,] <- rnorm(n = list_bill_dep$N, \n                   mean = subset_draws[r, \"mu\"], \n                   sd = subset_draws[r, \"sigma\"])\n}\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = yrep)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n### Posterior predictions in Stan\n\nWe can simulate our own data in R if we are comfortable translating between R and Stan. However, if you want, you can do the same process in Stan. Just combine the section we just looked at with the previous work on data simulation we started with: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_dist_rng <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng.stan\")\n\nnormal_dist_rng\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu, sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu, sigma);\n  }\n}\n```\n:::\n:::\n\n\nHere we have a handy random number generator _inside_ Stan.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamp_bill_dep_rng <- normal_dist_rng$sample(\n  data = list_bill_dep,\n  refresh = 0,\n  parallel_chains = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpYRTfHe/model-e4812a51a851.stan', line 10, column 2 to column 35)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n```\n:::\n\n```{.r .cell-code}\ndraws <- samp_bill_dep_rng$draws(variables = c(\"yrep\"))\ndraws_matrix <- posterior::as_draws_matrix(draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(draws_matrix, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe code is much shorter, because there is less to do in R. \nBoth of these gives the same outcome: the posterior predictive distribution. \nThis gives us a straightfoward way to test our model's performance: \n\n1. we use the model to generate fake observations. \n2. plot these on top of the real data\n3. if the data is a really poor match, we know our model has a distorted view of the world.\n\n## Different groups are different\n\nlet's add in differences among species\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x = bill_depth_mm, fill = species))+ \n  geom_histogram(binwidth = .5) + \n  scale_fill_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nNow we can see that the distribution is in fact three different shapes, all placed together. \n\n:::{.callout-warning}\nSometimes scientists will plot histograms of data at the beginning of a research project, and use the histogram to decide if their data are \"normally distributed\" or not. This is not helpful! Instead, decide on a model first, and ask yourself what kind of data you expect.\n:::\n\n## Stan code for species differences\n\n\n$$\n\\begin{align}\n\\text{Bill depth}_{\\text{sp}[i]} &\\sim \\text{Normal}(\\mu_{\\text{sp}[i]}, \\sigma) \\\\\n\\mu &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(2) \\\\\n\\end{align}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_dist_rng_spp_forloop <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp_forloop.stan\")\n\nnormal_dist_rng_spp_forloop\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int<lower=1,upper=3> spp_id;\n}\nparameters {\n  vector[3] mu;\n  real<lower=0> sigma;\n}\nmodel {\n  for (i in 1:N){\n    measurements[i] ~ normal(mu[spp_id[i]], sigma);\n  }\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n```\n:::\n:::\n\n\nThere's a few differences to notice here:\n\n* in the `data` block: We have a new input! A declaration of the array of integers at the top, saying if this is \"species 1\", \"species 2\", or \"species 3\"\n* `mu` is a vector now. why?\n* notice the for-loop. \n\n## Quick detour : vector indexing\n\nA **very** useful technique, in both R and Stan, is transforming a vector with _indexing_. \nVector indexing requires two vectors: the first contains values we want to select or replicate, the second contains integers giving the positions of the elements we want. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsome_values <- c(\"taco\", \"cat\", \"goat\", \"cheeze\", \"pizza\")\npositions <- c(1,1,2,2,3,1,1,5)\n\nsome_values[positions]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"taco\"  \"taco\"  \"cat\"   \"cat\"   \"goat\"  \"taco\"  \"taco\"  \"pizza\"\n```\n:::\n:::\n\n\nThis works for number values as well, and is very useful when you want to do simulations! let's simulate three groups with different averages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(525600)\nsome_means <- c(12, 17, 19)\nsome_labels <- c(\"taco\", \"cat\", \"goat\")\n\ndf_of_means <- data.frame(index = rep(1:3, each = 42)) |> \n  mutate(the_mean = some_means[index],\n         labels = some_labels[index],\n         obs = rnorm(n = length(the_mean),\n                     mean = the_mean,\n                     sd = 1))\n\ndf_of_means |> \n  ggplot(aes(x = obs, fill = labels)) + \n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Vector indexing in Stan\n\nWe can use this precise technique in Stan: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_dist_rng_spp <- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp.stan\")\n\nnormal_dist_rng_spp\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int<lower=1,upper=3> spp_id;\n}\nparameters {\n  vector[3] mu;\n  real<lower=0> sigma;\n}\nmodel {\n  measurements ~ normal(mu[spp_id], sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n```\n:::\n:::\n\n\nThe only difference to the previous model is in the line with the for-loop, which is now replaced with a vectorized expression. This is faster to write and will run faster in Stan. However it's not possible in every case.\n\n### Sampling the species model\n\nWe have to make a new data list, since we've added a new input: a vector of numbers 1, 2, or 3 that tells us if we are working with the first, second, or third species.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_bill_dep_spp <- with(penguins_nobillNA,\n     list(\n       N = length(bill_depth_mm),\n       measurements = bill_depth_mm,\n       spp_id = as.numeric(as.factor(species))\n     )\n)\n     \nsamp_normal_dist_rng_spp <- normal_dist_rng_spp$sample(\n  data = list_bill_dep_spp, \n  parallel_chains = 4,\n  refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n```\n:::\n\n```{.r .cell-code}\nsamp_normal_dist_rng_spp$draws(variables = c(\"mu\", \"sigma\")) |> \n  posterior::summarise_draws() |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|variable |      mean|    median|        sd|       mad|        q5|      q95|      rhat| ess_bulk| ess_tail|\n|:--------|---------:|---------:|---------:|---------:|---------:|--------:|---------:|--------:|--------:|\n|mu[1]    | 18.342111| 18.343000| 0.0903448| 0.0902903| 18.193885| 18.49411| 1.0009652| 4500.886| 2992.683|\n|mu[2]    | 18.414473| 18.414200| 0.1381799| 0.1374370| 18.186955| 18.64132| 1.0007758| 4798.435| 3080.274|\n|mu[3]    | 14.985503| 14.985300| 0.1013407| 0.1014840| 14.818200| 15.15663| 1.0010026| 4151.403| 3283.383|\n|sigma    |  1.124076|  1.121725| 0.0434414| 0.0427434|  1.055824|  1.19891| 0.9997731| 4879.822| 3210.395|\n:::\n:::\n\n\nLet's take a look at this in Shinystan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshinystan::launch_shinystan(samp_normal_dist_rng_spp)\n```\n:::\n\n\n\nand we can repeat the posterior checking from before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspp_yrep_draws <- samp_normal_dist_rng_spp$draws(variables = c(\"yrep\"))\nspp_draws_matrix <- posterior::as_draws_matrix(spp_yrep_draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(spp_draws_matrix, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nThe predicted distribution is now much more like the real data\n\nWe can also make figures for each individual species. \nHere we will move away from using `bayesplot` and try to visualize our posterior using the handy functions in the `tidybayes` package [add a link]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidybayes)\nspp_draws_df <- posterior::as_draws_df(spp_yrep_draws)\n\nnormal_dist_post_samp <- tidybayes::gather_draws(samp_normal_dist_rng_spp,\n                        yrep[row_id], \n                        ndraws = 50)\n\nnormal_dist_post_samp |> \n  mutate(species = penguins_nobillNA$species[row_id]) |> \n  ggplot(aes(x = .value, colour = species)) + \n  geom_density(aes(group = .iteration), alpha = .1) + \n  facet_wrap(~species) + \n  geom_density(aes(x = bill_depth_mm),\n               data = penguins_nobillNA,\n               colour = \"black\") + \n  scale_colour_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n### Exercises\n\n\n#### Level 1\n* repeat this experience for another variable in the dataset. Does the same code work on bill length? What about body size? What would you change about the model (if anything)\n* use bayesplot to examine the fit of body size to these data. \n\n#### Level 2\n* generate some random groups of your own, with known means. How well does the model fit these data\n* The present model is fixed for exactly 3 groups. how would you change it for any number of groups?\n\n#### Level 3\n* the function `tidybayes::compose_data` is a convenient way to set up your data for passing it into R. Try out this function. What does it produce for our dataset? How do you need to modify our Stan program so that it works for the output of `tidybayes::compose_data`?\n* As you can see, the model assumes the same sigma for all species. what if you relax this? \n\n### Optional! \nTry this on your own data! ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}