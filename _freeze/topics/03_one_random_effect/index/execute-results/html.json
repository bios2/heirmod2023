{
  "hash": "4054d8ab8315108b9eb687a5281b9b3b",
  "result": {
    "markdown": "---\ntitle: \"Models with one level of hierarchy\"\ndescription: |\n  Some of these things are somewhat like the others.\nexecute:\n  freeze: true\ncomments:\n  hypothesis: true\nformat:\n  html:\n    code-tools: true\n---\n\n\n\n:::{.callout-tip}\n## Bayesian workflow\n\n1. Visualize your data\n2. Decide on your model structure\n3. Simulate from the model to understand it\n4. Fit the model to the data\n5. Plot model predictions to evaluate the fit / draw conclusions\n:::\n\nToday's goal is to look at a couple of different model structures that we saw yesterday. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is cmdstanr version 0.5.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan path: /Users/amacdonald/.cmdstan/cmdstan-2.31.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- CmdStan version: 2.31.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\nlibrary(palmerpenguins)\n```\n:::\n\n\n\n## Gaussian random intercepts: Penguin body mass\n\nThe Palmer penguins are found on three different islands. Let's look at the distribution of body mass of each species on each island.\n\n### Plot the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island <- penguins |> \n  select(species, island, body_mass_g) |> \n  drop_na(body_mass_g) |> \n  unite(sp_island, species, island) |> \n  ## center mass and change the units\n  mutate(mass_kg = (body_mass_g)/1000)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island |> \n  ggplot(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gauss-inter-plot-1.png){width=672}\n:::\n:::\n\n\nAre the sample sizes equal among the species-island combinations?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island |> \n  count(sp_island) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|sp_island        |   n|\n|:----------------|---:|\n|Adelie_Biscoe    |  44|\n|Adelie_Dream     |  56|\n|Adelie_Torgersen |  51|\n|Chinstrap_Dream  |  68|\n|Gentoo_Biscoe    | 123|\n:::\n:::\n\n\n### Decide on a model structure\n\nWe'll begin by fitting a model that assumes that body size for each of these five groups is completely independent:\n\n$$\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{species}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{species}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(.5)\n\\end{align}\n$$\n\n### Simulate to understand this model\n\nHere's a little trick to get group indexes (numbers) from a character vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroup_names <- unique(penguin_mass_island$sp_island)\ngroup_numbers <- seq_along(group_names)\nnames(group_numbers) <- group_names\n\ngroup_numbers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdelie_Torgersen    Adelie_Biscoe     Adelie_Dream    Gentoo_Biscoe \n               1                2                3                4 \n Chinstrap_Dream \n               5 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_groupid <- penguin_mass_island |> \n  mutate(group_id = group_numbers[sp_island])\n\npenguin_groupid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 4\n   sp_island        body_mass_g mass_kg group_id\n   <chr>                  <int>   <dbl>    <int>\n 1 Adelie_Torgersen        3750    3.75        1\n 2 Adelie_Torgersen        3800    3.8         1\n 3 Adelie_Torgersen        3250    3.25        1\n 4 Adelie_Torgersen        3450    3.45        1\n 5 Adelie_Torgersen        3650    3.65        1\n 6 Adelie_Torgersen        3625    3.62        1\n 7 Adelie_Torgersen        4675    4.68        1\n 8 Adelie_Torgersen        3475    3.48        1\n 9 Adelie_Torgersen        4250    4.25        1\n10 Adelie_Torgersen        3300    3.3         1\n# ℹ 332 more rows\n```\n:::\n:::\n\n\nAs you can see, we're set up now with the names and the indexes we need. \n\nNow we can simulate data and plot it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngroup <- length(group_numbers)\noverall_mean <- rnorm(1, mean = 5, sd = 2)\ngroup_diffs <- rnorm(n = ngroup, mean = 0, sd = 1)\nsigma_obs <- rexp(1, .5)\n\npenguin_pred_obs <- penguin_groupid |> \n  mutate(fake_mass_avg = overall_mean + group_diffs[group_id],\n         fake_mass_obs = rnorm(length(fake_mass_avg), \n                               mean = fake_mass_avg, \n                               sd = sigma_obs))\n\npenguin_pred_obs |> \n  ggplot(aes(y = sp_island,\n             x = fake_mass_obs,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\nrun the above code a few times! if you want, try different prior values.\n:::\n\n### Write it in Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups <- cmdstan_model(stan_file = \"topics/03_one_random_effect/fixed_groups.stan\")\n\nfixed_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  int<lower=0> Ngroup;\n  array[N] int<lower=0, upper=Ngroup> group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma);\n  b_group ~ std_normal();\n  b_avg ~ normal(5, 2);\n  sigma ~ exponential(.5);\n}\ngenerated quantities {\n  vector[N] fake_obs;\n  \n  for (i in 1:N) {\n    fake_obs[i] = normal_rng(b_avg + b_group[group_id[i]], sigma);\n  }\n  \n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n  \n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(b_avg + b_group[k], sigma);\n  }\n}\n```\n:::\n:::\n\n\n### Fit the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_group_list <- with(penguin_groupid, \n         list(\n           N = length(mass_kg),\n           y = mass_kg,\n           Ngroup = max(group_id),\n           group_id = group_id\n         ))\n\nfixed_groups_samples <- fixed_groups$sample(\n  data = peng_group_list,\n  refresh = 0,\n  parallel_chains = 4\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c466a4b69e9.stan', line 13, column 2 to column 47)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c466a4b69e9.stan', line 13, column 2 to column 47)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 1 finished in 1.6 seconds.\nChain 2 finished in 1.5 seconds.\nChain 3 finished in 1.5 seconds.\nChain 4 finished in 1.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.6 seconds.\nTotal execution time: 1.8 seconds.\n```\n:::\n:::\n\n\n### Plot predictions to evaluate results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## bayesplot\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups_samples |> \n  tidybayes::gather_rvars(one_obs_per_group[group_id]) |> \n  mutate(sp_island = group_names[group_id]) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval() + \n  geom_point(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island), \n             inherit.aes = FALSE,\n             alpha = .2, data = penguin_groupid)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Make it hierarchical\n\nTK make them side by side\n\n$$\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{species}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{species}} &\\sim \\text{Normal}(0, \\sigma_{\\text{sp}}) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5) \\\\\n\\sigma_{\\text{sp}} &\\sim \\text{Exponential}(1)\n\\end{align}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups <- cmdstan_model(stan_file = \"topics/03_one_random_effect/hierarchical_groups.stan\")\n\nhierarchical_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  int<lower=0> Ngroup;\n  array[N] int<lower=0, upper=Ngroup> group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real<lower=0> sigma_obs;\n  real<lower=0> sigma_grp;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma_obs);\n  b_group ~ normal(0, sigma_grp);\n  b_avg ~ normal(5, 2);\n  sigma_obs ~ exponential(.5);\n  sigma_grp ~ exponential(1);\n}\ngenerated quantities {\n  vector[N] fake_obs;\n  \n  for (i in 1:N) {\n    fake_obs[i] = normal_rng(b_avg + b_group[group_id[i]], sigma_obs);\n  }\n  \n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n  \n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(b_avg + b_group[k], sigma_obs);\n  }\n  \n  // difference for a new group\n  real new_b_group = normal_rng(0, sigma_grp);\n  \n  // observations from that new group\n  real one_obs_new_group = normal_rng(b_avg + new_b_group, sigma_obs);\n  \n}\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples <- hierarchical_groups$sample(\n  data = peng_group_list, refresh = 0, parallel_chains = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c46a09925.stan', line 14, column 2 to column 51)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 1 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 3 finished in 1.6 seconds.\nChain 1 finished in 1.7 seconds.\nChain 2 finished in 1.7 seconds.\nChain 4 finished in 1.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 1.9 seconds.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__        88.53  88.89 2.10 1.99 84.59 91.34 1.01      955     1718\n b_avg        4.03   4.03 0.38 0.30  3.45  4.65 1.01      595      404\n b_group[1]  -0.32  -0.31 0.38 0.30 -0.94  0.26 1.01      595      441\n b_group[2]  -0.32  -0.31 0.38 0.31 -0.95  0.27 1.01      608      418\n b_group[3]  -0.34  -0.33 0.38 0.31 -0.96  0.25 1.01      607      436\n b_group[4]   1.04   1.05 0.38 0.30  0.42  1.63 1.01      599      413\n b_group[5]  -0.30  -0.29 0.38 0.30 -0.91  0.29 1.01      597      418\n sigma_obs    0.47   0.46 0.02 0.02  0.44  0.50 1.00     1682     1907\n sigma_grp    0.77   0.69 0.33 0.25  0.41  1.42 1.00     1233     1091\n fake_obs[1]  3.72   3.73 0.47 0.47  2.95  4.50 1.00     4037     3891\n\n # showing 10 of 358 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples |> \n  tidybayes::gather_rvars(b_group[group_id],\n                          new_b_group) |> \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples |> \n  tidybayes::gather_rvars(one_obs_per_group[group_id],\n                          one_obs_new_group) |> \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval() + \n  geom_point(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island), \n             inherit.aes = FALSE,\n             alpha = .2, data = penguin_groupid)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## Exercises\n\n1. Try leaving out a group and refitting the hierarchical model. Are the predictions for the missing group accurate?\n1. There are other categorical predictors in the dataset. Try including `year` as a part of the group-creating factor (i.e. in the call to `unite()` above). What changes?\n1. The posterior for both models includes a predicted `fake_obs` for EVERY observation. This opens the possibility of using `bayesplot` to make predictions. Look back at the code from Day 1 and create a posterior predictive check for both models. (e.g. using `ppc_dens_overlay`)\n1. We could perhaps have used `sex` as a grouping factor, but `sex` has missing values in it! Why is this a problem for this kind of model? What would it take to address that? (Discussion only; missing values are unfortunately outside the class scope!)\n\n## Poisson random intercepts: Mite abundance\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\n\n# combine data and environment\n\nmite_data_long <- mite |> \n  tibble::rownames_to_column(var = \"site_id\") |> \n  bind_cols(mite.env) |> \n  pivot_longer(Brachy:Trimalc2,\n               names_to = \"spp\", values_to = \"abd\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmite_data_long |> \n  mutate(site_id = forcats::fct_reorder(site_id, abd)) |> \n  ggplot(aes(y = site_id, x = abd)) +\n  geom_point() + \n  coord_cartesian(xlim = c(0,100)) + \n  stat_summary(fun = median, col = \"red\", geom = \"point\")\n```\n\n::: {.cell-output-display}\n![Abundance of each species at every site in the mite dataset. Points are species abundances, grouped on the row for that site.](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n### Exercise: write the model in the same notation as the original\n\nLet's model the counts of species abundances, using a random effect for each site. Write the model that corresponds to this!\n\n### Trying it with a Normal distribution:\n\nIt's actually possible to run the previous model on this one.\nlet's set up the data and try:\n\n::: {.cell}\n\n```{.r .cell-code}\nspecies_numbers <- with(mite_data_long,\n     setNames(seq_along(unique(spp)), unique(spp)))\n\nmite_data_groupID <- mutate(mite_data_long,\n                            group_id = species_numbers[spp])\n\nmite_list <- with(mite_data_groupID,\n                  list(\n                    N = length(abd),\n                    y = abd,\n                    Ngroup = dplyr::n_distinct(spp),\n                    group_id = group_id\n                  ))\n\nnormal_samples <- hierarchical_groups$sample(data = mite_list, refresh = 0, parallel_chains = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c46a09925.stan', line 15, column 2 to column 33)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c46a09925.stan', line 15, column 2 to column 33)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 3 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/RtmpYROeSn/model-15c46a09925.stan', line 15, column 2 to column 33)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 4 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 2 finished in 9.1 seconds.\nChain 1 finished in 9.3 seconds.\nChain 4 finished in 9.4 seconds.\nChain 3 finished in 9.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 9.3 seconds.\nTotal execution time: 9.6 seconds.\n```\n:::\n:::\n\n\nThis is interesting, but it would probably be better to fit this model with something meant for counts! With this comes the need to include a log link function. Fortunately, Stan makes all this possible with just a few small changes:\n\n### Exercise: translate it into Stan\n\nModify the program `hierarchical_groups.stan` to work for poisson data. Some things to keep in mind: \n\n* `data {}` block: remember that the Poisson distribution needs integers and set up the data inputs accordinly.\n* `parameters {}` block: think about which parameters the poisson does NOT need. \n* `model {}` block: remember to remove any unneeded parameters from the likelihood (the model of the data), and their priors too.\n* replace `normal` with `poisson_log`. Note that this evaluates its argument on the log scale. That is, it works like a typical GLM done in R. We can keep priors the same as in the last model, though we may decide to change their values.\n* `generated quantities {}` block: replace `normal_rng()` with `poisson_log_rng()` -- _where necessary_ -- and delete unused parameters.\n* again, remember that the Poisson needs to be making integers. For example, replace `vector[Ngroup]` with `array[Ngroup] int `\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphg <- cmdstan_model(stan_file = \"topics/03_one_random_effect/poisson_hier_groups.stan\")\n\nphg_samples <- phg$sample(data = mite_list, refresh = 20, parallel_chains = 2, chains = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 2 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:   20 / 2000 [  1%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:   20 / 2000 [  1%]  (Warmup) \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpw7jYom/model-daed605a6f38.stan', line 14, column 2 to column 33)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nChain 2 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChain 1 Iteration:   40 / 2000 [  2%]  (Warmup) \nChain 2 Iteration:   40 / 2000 [  2%]  (Warmup) \nChain 1 Iteration:   60 / 2000 [  3%]  (Warmup) \nChain 2 Iteration:   60 / 2000 [  3%]  (Warmup) \nChain 1 Iteration:   80 / 2000 [  4%]  (Warmup) \nChain 2 Iteration:   80 / 2000 [  4%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  120 / 2000 [  6%]  (Warmup) \nChain 2 Iteration:  120 / 2000 [  6%]  (Warmup) \nChain 1 Iteration:  140 / 2000 [  7%]  (Warmup) \nChain 2 Iteration:  140 / 2000 [  7%]  (Warmup) \nChain 1 Iteration:  160 / 2000 [  8%]  (Warmup) \nChain 2 Iteration:  160 / 2000 [  8%]  (Warmup) \nChain 2 Iteration:  180 / 2000 [  9%]  (Warmup) \nChain 1 Iteration:  180 / 2000 [  9%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  220 / 2000 [ 11%]  (Warmup) \nChain 1 Iteration:  220 / 2000 [ 11%]  (Warmup) \nChain 2 Iteration:  240 / 2000 [ 12%]  (Warmup) \nChain 2 Iteration:  260 / 2000 [ 13%]  (Warmup) \nChain 1 Iteration:  240 / 2000 [ 12%]  (Warmup) \nChain 2 Iteration:  280 / 2000 [ 14%]  (Warmup) \nChain 1 Iteration:  260 / 2000 [ 13%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  320 / 2000 [ 16%]  (Warmup) \nChain 1 Iteration:  280 / 2000 [ 14%]  (Warmup) \nChain 2 Iteration:  340 / 2000 [ 17%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  360 / 2000 [ 18%]  (Warmup) \nChain 1 Iteration:  320 / 2000 [ 16%]  (Warmup) \nChain 2 Iteration:  380 / 2000 [ 19%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  340 / 2000 [ 17%]  (Warmup) \nChain 2 Iteration:  420 / 2000 [ 21%]  (Warmup) \nChain 1 Iteration:  360 / 2000 [ 18%]  (Warmup) \nChain 2 Iteration:  440 / 2000 [ 22%]  (Warmup) \nChain 1 Iteration:  380 / 2000 [ 19%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  460 / 2000 [ 23%]  (Warmup) \nChain 1 Iteration:  420 / 2000 [ 21%]  (Warmup) \nChain 2 Iteration:  480 / 2000 [ 24%]  (Warmup) \nChain 1 Iteration:  440 / 2000 [ 22%]  (Warmup) \nChain 1 Iteration:  460 / 2000 [ 23%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  480 / 2000 [ 24%]  (Warmup) \nChain 2 Iteration:  520 / 2000 [ 26%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  540 / 2000 [ 27%]  (Warmup) \nChain 1 Iteration:  520 / 2000 [ 26%]  (Warmup) \nChain 2 Iteration:  560 / 2000 [ 28%]  (Warmup) \nChain 1 Iteration:  540 / 2000 [ 27%]  (Warmup) \nChain 2 Iteration:  580 / 2000 [ 29%]  (Warmup) \nChain 1 Iteration:  560 / 2000 [ 28%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  580 / 2000 [ 29%]  (Warmup) \nChain 2 Iteration:  620 / 2000 [ 31%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  620 / 2000 [ 31%]  (Warmup) \nChain 2 Iteration:  640 / 2000 [ 32%]  (Warmup) \nChain 1 Iteration:  640 / 2000 [ 32%]  (Warmup) \nChain 2 Iteration:  660 / 2000 [ 33%]  (Warmup) \nChain 2 Iteration:  680 / 2000 [ 34%]  (Warmup) \nChain 1 Iteration:  660 / 2000 [ 33%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  680 / 2000 [ 34%]  (Warmup) \nChain 2 Iteration:  720 / 2000 [ 36%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  720 / 2000 [ 36%]  (Warmup) \nChain 2 Iteration:  740 / 2000 [ 37%]  (Warmup) \nChain 1 Iteration:  740 / 2000 [ 37%]  (Warmup) \nChain 2 Iteration:  760 / 2000 [ 38%]  (Warmup) \nChain 2 Iteration:  780 / 2000 [ 39%]  (Warmup) \nChain 1 Iteration:  760 / 2000 [ 38%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  780 / 2000 [ 39%]  (Warmup) \nChain 2 Iteration:  820 / 2000 [ 41%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  820 / 2000 [ 41%]  (Warmup) \nChain 2 Iteration:  840 / 2000 [ 42%]  (Warmup) \nChain 1 Iteration:  840 / 2000 [ 42%]  (Warmup) \nChain 2 Iteration:  860 / 2000 [ 43%]  (Warmup) \nChain 2 Iteration:  880 / 2000 [ 44%]  (Warmup) \nChain 1 Iteration:  860 / 2000 [ 43%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  880 / 2000 [ 44%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  920 / 2000 [ 46%]  (Warmup) \nChain 2 Iteration:  940 / 2000 [ 47%]  (Warmup) \nChain 1 Iteration:  920 / 2000 [ 46%]  (Warmup) \nChain 2 Iteration:  960 / 2000 [ 48%]  (Warmup) \nChain 1 Iteration:  940 / 2000 [ 47%]  (Warmup) \nChain 1 Iteration:  960 / 2000 [ 48%]  (Warmup) \nChain 2 Iteration:  980 / 2000 [ 49%]  (Warmup) \nChain 1 Iteration:  980 / 2000 [ 49%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1020 / 2000 [ 51%]  (Sampling) \nChain 1 Iteration: 1020 / 2000 [ 51%]  (Sampling) \nChain 2 Iteration: 1040 / 2000 [ 52%]  (Sampling) \nChain 2 Iteration: 1060 / 2000 [ 53%]  (Sampling) \nChain 1 Iteration: 1040 / 2000 [ 52%]  (Sampling) \nChain 2 Iteration: 1080 / 2000 [ 54%]  (Sampling) \nChain 1 Iteration: 1060 / 2000 [ 53%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1080 / 2000 [ 54%]  (Sampling) \nChain 2 Iteration: 1120 / 2000 [ 56%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1140 / 2000 [ 57%]  (Sampling) \nChain 2 Iteration: 1160 / 2000 [ 58%]  (Sampling) \nChain 1 Iteration: 1120 / 2000 [ 56%]  (Sampling) \nChain 2 Iteration: 1180 / 2000 [ 59%]  (Sampling) \nChain 1 Iteration: 1140 / 2000 [ 57%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1160 / 2000 [ 58%]  (Sampling) \nChain 2 Iteration: 1220 / 2000 [ 61%]  (Sampling) \nChain 1 Iteration: 1180 / 2000 [ 59%]  (Sampling) \nChain 2 Iteration: 1240 / 2000 [ 62%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1260 / 2000 [ 63%]  (Sampling) \nChain 2 Iteration: 1280 / 2000 [ 64%]  (Sampling) \nChain 1 Iteration: 1220 / 2000 [ 61%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1240 / 2000 [ 62%]  (Sampling) \nChain 2 Iteration: 1320 / 2000 [ 66%]  (Sampling) \nChain 1 Iteration: 1260 / 2000 [ 63%]  (Sampling) \nChain 2 Iteration: 1340 / 2000 [ 67%]  (Sampling) \nChain 2 Iteration: 1360 / 2000 [ 68%]  (Sampling) \nChain 1 Iteration: 1280 / 2000 [ 64%]  (Sampling) \nChain 2 Iteration: 1380 / 2000 [ 69%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1320 / 2000 [ 66%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1340 / 2000 [ 67%]  (Sampling) \nChain 2 Iteration: 1420 / 2000 [ 71%]  (Sampling) \nChain 1 Iteration: 1360 / 2000 [ 68%]  (Sampling) \nChain 2 Iteration: 1440 / 2000 [ 72%]  (Sampling) \nChain 2 Iteration: 1460 / 2000 [ 73%]  (Sampling) \nChain 1 Iteration: 1380 / 2000 [ 69%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1480 / 2000 [ 74%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1420 / 2000 [ 71%]  (Sampling) \nChain 2 Iteration: 1520 / 2000 [ 76%]  (Sampling) \nChain 1 Iteration: 1440 / 2000 [ 72%]  (Sampling) \nChain 2 Iteration: 1540 / 2000 [ 77%]  (Sampling) \nChain 1 Iteration: 1460 / 2000 [ 73%]  (Sampling) \nChain 2 Iteration: 1560 / 2000 [ 78%]  (Sampling) \nChain 1 Iteration: 1480 / 2000 [ 74%]  (Sampling) \nChain 2 Iteration: 1580 / 2000 [ 79%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1520 / 2000 [ 76%]  (Sampling) \nChain 2 Iteration: 1620 / 2000 [ 81%]  (Sampling) \nChain 1 Iteration: 1540 / 2000 [ 77%]  (Sampling) \nChain 2 Iteration: 1640 / 2000 [ 82%]  (Sampling) \nChain 1 Iteration: 1560 / 2000 [ 78%]  (Sampling) \nChain 2 Iteration: 1660 / 2000 [ 83%]  (Sampling) \nChain 1 Iteration: 1580 / 2000 [ 79%]  (Sampling) \nChain 2 Iteration: 1680 / 2000 [ 84%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1620 / 2000 [ 81%]  (Sampling) \nChain 2 Iteration: 1720 / 2000 [ 86%]  (Sampling) \nChain 1 Iteration: 1640 / 2000 [ 82%]  (Sampling) \nChain 2 Iteration: 1740 / 2000 [ 87%]  (Sampling) \nChain 2 Iteration: 1760 / 2000 [ 88%]  (Sampling) \nChain 1 Iteration: 1660 / 2000 [ 83%]  (Sampling) \nChain 1 Iteration: 1680 / 2000 [ 84%]  (Sampling) \nChain 2 Iteration: 1780 / 2000 [ 89%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1820 / 2000 [ 91%]  (Sampling) \nChain 1 Iteration: 1720 / 2000 [ 86%]  (Sampling) \nChain 2 Iteration: 1840 / 2000 [ 92%]  (Sampling) \nChain 1 Iteration: 1740 / 2000 [ 87%]  (Sampling) \nChain 2 Iteration: 1860 / 2000 [ 93%]  (Sampling) \nChain 1 Iteration: 1760 / 2000 [ 88%]  (Sampling) \nChain 2 Iteration: 1880 / 2000 [ 94%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1780 / 2000 [ 89%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1920 / 2000 [ 96%]  (Sampling) \nChain 1 Iteration: 1820 / 2000 [ 91%]  (Sampling) \nChain 2 Iteration: 1940 / 2000 [ 97%]  (Sampling) \nChain 2 Iteration: 1960 / 2000 [ 98%]  (Sampling) \nChain 1 Iteration: 1840 / 2000 [ 92%]  (Sampling) \nChain 2 Iteration: 1980 / 2000 [ 99%]  (Sampling) \nChain 1 Iteration: 1860 / 2000 [ 93%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 11.9 seconds.\nChain 1 Iteration: 1880 / 2000 [ 94%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1920 / 2000 [ 96%]  (Sampling) \nChain 1 Iteration: 1940 / 2000 [ 97%]  (Sampling) \nChain 1 Iteration: 1960 / 2000 [ 98%]  (Sampling) \nChain 1 Iteration: 1980 / 2000 [ 99%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 12.9 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 12.4 seconds.\nTotal execution time: 13.0 seconds.\n```\n:::\n\n```{.r .cell-code}\nphg_samples |> \n  gather_rvars(one_obs_per_group[i]) |> \n  mutate(spp = names(species_numbers)[i],\n         spp = forcats::fct_reorder(spp, .value, median)) |> \n  ggplot(aes(y = spp, dist = .value)) + \n  stat_pointinterval() + \n  geom_point(aes(y = spp, x = m),\n             data = mite_data_groupID |> \n               group_by(spp) |> \n               summarize(m = mean(abd)),\n             inherit.aes = FALSE, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n### Exercises\n\n* Try modifying the program again, this time adding a predictor: water content. What happens to `sigma_grp` in this example?\n\n## Observation-level random effects: Mite species richness\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmite_species_richness <- mite_data_long |> \n  group_by(site_id, WatrCont) |> \n  summarize(S = sum(abd > 0)) |>\n  ungroup() |> \n  mutate(water_c = (WatrCont - mean(WatrCont))/100)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'site_id'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nmite_species_richness |> \n  ggplot(aes(x = water_c, y = S)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmite_species_richness |> \n  ggplot(aes(x = S)) + \n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\n## Bernoulli presence-absence data: Mite occurrance.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}