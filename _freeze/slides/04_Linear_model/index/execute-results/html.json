{
  "hash": "6661a8dd4dda9619468f3d3b2c6b94bf",
  "result": {
    "markdown": "---\ntitle: \"Linear models\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald -- Vincent Tolon\"\ndate: \"2023-05-09\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n### Basic regression model\n\n::: {style=\"font-size: 0.8em\"}\nHierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nIn its simplest form, a regression model is usually presented as \n:::\n\n. . .\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n$$\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nIt is known as a **simple linear model**, where :\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$y_i$ is the value of a response variable for observation $i$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$x_i$ is the value of an explanatory variable for observation $i$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\beta_0$ is the model intercept\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\beta_1$ is the model slope\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\varepsilon$ is the error term\n:::\n\n## Basic regression model\n\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\n\n. . .\n\nFor example if we are interested in knowing how a newly discovered plant species (*Bidonia exemplaris*) reacts to humidity, we can relate the biomass of *B. exemplaris* sampled at 100 sites with the soil humidity content and readily  visual the data and the trend.\n\n## Basic regression model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Estimating regression parameters\n\n. . . \n\nMany techniques have been proposed to estimate the parameters of a regression model.\n\n. . . \n\nThe goal of this course is not to study these techniques but we will learn how to play with the estimated parameters because it will be very useful as we move along.\n\n. . . \n\nThe most common way to build a regression model is \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreg <- lm(b.exemplaris ~ humidity)\n```\n:::\n\n\n. . . \n\nSay we now want to build a model's confidence interval from a linear regression\n\n. . . \n\n**How would you do it ?**\n\n. . . \n\nLet's look at the model's results, maybe it will help us\n\n## Estimating regression parameters\n\n### Model's results\n\n::: {style=\"font-size: 0.75em\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(summaryReg <- summary(reg))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = b.exemplaris ~ humidity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47988 -0.26475  0.00611  0.32590  1.36077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.57389    0.04720   54.53   <2e-16 ***\nhumidity     1.10086    0.07976   13.80   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4718 on 98 degrees of freedom\nMultiple R-squared:  0.6603,\tAdjusted R-squared:  0.6569 \nF-statistic: 190.5 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: \n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nLet's say we want to construst the model's confidence intervals by sampling multiple times (say 100 times!) the regression parameters, which we will assume follow Gaussian distribution. **How would you do this?**\n::: \n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nWe could sample the model parameters but how can we do this properly?\n\n. . .\n\n**Any suggestions?**\n\n![](https://www.i2symbol.com/pictures/emojis/c/6/0/e/c60e666a9af7bcd1a7b887437b3520c3_384.png){fig-align=\"center\" width=40%}\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIf we look at the estimated regression model coefficient, we can learn a few things\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummaryReg$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 2.573889 0.04720304 54.52804 4.011925e-75\nhumidity    1.100865 0.07975800 13.80256 1.035796e-24\n```\n:::\n:::\n\n\n. . .\n\nNotably, there are uncertainty around the parameters.\n\n. . .\n\nMaybe we can use this information to sample model parameters and reconstruct models across different iterations of parameters.\n\n. . . \n\nLet's give it a shot !\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIf we assume that the parameters of our particular model follow a Gaussian distribution, we can state that  \n\n. . .\n\n$$\\beta_0 \\sim \\mathcal{N}(2.574, 0.047^2)$$\n$$\\beta_1 \\sim \\mathcal{N}(1.101, 0.080^2)$$\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIn R, we can do this as follow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta_0 <- rnorm(100, mean = regCoef[1,1], sd = regCoef[1,2])\nbeta_1 <- rnorm(100, mean = regCoef[2,1], sd = regCoef[2,2])\n```\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nBut is this the right way to do it ?\n\n![](https://www.i2symbol.com/pictures/emojis/e/e/7/b/ee7b4fb9880ef3c8ee19626cdc14bf5c_384.png){fig-align=\"center\" width=20%}\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nActually, even if the model's confidence interval look about right, they are wrong ! \n\n![](https://www.i2symbol.com/pictures/emojis/8/b/f/8/8bf88079d65d540a26a1f181cf6ba060_384.png){fig-align=\"center\" width=20%}\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nThe approach presented in the previous slide works only if we assume that the parameters are completely independent from one another.\n\n. . . \n\nA situation that happens only in very specific circumstances.\n\n. . .\n\nSo... We need to find a way to account for the non-independencies between the parameters. \n\n. . .\n\nHow can we do this ? Any ideas ?\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n::: {style=\"font-size: 0.75em\"}\nAssuming the regression parameters are normally distributed is not a bad assumption. \n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nHowever to consider a dependencies between the parameters we need to sample them from a **multivariate** normal distribution where the variance of each parameter and their dependency is defined by a covariance matrix estimated specifically for the data we model.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nThe good news is that this covariance matrix is given by `summary.lm` function\n:::\n\n. . .\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n(covReg <- summaryReg$cov.unscaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              (Intercept)      humidity\n(Intercept)  0.0100098513 -0.0005305969\nhumidity    -0.0005305969  0.0285782940\n```\n:::\n:::\n\n:::\n## Estimating regression parameters\n\n### Sampling model parameters\n\nFor our specific model, mathematically, we assume that  \n\n. . .\n\n$$\\begin{bmatrix}\n  \\beta_0\\\\\n  \\beta_1\\\\\n\\end{bmatrix} \\sim \\mathcal{MVN} \\left( \\begin{bmatrix}\n  2.574\\\\\n  1.101\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0.0100 & -0.0005 \\\\\n  -0.0005 & 0.0286 \\\\\n\\end{bmatrix} \\right)$$\n\n. . .\n\n**Note** To present the multivariate normal distribution, we rely on matrix notation. This is our first introduction into matrix algebra. We will talk about this more into this course. \n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIn R, we can sample the parameters using a multivariate normal distribution using the following code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta <- MASS::mvrnorm(100, regCoef[,1], Sigma = covReg)\n```\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters - Comparison\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Estimating regression parameters\n\nSo far, we focused on the most commonly assessed regression parameters of the simple linear model, the **slope** and the **intercept**, but there is another one that is very important to consider, especially for this course.\n\n. . .\n\n**Any ideas which one it is ?**\n\n![](https://www.i2symbol.com/pictures/emojis/3/9/7/2/3972d0a5cdf7dffdaf58a912331839c7_384.png){fig-align=\"center\" width=35%}\n\n## Estimating regression parameters\n\n::: {style=\"font-size: 0.75em\"}\nIf we go back to the mathematical description of the model\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nwe can see that in the simple linear regression the error term ($\\varepsilon$) has actually a very precise definition:\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\n$$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n:::\n\n::: {style=\"font-size: 0.75em\"}\nwhere $\\sigma^2$ is an estimated variance.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nFor most the course, we will play with the variance parameter $\\sigma^2$ in a bunch of different ways. \n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nBut before we do this, we need to understand a bit more about how this parameter influence the model.\n::: \n\n\n## Estimating regression parameters\n\nA first way to do this is to think about the simple linear regression in a slightly different way. Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as \n$$\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i}, \\sigma^2)\n$$\n\n. . .\n\nAs we will see later in this course, this writting style will become particularly useful as we move forward in this course.\n\n## Estimating regression parameters\n\n### Variance of the model ($\\sigma^2$)\n\n::: {style=\"font-size: 0.75em\"}\nIn essence, $\\sigma^2$ tells us about what the model could not account for.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nFor example, let's compare the biomass of *Bidonia exemplaris* with that of *Ilovea chicktighii*, another species (a carnivorous plant) \n:::\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n## Estimating regression parameters\n### Variance of the model ($\\sigma^2$)\n\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using `summary.lm`)\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regression model\nregBexemplaris <- lm(b.exemplaris ~ humidity)\nregIchicktighii <- lm(i.chicktighii ~ humidity)\n\n# Summary\nsummaryRegBexemplaris <- summary(regBexemplaris)\nsummaryRegIchicktighii <- summary(regIchicktighii)\n```\n:::\n\n\n## Estimating regression parameters\n### Variance of the model ($\\sigma^2$)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nFor *Bidonia exemplaris*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error\n(Intercept) 2.573889 0.04720304\nhumidity    1.100865 0.07975800\n```\n:::\n\n```{.r .cell-code}\n# Estimated variance\nsummaryRegBexemplaris$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.471798\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nFor *Ilovea chicktighii*\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated coefficients\nsummaryRegIchicktighii$coefficients[,1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n```\n:::\n\n```{.r .cell-code}\n# Estimated variance\nsummaryRegIchicktighii$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.089239\n```\n:::\n:::\n\n:::\n::::\n\n## Limits of the simple linear regression \n\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\n. . .\n\n1.  One explanatory is almost never enough to approach biological questions nowadays.\n\n. . .\n\n2.  The simple linear model assumes that the error follows a Gaussian distribution.\n\n## Multiple linear regression\n\n::: {style=\"font-size: 0.85em\"}\nSimple linear regression can be extended to account for multiple explanatory variables to study more complexe problems. This type of regression model is known as a **multiple linear regression**.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em\"}\nMathematically, a multiple linear regression can be defined as \n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n$$\n:::\n\n. . . \n\n::: {style=\"font-size: 0.85em\"}\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, in practice, matrix algebra is quite practical to use in this context and also for this course in genral.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em\"}\nIn this respect, let's take a bit of time to get acquinted with different basic (and maybe not so basic!) knowledge of matrix algebra.\n:::\n\n# Matrix algebra interlude\n\n## A general way to write matrices\n$$\n\\mathbf{A} = \\begin{bmatrix}\n\t\t\t\tA_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n\t\t\t\tA_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n\t\t\\end{bmatrix}\n$$\n$$A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}$$\n\n# Basic matrix operations\n\n## The transpose of a matrix\n\n::::: { style=\"font-size: 0.85em\"}\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n$$A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n$$\n\n$$B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n$$\n\n$$C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n$$\n:::\n\n::: {.column width=\"50%\"}\n\n$$A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n$$\n    \n \n$$B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n$$\n    \n\n$$C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n$$\n:::\n::::\n:::::\n\n## The transpose of a matrix\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n```\n:::\n\n```{.r .cell-code}\nt(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2\n```\n:::\n:::\n\n## Addition and Substraction\n\n$$\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}$$\n$$C_{ij} = A_{ij} \\pm B_{ij}$$\n\n::: {style=\"color: blue;\"}\n$$\\begin{bmatrix}\n\t\t\t\t3 & 5\\\\\n\t\t\t\t1 & -2\\\\\n\t\t\t\\end{bmatrix} + \n\t\t\t\\begin{bmatrix}\n\t\t\t\t2 & 1\\\\\n\t\t\t\t4 & -2\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\\begin{bmatrix}\n\t\t\t\t3+2 & 5+1\\\\\n\t\t\t\t1+4 & -2-2\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\\begin{bmatrix}\n\t\t\t\t5 & 6\\\\\n\t\t\t\t5 & -4\\\\\n\t\t\t\\end{bmatrix}$$\n:::\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB <- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4\n```\n:::\n:::\n\n\n## Multiplying a matrix by a scalar\n\t\n$$\\mathbf{B} = c\\mathbf{A}$$\n$$B_{ij} = cA_{ij}$$\n\t\n::: {style=\"color: blue;\"}\n$$\n\t\t\t0.3 \\begin{bmatrix}\n\t\t\t\t3 & 5\\\\\n\t\t\t\t1 & -2\\\\\n\t\t\t\\end{bmatrix} =  \n\t\t\t\\begin{bmatrix}\n\t\t\t\t0.9 & 1.5\\\\\n\t\t\t\t0.3 & -0.6\\\\\n\t\t\t\\end{bmatrix}\n$$\n:::\t\t\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc <- 0.3\nc*A \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6\n```\n:::\n:::\n\n\n## Matrix multiplications (not divisions!)\n\t\n$$\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$$\n\n$$C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}$$\n\n**Rules**\n\t\nAssociative: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}$ \n\t\nDistributive: $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}$\n\t\nNot commutative: $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$\n\n\n## Inner product\n$$(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j$$\n\n:::{style=\"color: blue;\"}\n$$\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n$$\n:::\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx <- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   31\n[2,]   -8\n```\n:::\n:::\n\n\n# Some important matrices\n\n## Identity matrix\n\nThe identity matrix is a square matrix where all values of its diagonal are 0 except  the diagonal values which are all 1s.\n\n::: { style=\"font-size: 0.9em\"}\n$$\n\\mathbf{I}=\\begin{bmatrix}\n\t\t\t\t1 & 0 & 0\\\\\n\t\t\t\t0 & 1 & 0\\\\\n\t\t\t\t0 & 0 & 1\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\nThe identity matrix is important because \n\n$$\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}$$\nor \n\n$$\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}$$\n:::\n\n## Diagonal matrix\n\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n:::: {style=\"font-size: 0.8em\"}\n$$D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}$$\n\nAn example\n\n::: {style=\"color: blue;\"}\n$$\n\\begin{bmatrix}\n\t\t\t\t-1 & 0 & 0\\\\\n\t\t\t\t0 & 0 & 0\\\\\n\t\t\t\t0 & 0 & 6\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n::::\n\n## Triangular matrix\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n#### Lower triangular matrix\n$$\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}$$\n:::\n::: {.column width=\"50%\"}\n#### Upper triangular matrix\n$$\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}$$\n:::\n::::\n\n## Symmetric matrix\n\nThe values on the  above and below the diagonal are match so that $A = A^t$\n\n::: {style=\"color: blue;\"}\n$$\n\\begin{bmatrix}\n\t\t\t\t3 & 4 & -10\\\\\n\t\t\t\t4 & 5 & 7\\\\\n\t\t\t\t-10 & 7 & -6\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n\n## Matrix inversion\n\n::: { style=\"font-size: 0.8em\"}\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix $\\mathbf{A}$ is defined as $\\mathbf{A}^{-1}$\n\nAs such,\n$$\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}$$ \n:::\n\n. . .\n\n::: { style=\"font-size: 0.8em\"}\nIn R\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv <- solve(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n```\n:::\n\n```{.r .cell-code}\nA %*% Ainv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1\n```\n:::\n:::\n\n\n## Matrix inversion\n### Inverting a diagonal matrix\n\n$$D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}$$\n\n# End of matrix algebra interlude\n\n## Multiple linear regression\n\n. . .\n\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the $\\beta$s) can depend on other other data and parameters. \n\n. . .\n\nAs we saw earlier, a classic way to write multiple linear regression is \n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n$$\n\n. . .\n\nHowever, we can rewrite this using matrix notation has\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n\n## Multiple linear regression\n### Matrix notation\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n\nUsing the matrix notation, we assume that \n\n. . .\n\n\n::: { style=\"font-size: 0.9em\"}\n- $\\mathbf{y}$ is a vector of length $n$ (samples)\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\mathbf{X}$ is a matrix with $n$ samples (rows) and representing $p$ explanatory variables (columns)\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\boldsymbol{\\beta}$ is a vector of $p$ regression coefficients\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\boldsymbol{\\varepsilon}$ is a vector of residuals of length $n$\n:::\n\n## Error in linear models\n\nAs previously mentioned, in (simple and multiple!) linear regression the error term ($\\varepsilon$) has actually a very precise definition:\n\n$$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$ where $\\sigma^2$ is an estimated variance\n\n. . .\n\nwhich means that the error linear regression follows a Gaussian distribution with an estimateed variance.\n\n. . .\n\n**Note** The model can be written using matrix notation as  \n$$\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\beta}\\mathbf{X}, \\sigma^2)$$ where $\\sigma^2$\n\n## Generalized linear models\n\n. . .\n\nTo go around this problem, *generalized* linear models (GLMs) have been proposed. In essence, GLMs use *link functions* to adapt models for them to be used on non-Gaussian data.\n\n. . .\n\nMathematically, the generic way to write generalized linear model is\n\n$$\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n$$\n\n. . .\n\nor in matrix notation\n\n$$\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n$$\n\nwhere $g$ is the link function and $g^{-1}$ the inverse link function.\n\n## Generalized linear models\n### Link functions\n\n::: {style=\"font-size: 0.75em\"}\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nArguably the most common link function in ecology is \n:::\n\n. . .\n\n**logit link function**\n\n::: {style=\"font-size: 0.75em\"}\nIt is commonly used for modelling binary (0-1) data.\n\n$$\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nThe inverse logit link function is \n\n$$\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n$$\n:::\n\n## Generalized linear models\n### Link functions\n\nAnother commonly used link function is\n\n. . .\n\n**log link function**\n\nIt is commonly used for modelling count data.\n\n$$\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n$$\n\n. . .\n\nThe inverse logit link function is \n\n$$\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n$$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}