{
  "hash": "7e612ff30d01bf6ae001da315e874bb6",
  "result": {
    "markdown": "---\ntitle: \"Complex hierarchical models\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald -- Vincent Tolon\"\ndate: \"2023-05-10\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n## \"Complex\" hierarchical model\n\nBy \"complex\" we refer to hierarchical models for which more than one parameters are accounted for in a parameter hierarchy. \n\nAs we will see, there are a number of ways this can complexify the structure of a model in ways that are not always obvious.\n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nWhen estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure. \n\nHowever, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=2400}\n:::\n:::\n\n::: \n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nEven if you run the model for many, many (many !) iterations, it never seems to converge.\n\nWhat should we do ?\n\n![](https://www.i2symbol.com/pictures/emojis/a/3/6/b/a36b215220fc0153c107ee9d022cb75e_384.png){fig-align=\"center\" width=20%}\n::: \n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nThere is a very cool trick that can help us here. \n\nBefore we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution ($\\mathcal{N}(0,1)$) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.\n\n### The convergence trick\n\nIf we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.\n\n:::: {style=\"text-align: center; font-size: 2em; color: red \"}\nAny ideas how to do this ?\n::::\n:::\n\n::: {style=\"font-size: 0.9em\"}\n## The convergence trick\n\n### Translation\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=1920}\n:::\n:::\n\n:::\n\n## The convergence trick\n\n### Translation\n\nMathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.\n\nThis means that \n\n$$\\mathcal{N}(\\mu, \\sigma^2)$$\nis exactly the same as \n\n$$\\mathcal{N}(0, \\sigma^2) + \\mu$$\n\n## The convergence trick\n\n### Scaling\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## The convergence trick\n\n### Scaling\n\nMathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.\n\nThis means that \n\n$$\\mathcal{N}(\\mu, \\sigma^2)$$\nis exactly the same as \n\n$$\\mathcal{N}(\\mu, 1) \\times \\sigma^2$$\n\n## The convergence trick\n\nThe convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from **outside** the distribution\n\n$$\\mathcal{N}(0, 1) \\times \\sigma^2 + \\mu$$\nWhen implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient.\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n:::\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\n\nTo show how our convergence trick works for a multivariate Gaussian distribution, let's first visualize the two dimensional version of this distribution.\n:::\n\n## Bivariate Gaussian distribution\n\n$$\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  0\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n  2 & -1\\\\\n  -1 & 2\\\\\n\\end{bmatrix}\\right)$$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Translation\n\nFor a multivariate distribution, a translation amounts to adding a **vector** of values to make the translation. \n\nMathematically, this means that\n\n$$\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right)=\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  \\vdots\\\\\n  0\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right) + \\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix}$$\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Scaling\n\nUnlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform... But mathematician and statistician have worked hard to figure out how to do this properly.\n\nHowever, we need to delve a little deeper into matrix algebra to understand how to scale a multivariate Gaussian distribution.\n\n# Matrix algebra interlude (part 2!)\n\n## Scaling a covariance matrix {auto-animate=\"true\"}\n\n::: {style=\"font-size: 0.8em\"}\nFirst recall that an covariance matrix $\\mathbf{\\Sigma}$ is a square matrix (i.e. it is an $n\\times n$ matrix).\n\nTo scale $\\mathbf{\\Sigma}$, we cannot only multiply it by a scalar or even by a single matrix, we need to use the following matrix multiplication\n\n$$\\mathbf{A}\\mathbf{\\Sigma}\\mathbf{A}^t$$\nwhere $\\mathbf{A}$ is a $p\\times n$ matrix of weight to be used for the scaling (a \"scaling\" matrix) and $\\mathbf{A}^t$ is its tranpose.\n\nThe technical reason why we **need** to use the equation above is to ensure that the resulting scaled covariance matrix also has an $n \\times n$ dimension. \n\nIf only\n$$\\mathbf{A}\\mathbf{\\Sigma}$$\nis used the dimension of the resulting matrix also would be $p \\times p$.\n:::\n\n## Square-root of a matrix\n\n::: {style=\"font-size: 0.8em\"}\nBecause in our problem weighting (or scaling) matrices are usually other covariance matrices, to apply the matrix scaling operation described previously, we need to find a way square-root a matrix.\n\nThis where the genious of André-Louis Cholesky comes to the rescue.\n:::\n\n![](https://upload.wikimedia.org/wikipedia/commons/5/5f/Andre_Cholesky.jpg){fig-align=\"center\" width=20%}\n\n## Square-root of a matrix\n\n### Cholesky decomposition\n\n::: { style=\"font-size: 0.8em\"}\nAndré-Louis Cholesky discovered a matrix decomposition approach probably around 1902 (so when he was 27 years old!), although it was attributed to him a few years after his death.\n\nThe Cholesky decomposition allows to decompose a square matrix in a triangular matrix, which, when multiplied by its transposed will allow us to recover the initial matrix. \n\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice\n\nIn math terms the Cholesky decomposition is defined as \n$$\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t$$\n:::\n\n## Square-root of a matrix\n\n### Cholesky decomposition\n\n#### Example\n$$\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t$$\n\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1 & 1 & 1\\\\\n\t\t\t1 & 5 & 5\\\\\n\t\t\t1 & 5 & 14\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t1 & 0 & 0\\\\\n\t\t\t1 & 2 & 0\\\\\n\t\t\t1 & 2 & 3 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t1 & 1 & 1\\\\\n\t\t\t0 & 2 & 2\\\\\n\t\t\t0 & 0 & 3 \\\\\n\t\t\\end{bmatrix}\n$$\n\n# End of matrix algebra interlude\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Scaling\n\n::: { style=\"font-size: 0.7em\"}\nTo scale the following multivariate Gaussian distribution\n$$\\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{\\Sigma}\\right),$$ \n\nThe following steps need to be applied \n\n1. Apply the Cholesky decomposition on the scaling matrix, here $\\mathbf{\\Sigma}$\n$$\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^t$$\n2. Multiply the $\\mathbf{L}$ matrix to a standard variance multivariate Gaussian distribution\n\n$$\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t.$$\n\nRecall, that $\\mathbf{I}$ is the identity matrix.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\nIf we apply translation and scaling together on a multivariate Gaussian distribution, we get\n\n$$\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\mathbf{0},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t + \\boldsymbol{\\mu}$$\nWhen implementing in Stan some of the models we will discuss in this course, this convergence trick becomes very practical because it can lead a model to convergence much faster than without using this trick.\n\n\n# \"Complex\" hierarchy on the intercept\n\n## Interacting hierarchies\n\n::: {style=\"font-size: 0.7em\"}\n`lme4` notation : `y ~ (1 | f:g)`\n\nThis model assumes that factors `f` and `g` interact to make a hierarchy.\n\nMathematically, it can be translated to \n\n$$\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f[l_f]\\times g[l_g] },\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,l_g = 1\\dots k_g$$\nor \n\n$$y_i = b_{f[l_f]\\times g[l_g]} + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,,\\,\\,\\,l_g = 1\\dots k_g\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n$$\nIn words, a multi-factor hierarchy can be constructed my multiplying the levels of individual factors to account for a more complexe hierarchy. Another way to think about this structure is to construct a complexe hierarchy using multiple simpler hierarchies. \n\nThis is because in this model\n\n$$\\mathbf{b} \\sim \\mathcal{N}\\left(0, \\sigma^2_{f\\times g}\\right)$$\n:::\n\n## Interacting hierarchies\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Hierarchies within hierarchies\n\n::: {style=\"font-size: 0.7em\"}\n`lme4` notation : `y ~ (1 | f/g)` or `y ~ (1 | f) + (1 | f:g)`\n\nThis model assumes there is a hierarchy that varies among the levels of factor `f` and among the levels of factor `g` but only within the levels of factor `f`.\n\nMathematically, it can be translated to \n\n$$\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{g[l_g]\\in f[l_f]},\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,l_g = 1\\dots k_g$$\nor \n\n$$y_i = b_{g[l_g]\\in f[l_f]} + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,,\\,\\,\\,l_g = 1\\dots k_g\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n$$\nIn words, the model parameter $b$ will change for a sample $i$ only when the level $l_g$ of the factor $g$ **within** the level $l_f$ of the factor $f$ changes. \n\nThis is because in this model\n\n$$\\mathbf{b} \\sim \\mathcal{N}\\left(0,\\sigma^2_{g\\in f}\n                                \\right)$$\n:::\n\n## Hierarchies within hierarchies\n\nICI\n\n\n\n## `y ~ (1 | f/g)`\n### Stan code for this model\n\n::: {style=\"margin-top: 200px; font-size: 2.5em; color: red;\"}\nFor Andrew?\n:::\n\n## `y ~ (1 | f/g)`\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## `y ~ (1 | f) + (1 | g)`\n\n::: {style=\"font-size: 0.7em\"}\nOther notation used : `y ~ 1 + (1 | f) + (1 | g)`\n\nThis model assumes there is a hierarchy that varies among the two factors.\n\nMathematically, it can be translated to \n\n$$\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f[l_f]\\times g[l_g]},\\sigma^2\\mathbf{I}) \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,\\text{and}\\,\\,\\,l_g = 1\\dots k_g$$\nor \n\n$$y_i = b_{f_i[l_f]\\times g_i[l_g]} + \\varepsilon \\quad\\forall\\quad l_f = 1\\dots k_f\\,\\,\\,\\,,\\,\\,\\,l_g = 1\\dots k_g\\,\\,\\,\\,\\text{and}\\,\\,\\,i = 1\\dots n$$\nIn words, this means that the model values $b$ will change for a sample $i$ only when the interaction the level $l_f$ of the factor $f$ and the  level $l_g$ of the factor $g$ changes. \n\nThis is because in this model\n\n$$\\mathbf{b} \\sim \\mathcal{N}\\left(0, \n                                \\begin{bmatrix}\n                                  \\sigma^2_f & \\sigma_f\\sigma_g\\\\\n                                  \\sigma_f\\sigma_g& \\sigma^2_g\\\\\n                                \\end{bmatrix}\n                                \\right)$$\n:::\n\n## `y ~ (1 | f) + (1 | g)`\n\nThis model has a number interesting properties\n\n- It does **not** assumes that the two factors act independent. Actually, if you are interested in such a model, the code to use is not as straight forward to write with these packages.\n- If a particular levels is associated to the same samples for the two factors, usually this create technical problems and the model cannot be estimated properly (this is true regardless of how you estimate these parameter)\n- This can be generalized to as many factors as we want. We will see how this can be useful later.\n\n\n## `y ~ x + (x || g)`\n\n## second\n\n::: r-fit-text\nTest your model\n:::\n\n## \n\nimg:\n\n![](img/bg.jpg)\n\nit is a landscape\n\n##  {auto-animate=\"true\"}\n\n::: {style=\"margin-top: 100px;\"}\ncheck with simulations\n:::\n\n##  {auto-animate=\"true\"}\n\n::: {style=\"margin-top: 200px; font-size: 2.5em; color: red;\"}\ncheck with simulations\n:::\n\n## choose parameters {auto-animate=\"TRUE\"}\n\n``` r\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\n```\n\n## make up an X variable {auto-animate=\"TRUE\"}\n\n``` r\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\n```\n\n## calculate the average {auto-animate=\"TRUE\"}\n\n``` r\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\n```\n\n## simulate some observations {auto-animate=\"TRUE\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n```\n:::\n\n\n## finally, visualize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y_obs ~ x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## here it is all on one slide\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Or we can present the code and results separately\n\n::: panel-tabset\n### The code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyintercept <- 4\nslope <- 1.3\nobs_error <- .5\nx <- runif(200, min = -1, max = 1)\ny_mean <- yintercept + slope * x\ny_obs <- rnorm(200, mean = y_mean, sd = obs_error)\n```\n:::\n\n\n### The figure\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y_obs ~ x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n:::\n\n## another equation\n\n$$\n2 + 4 = 6\n$$\n\n## The equation\n\n$$\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n$$\n\n## The model {auto-animate=\"TRUE\"}\n\n``` stan\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}\n```\n\n## Declare the data {auto-animate=\"TRUE\"}\n\n``` {.stan code-line-numbers=\"1-4\"}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}\n```\n\n## State parameters {auto-animate=\"TRUE\"}\n\n``` {.stan code-line-numbers=\"5-8\"}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}\n```\n\n## Write the likelihood and priors {auto-animate=\"TRUE\"}\n\n``` {.stan code-line-numbers=\"9-13\"}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}\n```\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}