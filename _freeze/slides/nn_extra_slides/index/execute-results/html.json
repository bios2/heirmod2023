{
  "hash": "0daa494d518798c14efdc84116788119",
  "result": {
    "markdown": "---\ntitle: \"Extra stuff\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Andrew MacDonald and Guillaume Blanchet and Vincent Tolon\"\ndate: \"2023-02-10\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n## Square matrix\n\nThe square matrix has as many rows at it has columns \n$$\n\\mathbf{B} = \\begin{bmatrix}\n\t\t\t\tB_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n\t\t\t\tB_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n\t\t\\end{bmatrix}\n$$\n\n## Determinant of a matrix\n\nNot sure if it should be included or not\n\n## Eigenvectors and eigenvalues\n\t\n::: {style=\"font-size: 0.9em\"}\nRight eigenvector is :\n\t\n$$\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}$$\nLeft eigenvector is :\n\t\n$$\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}$$\n\t\n**Rules**\n\n- $\\mathbf{A}$ has to be a square matrix\n- If $\\mathbf{w}$ is an eigenvector of $\\mathbf{A}$, so is $c\\mathbf{w}$ for any value of $c \\neq0$\n- The right eigenvector of $\\mathbf{A}^T$ is the left eigenvector of $\\mathbf{A}$\n- Eigenvectors are linearly independent\n:::\n\n## Positive definite matrix\n\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\n:::{style=\"font-size: 0.8em\"}\n`Error: Matrix X is not positive definite`\n:::\n\nor similarly\n\n:::{style=\"font-size: 0.8em\"}\n`Error: Matrix X is not positive semi-definite`\n:::\n\nWhat does this mean ? Any idea ?\n\n## Positive (semi-)definite matrix\n### Nerdy mathematical definition\n\n**Positive definite matrix**\n\n$\\mathbf{M}$ is a positive definite matrix if, for any real vector $\\mathbf{z}$, $\\mathbf{z}^t\\mathbf{M}\\mathbf{z} > 0$\n\n**Positive semi-definite matrix**\n\n$\\mathbf{M}$ is a positive semi-definite matrix if, for any real vector $\\mathbf{z}$, $\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0$\n\n## Positive (semi-)definite matrix\n### Checking if a matrix is positive (semi-)definite\n\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\n\nAll we have to do is look at the eigenvalue of a square matrix. \n\nIf all eigenvalues of a matrix $\\mathbf{M}$ larger than 0, matrix $\\mathbf{M}$ is positive definite.\n\nIf all eigenvalues of a matrix $\\mathbf{M}$ larger than or equal ro 0, matrix $\\mathbf{M}$ is positive semi-definite.\n\n## Dot product\n$$\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n$$\n\n:::{style=\"color: blue;\"}\n$$\n\t\t\t\\begin{bmatrix}\n\t\t\t\t3 & 1\\\\\n\t\t\t\\end{bmatrix}\n\t\t\t\\cdot\n\t\t\t\\begin{bmatrix}\n\t\t\t\t2\\\\ 5\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\t3 \\times  2 + 1 \\times 5 = 11\n$$\n:::\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv <- c(3, 1)\nx <- c(2,5)\nsum(v * x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\n\n## Solving systems of linear equation\n\n$$\n\\begin{align*}\n\t\t1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n\t\t0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n\t\t1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n\t\\end{align*}\n$$\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\n## Solving systems of linear equation\n\n::: {style=\"font-size: 0.9em\"}\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\nHow do we mathematically solve for $\\boldsymbol{\\beta}$?\n\n$$\n\t\\begin{align*}\n\t\t\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n\t\\end{align*}\n$$\n:::\n\n## Solving systems of linear equation\n\n::: {style=\"font-size: 0.9em\"}\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\nHow do we solve for $\\boldsymbol{\\beta}$ in R?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny <- c(1, 0, 1)\n\n(beta <- solve(X, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.20000000  0.05714286 -0.02857143\n```\n:::\n:::\n\n:::\n\n\n\n## A few words about the prior\n\n### Conjugate priors\n\nThese types of priors are convenient to use because \n\n- They are computationally faster to use\n- They can be interepreted as additional data\n\n#### Why are they useful?\n\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation.\n\n## A few words about the prior\n\n### Conjugate priors\n\n#### What does it mean to be of the same *functional form*?\n\nIt means that both distribution have th same mathematical structure. \n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Binomial distribution**\n$$\\theta^a(1-\\theta)^b$$\n:::\n::: {.column width=\"50%\"}\n**Beta distribution**\n$$\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}$$\n:::\n::::\n\n<https://en.wikipedia.org/wiki/Conjugate_prior>\n\n\n\n\n\n## Move this in another place \nTechnically, we can sample all $\\boldsymbol{\\beta}_{f[l]}$ independently, however, using multivariate Gaussian distribution, we can sample the $\\boldsymbol{\\beta}_{f}$ for all levels of the factor in one go as\n\n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$$\nwhere \n\n- $\\boldsymbol{\\mu}_{f}$ is a vector of $k$ means, one for each level of the factor\n- $\\mathbf{D}_f$ is a $k\\times k$ diagonal matrix with variance term on the diagonal\n\n## Hierarchy on the intercept's mean\n\n::: {style=\"font-size: 0.7em\"}\nThe structure of matrix $\\mathbf{D}_f$ can be considered in two different ways in \n\n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$$\n\nWritten in the general form as we did in the equation above, we assume that all variance on the diagonal are potentially different. Or in other words, the variance in each group is assumed to be different\n::: \n::: {style=\"font-size: 0.6em\"}\n$$\\mathbf{D}_f = \\begin{bmatrix}\n\t\t\t\t\\sigma^2_{f[1]} & 0 & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t0 & \\sigma^2_{f[2]} & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  0 & 0 & \\dots & \\sigma^2_{f[l]} & \\dots & 0\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f[k]}\\\\\n\t\t\\end{bmatrix}$$\n:::\n\n## Hierarchy on the intercept's mean\n\n::: {style=\"font-size: 0.7em\"}\nHowever, it can be assumed to be all the same variance regardless of the group considered \n\n::: {style=\"font-size: 0.68em\"}\n$$\\mathbf{D}_f = \\begin{bmatrix}\n\t\t\t\t\\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n\t\t\\end{bmatrix}$$\n:::\n\nIn this case, $\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$\ncan be rewritten as \n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})$$\n**Note**: This is essentially the same thing as a one-way analysis of variance.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}