{
  "hash": "bac96b2be8bea41f83ffdd793cdc0ac5",
  "result": {
    "markdown": "---\ntitle: \"Extra stuff\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Andrew MacDonald -- Guillaume Blanchet -- Vincent Tolon\"\ndate: \"2023-02-10\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n## Square matrix\n\nThe square matrix has as many rows at it has columns \n$$\n\\mathbf{B} = \\begin{bmatrix}\n\t\t\t\tB_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n\t\t\t\tB_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n\t\t\\end{bmatrix}\n$$\n\n## Determinant of a matrix\n\nNot sure if it should be included or not\n\n## Eigenvectors and eigenvalues\n\t\n::: {style=\"font-size: 0.9em\"}\nRight eigenvector is :\n\t\n$$\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}$$\nLeft eigenvector is :\n\t\n$$\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}$$\n\t\n**Rules**\n\n- $\\mathbf{A}$ has to be a square matrix\n- If $\\mathbf{w}$ is an eigenvector of $\\mathbf{A}$, so is $c\\mathbf{w}$ for any value of $c \\neq0$\n- The right eigenvector of $\\mathbf{A}^T$ is the left eigenvector of $\\mathbf{A}$\n- Eigenvectors are linearly independent\n:::\n\n## Positive definite matrix\n\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\n:::{style=\"font-size: 0.8em\"}\n`Error: Matrix X is not positive definite`\n:::\n\nor similarly\n\n:::{style=\"font-size: 0.8em\"}\n`Error: Matrix X is not positive semi-definite`\n:::\n\nWhat does this mean ? Any idea ?\n\n## Positive (semi-)definite matrix\n### Nerdy mathematical definition\n\n**Positive definite matrix**\n\n$\\mathbf{M}$ is a positive definite matrix if, for any real vector $\\mathbf{z}$, $\\mathbf{z}^t\\mathbf{M}\\mathbf{z} > 0$\n\n**Positive semi-definite matrix**\n\n$\\mathbf{M}$ is a positive semi-definite matrix if, for any real vector $\\mathbf{z}$, $\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0$\n\n## Positive (semi-)definite matrix\n### Checking if a matrix is positive (semi-)definite\n\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\n\nAll we have to do is look at the eigenvalue of a square matrix. \n\nIf all eigenvalues of a matrix $\\mathbf{M}$ larger than 0, matrix $\\mathbf{M}$ is positive definite.\n\nIf all eigenvalues of a matrix $\\mathbf{M}$ larger than or equal ro 0, matrix $\\mathbf{M}$ is positive semi-definite.\n\n## Dot product\n$$\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n$$\n\n:::{style=\"color: blue;\"}\n$$\n\t\t\t\\begin{bmatrix}\n\t\t\t\t3 & 1\\\\\n\t\t\t\\end{bmatrix}\n\t\t\t\\cdot\n\t\t\t\\begin{bmatrix}\n\t\t\t\t2\\\\ 5\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\t3 \\times  2 + 1 \\times 5 = 11\n$$\n:::\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv <- c(3, 1)\nx <- c(2,5)\nsum(v * x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\n\n## Solving systems of linear equation\n\n$$\n\\begin{align*}\n\t\t1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n\t\t0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n\t\t1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n\t\\end{align*}\n$$\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\n## Solving systems of linear equation\n\n::: {style=\"font-size: 0.9em\"}\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\nHow do we mathematically solve for $\\boldsymbol{\\beta}$?\n\n$$\n\t\\begin{align*}\n\t\t\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n\t\t\\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n\t\\end{align*}\n$$\n:::\n\n## Solving systems of linear equation\n\n::: {style=\"font-size: 0.9em\"}\n$$\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta} \n$$\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1\\\\ 0\\\\ 1\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t3 & 5 & -4\\\\\n\t\t\t1 & -2 & 3\\\\\n\t\t\t4 & 6 & 5 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n\t\t\\end{bmatrix}\n$$\n\nHow do we solve for $\\boldsymbol{\\beta}$ in R?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny <- c(1, 0, 1)\n\n(beta <- solve(X, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.20000000  0.05714286 -0.02857143\n```\n:::\n:::\n\n:::\n\n\n\n## A few words about the prior\n\n### Conjugate priors\n\nThese types of priors are convenient to use because \n\n- They are computationally faster to use\n- They can be interepreted as additional data\n\n#### Why are they useful?\n\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation.\n\n## A few words about the prior\n\n### Conjugate priors\n\n#### What does it mean to be of the same *functional form*?\n\nIt means that both distribution have th same mathematical structure. \n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Binomial distribution**\n$$\\theta^a(1-\\theta)^b$$\n:::\n::: {.column width=\"50%\"}\n**Beta distribution**\n$$\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}$$\n:::\n::::\n\n<https://en.wikipedia.org/wiki/Conjugate_prior>\n\n\n\n\n\n## Move this in another place \nTechnically, we can sample all $\\boldsymbol{\\beta}_{f[l]}$ independently, however, using multivariate Gaussian distribution, we can sample the $\\boldsymbol{\\beta}_{f}$ for all levels of the factor in one go as\n\n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$$\nwhere \n\n- $\\boldsymbol{\\mu}_{f}$ is a vector of $k$ means, one for each level of the factor\n- $\\mathbf{D}_f$ is a $k\\times k$ diagonal matrix with variance term on the diagonal\n\n## Hierarchy on the intercept's mean\n\n::: {style=\"font-size: 0.7em\"}\nThe structure of matrix $\\mathbf{D}_f$ can be considered in two different ways in \n\n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$$\n\nWritten in the general form as we did in the equation above, we assume that all variance on the diagonal are potentially different. Or in other words, the variance in each group is assumed to be different\n::: \n::: {style=\"font-size: 0.6em\"}\n$$\\mathbf{D}_f = \\begin{bmatrix}\n\t\t\t\t\\sigma^2_{f[1]} & 0 & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t0 & \\sigma^2_{f[2]} & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  0 & 0 & \\dots & \\sigma^2_{f[l]} & \\dots & 0\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f[k]}\\\\\n\t\t\\end{bmatrix}$$\n:::\n\n## Hierarchy on the intercept's mean\n\n::: {style=\"font-size: 0.7em\"}\nHowever, it can be assumed to be all the same variance regardless of the group considered \n\n::: {style=\"font-size: 0.68em\"}\n$$\\mathbf{D}_f = \\begin{bmatrix}\n\t\t\t\t\\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n\t\t\\end{bmatrix}$$\n:::\n\nIn this case, $\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)$\ncan be rewritten as \n$$\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})$$\n**Note**: This is essentially the same thing as a one-way analysis of variance.\n\n:::\n\n\n\n## A (very !) general formulation\n\n::: {style=\"font-size: 0.68em\"}\nAs discuss yesterday, a linear model can be writen as\n\n$$(\\mathbf{y}|\\mathbf{X}, \\boldsymbol{\\beta}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma\\mathbf{y}^2\\mathbf{I})$$\n\nwhere\n\n- $\\mathbf{y}$ is a vector quantifying a response variable of length $n$\n- $\\mathbf{X}$ is a matrix of explanatory variables with $n$ rows (samples) and $p$ columns (explanatory varaibles) \n- $\\boldsymbol{\\beta}$ is a vector $p$ pararameters weighting the importance of each explanatory variables in $\\mathbf{X}$\n- $\\sigma_\\mathbf{y}^2$ is a measure of variance of the error in the regression model\n- $\\mathbf{I}$ is an $n \\times n$ identity matrix \n:::\n\n## A (very !) general formulation\n\n::: {style=\"font-size: 0.8em\"}\nA hierarchical model is a generalization of the linear model such that \n\n$$(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})$$\n:::\n::: {style=\"font-size: 0.68em\"}\n\nwhere\n\n- $\\mathbf{y}$ is a vector quantifying a response variable of length $n$\n- $\\mathbf{X}$ is a matrix of explanatory variables with $n$ rows (samples) and $p$ columns (explanatory variables) \n- $\\boldsymbol{\\beta}$ is a vector $p$ pararameters weighting the importance of each explanatory variables in $\\mathbf{X}$\n- $\\sigma_\\mathbf{y}^2$ is a measure of variance of the error in the regression model\n- $\\mathbf{I}$ is an $n \\times n$ identity matrix \n\n:::: {style=\"color: blue\"}\n- $\\mathbf{Z}$ is another matrix of explanatory variables with $n$ rows (samples) and $q$ columns (explanatory variables) \n- $\\mathbf{b}$ is a vector $q$ pararameters weighting the importance of each explanatory variables in $\\mathbf{Z}$\n::::\n:::\n\n## A (very !) general formulation\n\n::: {style=\"font-size: 0.8em\"}\nA hierarchical model is a generalization of the linear model such that \n\n$$(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})$$\nWhat is also noticeable in this model is the conditional relationship between $\\mathbf{y}$ and $\\mathbf{b}$.\n\nSpecifically, in this formulation, \n\n$$\\mathbf{b}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})$$\nwhere $\\mathbf{\\Sigma}$ is a covariance matrix.\n\nBased on this  general formulation, we can now define all unconstrained hierarchical models.\n:::\n\n\n\n## Markov Chain Monte Carlo (MCMC)\n\n:::{style=\"font-size: 0.75em;\"}\nFor simplicity, let's assume that we are monitoring the behaviour of the mallard every minutes and that we are recording whether it is\n\n#### On land \n\n![](https://ichef.bbci.co.uk/news/976/cpsprodpb/33B5/production/_127273231_comedy5.jpg){fig-align=\"center\" width=30%}\n\n#### In the water\n\n![](https://img.bbg.org/large/41847802500.jpg){fig-align=\"center\" width=20%}\n:::\n\n\n\n## Markov Chain Monte Carlo (MCMC)\n\nUsing this information, we can draw diagram defining how the behaviour of the mallard changes at every time steps\n\n. . .\n\n![](../img/DAG.png){fig-align=\"center\" width=70%}\n\n## *Markov Chain* Monte Carlo (MCMC)\n\n:::{style=\"font-size: 0.75em;\"}\nIn a markov chain, we assume that we know how probable it is to go from one behaviour (land) to another (water)\n:::\n\n. . .\n\n![](../img/DAG_Markov_chain.png){fig-align=\"center\" width=70%}\n\n## Markov Chain *Monte Carlo* (MCMC)\n\n:::{style=\"font-size: 0.75em;\"}\nIn an MCMC, we assume that the likeliness of passing from one behaviour (land) to another (water) depends on a statistical distribution. \n:::\n\n. . .\n\n![](../img/DAG_Markov_chain_monte_Carlo.png){fig-align=\"center\" width=70%}\n\n\n## Addition and Substraction\n\n$$\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}$$\n$$C_{ij} = A_{ij} \\pm B_{ij}$$\n\n::: {style=\"color: blue;\"}\n$$\\begin{bmatrix}\n\t\t\t\t3 & 5\\\\\n\t\t\t\t1 & -2\\\\\n\t\t\t\\end{bmatrix} + \n\t\t\t\\begin{bmatrix}\n\t\t\t\t2 & 1\\\\\n\t\t\t\t4 & -2\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\\begin{bmatrix}\n\t\t\t\t3+2 & 5+1\\\\\n\t\t\t\t1+4 & -2-2\\\\\n\t\t\t\\end{bmatrix} = \n\t\t\t\\begin{bmatrix}\n\t\t\t\t5 & 6\\\\\n\t\t\t\t5 & -4\\\\\n\t\t\t\\end{bmatrix}$$\n:::\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB <- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4\n```\n:::\n:::\n\n\n\n\n## Triangular matrix\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n#### Lower triangular matrix\n$$\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}$$\n:::\n::: {.column width=\"50%\"}\n#### Upper triangular matrix\n$$\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}$$\n:::\n::::\n\n## Symmetric matrix\n\nThe values on the  above and below the diagonal are match so that $A = A^t$\n\n::: {style=\"color: blue;\"}\n$$\n\\begin{bmatrix}\n\t\t\t\t3 & 4 & -10\\\\\n\t\t\t\t4 & 5 & 7\\\\\n\t\t\t\t-10 & 7 & -6\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n\n## Matrix inversion\n\n::: { style=\"font-size: 0.8em\"}\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix $\\mathbf{A}$ is defined as $\\mathbf{A}^{-1}$\n\nAs such,\n$$\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}$$ \n:::\n\n. . .\n\n::: { style=\"font-size: 0.8em\"}\nIn R\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv <- solve(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n```\n:::\n\n```{.r .cell-code}\nA %*% Ainv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1\n```\n:::\n:::\n\n\n## Matrix inversion\n### Inverting a diagonal matrix\n\n$$D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}$$\n\n\n## Estimating regression parameters\n\n. . . \n\nMany techniques have been proposed to estimate the parameters of a regression model.\n\n. . . \n\nThe goal of this course is not to study these techniques but we will learn how to play with the estimated parameters because it will be very useful as we move along.\n\n. . . \n\nThe most common way to build a regression model is \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreg <- lm(b.exemplaris ~ humidity)\n```\n:::\n\n\n. . . \n\nSay we now want to build a model's confidence interval from a linear regression\n\n. . . \n\n**How would you do it ?**\n\n. . . \n\nLet's look at the model's results, maybe it will help us\n\n## Estimating regression parameters\n\n### Model's results\n\n::: {style=\"font-size: 0.75em\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(summaryReg <- summary(reg))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = b.exemplaris ~ humidity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47988 -0.26475  0.00611  0.32590  1.36077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.57389    0.04720   54.53   <2e-16 ***\nhumidity     1.10086    0.07976   13.80   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4718 on 98 degrees of freedom\nMultiple R-squared:  0.6603,\tAdjusted R-squared:  0.6569 \nF-statistic: 190.5 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: \n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nLet's say we want to construst the model's confidence intervals by sampling multiple times (say 100 times!) the regression parameters, which we will assume follow Gaussian distribution. **How would you do this?**\n::: \n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nWe could sample the model parameters but how can we do this properly?\n\n. . .\n\n**Any suggestions?**\n\n![](https://www.i2symbol.com/pictures/emojis/c/6/0/e/c60e666a9af7bcd1a7b887437b3520c3_384.png){fig-align=\"center\" width=40%}\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIf we look at the estimated regression model coefficient, we can learn a few things\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummaryReg$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 2.573889 0.04720304 54.52804 4.011925e-75\nhumidity    1.100865 0.07975800 13.80256 1.035796e-24\n```\n:::\n:::\n\n\n. . .\n\nNotably, there are uncertainty around the parameters.\n\n. . .\n\nMaybe we can use this information to sample model parameters and reconstruct models across different iterations of parameters.\n\n. . . \n\nLet's give it a shot !\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIf we assume that the parameters of our particular model follow a Gaussian distribution, we can state that  \n\n. . .\n\n$$\\beta_0 \\sim \\mathcal{N}(2.574, 0.047^2)$$\n$$\\beta_1 \\sim \\mathcal{N}(1.101, 0.080^2)$$\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIn R, we can do this as follow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta_0 <- rnorm(100, mean = regCoef[1,1], sd = regCoef[1,2])\nbeta_1 <- rnorm(100, mean = regCoef[2,1], sd = regCoef[2,2])\n```\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nBut is this the right way to do it ?\n\n![](https://www.i2symbol.com/pictures/emojis/e/e/7/b/ee7b4fb9880ef3c8ee19626cdc14bf5c_384.png){fig-align=\"center\" width=20%}\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nActually, even if the model's confidence interval look about right, they are wrong ! \n\n![](https://www.i2symbol.com/pictures/emojis/8/b/f/8/8bf88079d65d540a26a1f181cf6ba060_384.png){fig-align=\"center\" width=20%}\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nThe approach presented in the previous slide works only if we assume that the parameters are completely independent from one another.\n\n. . . \n\nA situation that happens only in very specific circumstances.\n\n. . .\n\nSo... We need to find a way to account for the non-independencies between the parameters. \n\n. . .\n\nHow can we do this ? Any ideas ?\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n::: {style=\"font-size: 0.75em\"}\nAssuming the regression parameters are normally distributed is not a bad assumption. \n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nHowever to consider a dependencies between the parameters we need to sample them from a **multivariate** normal distribution where the variance of each parameter and their dependency is defined by a covariance matrix estimated specifically for the data we model.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nThe good news is that this covariance matrix is given by `summary.lm` function\n:::\n\n. . .\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n(covReg <- summaryReg$cov.unscaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              (Intercept)      humidity\n(Intercept)  0.0100098513 -0.0005305969\nhumidity    -0.0005305969  0.0285782940\n```\n:::\n:::\n\n:::\n## Estimating regression parameters\n\n### Sampling model parameters\n\nFor our specific model, mathematically, we assume that  \n\n. . .\n\n$$\\begin{bmatrix}\n  \\beta_0\\\\\n  \\beta_1\\\\\n\\end{bmatrix} \\sim \\mathcal{MVN} \\left( \\begin{bmatrix}\n  2.574\\\\\n  1.101\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0.0100 & -0.0005 \\\\\n  -0.0005 & 0.0286 \\\\\n\\end{bmatrix} \\right)$$\n\n. . .\n\n**Note** To present the multivariate normal distribution, we rely on matrix notation. This is our first introduction into matrix algebra. We will talk about this more into this course. \n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\nIn R, we can sample the parameters using a multivariate normal distribution using the following code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Object that include regression coefficients\nregCoef <- summaryReg$coefficients\n\n# Sample regression parameters\nbeta <- MASS::mvrnorm(100, regCoef[,1], Sigma = covReg)\n```\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Estimating regression parameters\n\n### Sampling model parameters - Comparison\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=1536}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}